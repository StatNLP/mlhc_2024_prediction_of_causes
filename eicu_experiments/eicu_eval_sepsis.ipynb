{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99d86537",
   "metadata": {},
   "source": [
    "*ToDo*\n",
    "\n",
    "02) pred_window und obs_window ausprobieren.\n",
    "03) output nochmal genau anschauen.\n",
    "04) was ist maskiert anschauen - und wie genau?\n",
    "05) VRAM wenig ausgelastet. Batch size mal mit 320 ausprobieren, aber auch durchziehen.\n",
    "06) Task pred: scheduler einfÃ¼gen.\n",
    "07) Analyse each var (sparsity)\n",
    "08) loss per var/quality of varis loss (is this already ablation to forecast only one var)\n",
    "09) Include sepsis definition\n",
    "10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "driven-termination",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random \n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, auc\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchinfo import summary\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5237f27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int) -> None:\n",
    "    \"\"\"\n",
    "    Set the random seed for modules torch, numpy and random.\n",
    "\n",
    "    :param seed: random seed\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    if torch.cuda.is_available() and torch.cuda.device_count() > 0:\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recreational-record",
   "metadata": {},
   "source": [
    "## Load forecast dataset into matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c06d2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344a82d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fore_test_op_dense = joblib.load(open('/home/mitarb/fracarolli/eicu/final/sepsis_label/fore_test_op_susinf.pkl', 'rb'))\n",
    "fore_test_ip_dense = joblib.load(open('/home/mitarb/fracarolli/eicu/final/sepsis_label/fore_test_ip_susinf.pkl', 'rb'))\n",
    "test_input = fore_test_op_dense\n",
    "test_ip = fore_test_ip_dense\n",
    "\n",
    "mean_std_dict = joblib.load(open('/home/mitarb/fracarolli/eicu/final/mean_std_dict.pkl', 'rb'))\n",
    "var_to_ind = dict()\n",
    "with open(\"/home/mitarb/fracarolli/eicu/final/var_to_ind.csv\") as file:\n",
    "    for line in file:\n",
    "        key, value = line.strip().split(\",\")\n",
    "        var_to_ind[key]=int(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33632ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_input.shape)\n",
    "print(len(test_ip[0]))\n",
    "#fore_test_op_triplet = joblib.load(open('/home/mitarb/fracarolli/eicu/final/evaluation_scripts+pkl/fore_test_op_dense.pkl', 'rb'))\n",
    "#fore_test_ip_triplet = joblib.load(open('fore_test_ip_dense.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d0f183",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4a17fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sofa(matrix, var_to_ind): #24xV matrix\n",
    "    # GCS: min_eye, min_motor, min_verbal = 5, 5, 5\n",
    "    print(matrix.size())\n",
    "    #raise Exception\n",
    "    key =\"GCS eye\"\n",
    "    var_to_ind = {x:i-1 for x,i in var_to_ind.items()}\n",
    "    a=matrix[:, var_to_ind[key]][torch.nonzero(matrix[:, var_to_ind[key]], as_tuple=True)]\n",
    "    print(a)\n",
    "    min_eye = min(matrix[:, var_to_ind[key]][torch.nonzero(matrix[:, var_to_ind[key]], as_tuple=True)]*mean_std_dict[key][1]+mean_std_dict[key][0], default=4)\n",
    "    key = \"GCS motor\"\n",
    "    min_motor = min(matrix[:, var_to_ind[key]][torch.nonzero(matrix[:, var_to_ind[key]], as_tuple=True)]*mean_std_dict[key][1]+mean_std_dict[key][0], default=6)\n",
    "    key = \"GCS verbal\"\n",
    "    min_verbal = min(matrix[:, var_to_ind[key]][torch.nonzero(matrix[:, var_to_ind[key]], as_tuple=True)]*mean_std_dict[key][1]+mean_std_dict[key][0], default=5)\n",
    "    \n",
    "\n",
    "    GCS = min_eye + min_motor + min_verbal\n",
    "    if GCS > 14: GCS_sofa = 0\n",
    "    elif GCS > 12: GCS_sofa = 1\n",
    "    elif GCS > 9:  GCS_sofa = 2\n",
    "    elif GCS > 5:  GCS_sofa = 3\n",
    "    else: GCS_sofa = 4\n",
    "    #print('GCS_sofa is', GCS_sofa, ';     GCS is', GCS,'; GCS eye', min_eye, '; GCS motor', min_motor, '; GCS verbal', min_verbal)\n",
    "\n",
    "    key = \"Bilirubin (Total)\"\n",
    "    bilir = max(matrix[:, var_to_ind[key]][torch.nonzero(matrix[:, var_to_ind[key]], as_tuple=True)]*mean_std_dict[key][1]+mean_std_dict[key][0], default=1)\n",
    "    if bilir > 12: bilir_sofa = 4\n",
    "    elif bilir > 6: bilir_sofa = 3\n",
    "    elif bilir > 2: bilir_sofa = 2\n",
    "    elif bilir > 1.2: bilir_sofa = 1\n",
    "    else: bilir_sofa = 0\n",
    "    #print('bilir_Sofa is', bilir_sofa, ';   bilirubin is', bilir)\n",
    "    \n",
    "    # Coagulation (Platelets)\n",
    "    key = \"Platelets\"\n",
    "    plate = max(matrix[:, var_to_ind[key]][torch.nonzero(matrix[:, var_to_ind[key]], as_tuple=True)]*mean_std_dict[key][1]+mean_std_dict[key][0], default=160)\n",
    "    if plate > 150: plate_sofa = 0\n",
    "    elif plate > 100: plate_sofa = 1\n",
    "    elif plate > 50: plate_sofa = 2\n",
    "    elif plate > 20: plate_sofa = 3\n",
    "    else: plate_sofa = 4\n",
    "    #print('plate_sofa is', plate_sofa, ';   platelet count is', plate)\n",
    "    \n",
    "    # print('Urinmenge 24h', sum(data_var[data_var['variable']=='Urine']['value2']))\n",
    "\n",
    "    key = \"Urine\"\n",
    "    urine = sum(matrix[:, var_to_ind[key]][torch.nonzero(matrix[:, var_to_ind[key]], as_tuple=True)]*mean_std_dict[key][1]+mean_std_dict[key][0])\n",
    "    key = \"Creatinine (Blood)\"\n",
    "    creat = max(matrix[:, var_to_ind[key]][torch.nonzero(matrix[:, var_to_ind[key]], as_tuple=True)]*mean_std_dict[key][1]+mean_std_dict[key][0], default=1)\n",
    "    \n",
    "    if (urine < 200) or (creat > 5): renal_sofa = 4\n",
    "    elif  (urine < 500) or (creat > 3.5): renal_sofa = 3\n",
    "    elif creat > 2.0: renal_sofa = 2\n",
    "    elif creat > 1.2: renal_sofa = 1\n",
    "    else: renal_sofa = 0\n",
    "    #print('renal_sofa:',renal_sofa,';       urine 24:',urine,'; creat:', creat)\n",
    "    \n",
    "    CS_data = get_CS(matrix, var_to_ind)\n",
    "    cs_sofa = CS_SOFA(CS_data)\n",
    "    \n",
    "    #cs_sofa = 0\n",
    "    key=\"FiO2\"\n",
    "    fio2 = (matrix[:, var_to_ind[key]]*mean_std_dict[key][1]+mean_std_dict[key][0])\n",
    "    key=\"PaO2\"\n",
    "    po2 = (matrix[:, var_to_ind[key]]*mean_std_dict[key][1]+mean_std_dict[key][0])\n",
    "    PaO2FiO2 = 100*po2/fio2\n",
    "    print(\"size\", PaO2FiO2.size())\n",
    "    PaO2FiO2 = PaO2FiO2[torch.nonzero(PaO2FiO2, as_tuple=True)]\n",
    "    pao2fio2 = min(PaO2FiO2)\n",
    "    if pao2fio2<100: resp=4\n",
    "    elif pao2fio2<200: resp=3\n",
    "    elif pao2fio2<300:resp=2\n",
    "    elif pao2fio2<400:resp=1\n",
    "    else: resp=0\n",
    "    return GCS_sofa, cs_sofa, resp, plate_sofa, bilir_sofa, renal_sofa\n",
    "\n",
    "def get_CS(matrix, var_to_ind):\n",
    "    #data_var = data_pat[data_pat['variable'].isin(['Dobutamine','Dopamine','Epinephrine','Norepinephrine','Weight'])]\n",
    "    #data_var['value2'] = data_var['value']*data_var['std']+data_var['mean']\n",
    "    \n",
    "    #weight = min(data_var[data_var['variable']=='Weight']['value2'], default=80)  # set default weight to 80kg.\n",
    "    key = \"Weight\"\n",
    "    weight = 80# min(matrix[:, var_to_ind[key]][torch.nonzero(matrix[:, var_to_ind[key]], as_tuple=True)], default=80)#*mean_std_dict[key][1]+mean_std_dict[key][0], default=1)\n",
    "    key = \"d Dopamine ratio\"\n",
    "    try:\n",
    "        data_dop = max(matrix[:, var_to_ind[key]][torch.nonzero(matrix[:, var_to_ind[key]], as_tuple=True)]*mean_std_dict[key][1]+mean_std_dict[key][0], default=1)\n",
    "        data_dop = data_dop /60/weight*1000\n",
    "    except:\n",
    "        data_dop = 0\n",
    "\n",
    "    key = \"d Dobutamine ratio\"\n",
    "    \n",
    "    try:\n",
    "        data_dobu = max(matrix[:, var_to_ind[key]][torch.nonzero(matrix[:, var_to_ind[key]], as_tuple=True)]*mean_std_dict[key][1]+mean_std_dict[key][0], default=1)\n",
    "        data_dobu = data_dobu  /60/weight*1000\n",
    "    except:\n",
    "        data_dobu = 0\n",
    "    key = \"d Epinephrine ratio\"\n",
    "    \n",
    "    try:\n",
    "        data_epi = max(matrix[:, var_to_ind[key]][torch.nonzero(matrix[:, var_to_ind[key]], as_tuple=True)]*mean_std_dict[key][1]+mean_std_dict[key][0], default=1)\n",
    "        data_epi = data_epi  /60/weight*1000\n",
    "    except:\n",
    "        data_epi = 0\n",
    "    key = \"d Norepinephrine ratio\"\n",
    "    \n",
    "    try:\n",
    "        data_nore = max(matrix[:, var_to_ind[key]][torch.nonzero(matrix[:, var_to_ind[key]], as_tuple=True)]*mean_std_dict[key][1]+mean_std_dict[key][0], default=1)\n",
    "        data_nore = data_nore /60/weight*1000\n",
    "    except:\n",
    "        data_nore = 0\n",
    "        \n",
    "\n",
    "\n",
    "    key = \"SBP\"\n",
    "    SBP = (matrix[:, var_to_ind[key]]*mean_std_dict[key][1]+mean_std_dict[key][0])\n",
    "\n",
    "    key = \"DBP\"\n",
    "    DBP = (matrix[:, var_to_ind[key]]*mean_std_dict[key][1]+mean_std_dict[key][0])\n",
    "\n",
    "    MAP = 2/3 * DBP + 1/3 * SBP\n",
    "    MAP = min(MAP[torch.nonzero(MAP, as_tuple=True)], default=100)\n",
    "                 \n",
    "        \n",
    "    return MAP, data_dop, data_dobu, data_epi, data_nore \n",
    "    \n",
    "def CS_SOFA(data):\n",
    "    map = data[0]\n",
    "    dop, dobu, epi, nore = data[1:5]\n",
    "    # print('CS data: mdden', data)\n",
    "    if (dop > 15) or (epi > 0.1) or (nore > 0.01): CS = 4\n",
    "    elif (dop > 5) or (epi > 0) or (nore > 0): CS = 3\n",
    "    elif (dop > 0) or (dobu > 0): CS = 2\n",
    "    elif map < 70: CS = 1\n",
    "    else: CS = 0\n",
    "    # print('CS Sofa is:', CS)\n",
    "    return CS \n",
    "    \n",
    "factor_keys =[\"GCS eye\", \"GCS motor\", \"GCS verbal\", \"Bilirubin (Total)\", \"Platelets\", \"Urine\", \"Creatinine (Blood)\", \"FiO2\", \"PaO2\", \"d Dopamine ratio\", \"d Dobutamine ratio\",\n",
    "\"d Epinephrine ratio\", \"d Norepinephrine ratio\", \"SBP\", \"DBP\"]\n",
    "factor_indices = sorted([var_to_ind[x]-1 for x in factor_keys])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd8241f",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model, e_layers, d_layers, d_ff = 256, 2, 2, 2048\n",
    "number_of_epochs = 100\n",
    "V, D = 98, 17\n",
    "fore_max_len = 2640\n",
    "print('loading of joblib completed')\n",
    "\n",
    "\n",
    "\n",
    "#Michi: die ganzen Argparses kÃ¶nnte man noch anders implementieren, wollte es nur schnell zum laufen bringen.\n",
    "import argparse\n",
    "parser = argparse.ArgumentParser(description='Autoformer & Transformer family for Time Series Forecasting')\n",
    "\n",
    "parser.add_argument('--is_training', type=int, required=False, default=1, help='status')\n",
    "parser.add_argument('--train_only', type=bool, required=False, default=False, help='perform training on full input dataset without validation and testing')\n",
    "parser.add_argument('--model_id', type=str, required=False, default='test', help='model id')\n",
    "parser.add_argument('--model', type=str, required=False, default='Autoformer',\n",
    "                    help='model name, options: [Autoformer, Informer, Transformer]')\n",
    "\n",
    "# data loader\n",
    "parser.add_argument('--data', type=str, required=False, default='ETTm1', help='dataset type')\n",
    "parser.add_argument('--root_path', type=str, default='./data/ETT/', help='root path of the data file')\n",
    "parser.add_argument('--data_path', type=str, default='ETTh1.csv', help='data file')\n",
    "parser.add_argument('--features', type=str, default='M',\n",
    "                    help='forecasting task, options:[M, S, MS]; M:multivariate predict multivariate, S:univariate predict univariate, MS:multivariate predict univariate')\n",
    "parser.add_argument('--target', type=str, default='OT', help='target feature in S or MS task')\n",
    "parser.add_argument('--freq', type=str, default='h',\n",
    "                    help='freq for time features encoding, options:[s:secondly, t:minutely, h:hourly, d:daily, b:business days, w:weekly, m:monthly], you can also use more detailed freq like 15min or 3h')\n",
    "parser.add_argument('--checkpoints', type=str, default='./checkpoints/', help='location of model checkpoints')\n",
    "\n",
    "# forecasting task\n",
    "parser.add_argument('--seq_len', type=int, default=24, help='input sequence length')\n",
    "parser.add_argument('--label_len', type=int, default=0, help='start token length')\n",
    "parser.add_argument('--pred_len', type=int, default=24, help='prediction sequence length')\n",
    "\n",
    "\n",
    "# DLinear\n",
    "parser.add_argument('--individual', action='store_true', default=False, help='DLinear: a linear layer for each variate(channel) individually')\n",
    "# Formers \n",
    "#Michi: bisher nur default=3 zum laufen gebracht\n",
    "parser.add_argument('--embed_type', type=int, default=3, help='0: default 1: value embedding + temporal embedding + positional embedding 2: value embedding + temporal embedding 3: value embedding + positional embedding 4: value embedding')\n",
    "parser.add_argument('--enc_in', type=int, default=V*2, help='encoder input size') # DLinear with --individual, use this hyperparameter as the number of channels\n",
    "parser.add_argument('--dec_in', type=int, default=V, help='decoder input size')\n",
    "parser.add_argument('--c_out', type=int, default=V, help='output size')\n",
    "parser.add_argument('--d_model', type=int, default=d_model, help='dimension of model')\n",
    "parser.add_argument('--n_heads', type=int, default=8, help='num of heads')\n",
    "parser.add_argument('--e_layers', type=int, default=e_layers, help='num of encoder layers')\n",
    "parser.add_argument('--d_layers', type=int, default=d_layers, help='num of decoder layers')\n",
    "parser.add_argument('--d_ff', type=int, default=d_ff, help='dimension of fcn')\n",
    "parser.add_argument('--moving_avg', type=int, default=25, help='window size of moving average')\n",
    "parser.add_argument('--factor', type=int, default=1, help='attn factor')\n",
    "parser.add_argument('--distil', action='store_false',\n",
    "                    help='whether to use distilling in encoder, using this argument means not using distilling',\n",
    "                    default=True)\n",
    "parser.add_argument('--dropout', type=float, default=0.05, help='dropout')\n",
    "parser.add_argument('--embed', type=str, default='timeF',\n",
    "                    help='time features encoding, options:[timeF, fixed, learned]')\n",
    "parser.add_argument('--activation', type=str, default='gelu', help='activation')\n",
    "parser.add_argument('--output_attention', action='store_true', help='whether to output attention in ecoder')\n",
    "parser.add_argument('--do_predict', action='store_true', help='whether to predict unseen future data')\n",
    "\n",
    "# optimization\n",
    "parser.add_argument('--num_workers', type=int, default=10, help='data loader num workers')\n",
    "parser.add_argument('--itr', type=int, default=2, help='experiments times')\n",
    "parser.add_argument('--train_epochs', type=int, default=10, help='train epochs')\n",
    "parser.add_argument('--batch_size', type=int, default=32, help='batch size of train input data')\n",
    "parser.add_argument('--patience', type=int, default=3, help='early stopping patience')\n",
    "parser.add_argument('--learning_rate', type=float, default=0.0001, help='optimizer learning rate')\n",
    "parser.add_argument('--des', type=str, default='test', help='exp description')\n",
    "parser.add_argument('--loss', type=str, default='mse', help='loss function')\n",
    "parser.add_argument('--lradj', type=str, default='type1', help='adjust learning rate')\n",
    "parser.add_argument('--use_amp', action='store_true', help='use automatic mixed precision training', default=False)\n",
    "\n",
    "# GPU\n",
    "parser.add_argument('--use_gpu', type=bool, default=True, help='use gpu')\n",
    "parser.add_argument('--gpu', type=int, default=0, help='gpu')\n",
    "parser.add_argument('--use_multi_gpu', action='store_true', help='use multiple gpus', default=False)\n",
    "parser.add_argument('--devices', type=str, default='0,1,2,3', help='device ids of multile gpus')\n",
    "parser.add_argument('--test_flop', action='store_true', default=False, help='See utils/tools for usage')\n",
    "args = parser.parse_args(args=[])\n",
    "import importlib\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b118361e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import models.InformerAutoregressiveWithForcedDecoding as autoformer\n",
    "importlib.reload(autoformer)\n",
    "model = autoformer.Model(args).cuda()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875c7053",
   "metadata": {},
   "source": [
    "# Crossinteractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bc3213",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "torch.cuda.empty_cache()\n",
    "import models.InformerAutoregressiveWithForcedDecoding as autoformer\n",
    "importlib.reload(autoformer)\n",
    "batch_size, lr, patience = 32, 0.0005, 6  # batch_size increased, patience 10 --> 6\n",
    "d, N, he, dropout = 50, 2, 4, 0.2\n",
    "model = autoformer.Model(args).cuda()\n",
    "\n",
    "\n",
    "# Load pretrained weights here.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "#fore_savepath = 'mimic_iii_24h_strats_no_interp_with_ss_fore_blubdiblub.pytorch'\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'max', patience=3)\n",
    "model.load_state_dict(torch.load(\"mimic_iii_24h_strats_no_interp_with_ss_fore_informer_ims_fr_detach.pytorch\"))\n",
    "#model.load_state_dict(torch.load(fore_savepath))\n",
    "model.eval()\n",
    "y_true, y_pred = [], []\n",
    "print(test_input)\n",
    "pbar = tqdm(range(0, len(test_input), batch_size))\n",
    "loss_list = []\n",
    "test_loss = 0\n",
    "\n",
    "keys = [\"Dopamine\", \"Dobutamine\", \"Norepinephrine\"]\n",
    "\n",
    "#data_dop = max(matrix[:, var_to_ind[key]][torch.nonzero(matrix[:, var_to_ind[key]], as_tuple=True)]*mean_std_dict[key][1]+mean_std_dict[key][0], default=1)\n",
    "\n",
    "min_dict = dict()\n",
    "max_dict = dict()\n",
    "twentyfive_dict = dict()\n",
    "median_dict = dict()\n",
    "seventyfive_dict = dict()\n",
    "matrix = torch.tensor(test_input)\n",
    "for x in range(0, len(test_input)):\n",
    "\n",
    "    for key in keys:\n",
    "        try:\n",
    "            min_data = min(matrix[x, :, var_to_ind[key]][torch.nonzero(matrix[x, :, var_to_ind[key]], as_tuple=True)])#*mean_std_dict[key][1]+mean_std_dict[key][0], default=1)\n",
    "            if key not in min_dict:\n",
    "                min_dict[key] = min_data\n",
    "            else:\n",
    "                min_dict[key] = min([min_data, min_dict[key]])\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            max_data = max(matrix[x, :, var_to_ind[key]][torch.nonzero(matrix[x, :, var_to_ind[key]], as_tuple=True)])#*mean_std_dict[key][1]+mean_std_dict[key][0], default=1)\n",
    "\n",
    "            if key not in max_dict:\n",
    "                max_dict[key] = max_data\n",
    "            else:\n",
    "                max_dict[key] = max([max_data, max_dict[key]])\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            twentyfive_data = torch.quantile(matrix[x, :, var_to_ind[key]][torch.nonzero(matrix[x, :, var_to_ind[key]], as_tuple=True)], 0.25)#*mean_std_dict[key][1]+mean_std_dict[key][0], default=1)\n",
    "\n",
    "            if key not in twentyfive_dict:\n",
    "                twentyfive_dict[key] = twentyfive_data.item()\n",
    "            else:\n",
    "                twentyfive_dict[key] = max([twentyfive_data, twentyfive_dict[key]])\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            median_data = torch.quantile(matrix[x, :, var_to_ind[key]][torch.nonzero(matrix[x, :, var_to_ind[key]], as_tuple=True)], 0.50)#*mean_std_dict[key][1]+mean_std_dict[key][0], default=1)\n",
    "\n",
    "            if key not in median_dict:\n",
    "                median_dict[key] = median_data.item()\n",
    "            else:\n",
    "                median_dict[key] = max([median_data, median_dict[key]])\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            seventyfive_data = torch.quantile(matrix[x, :, var_to_ind[key]][torch.nonzero(matrix[x, :, var_to_ind[key]], as_tuple=True)], 0.75)#*mean_std_dict[key][1]+mean_std_dict[key][0], default=1)\n",
    "\n",
    "            if key not in seventyfive_dict:\n",
    "                seventyfive_dict[key] = seventyfive_data.item()\n",
    "            else:\n",
    "                seventyfive_dict[key] = max([seventyfive_data, seventyfive_dict[key]])\n",
    "        except:\n",
    "            pass\n",
    "print(min_dict, max_dict)\n",
    "resultdict = dict()\n",
    "for key in keys:\n",
    "    maxes, mins, twentyfives, medians, seventyfives = [], [], [], [], []\n",
    "    for start in pbar:\n",
    "        print(start)\n",
    "        matrix = torch.tensor(test_input[start:start+batch_size], dtype=torch.float32).cuda()\n",
    "        input_matrix = matrix[:, :24]\n",
    "\n",
    "        #torch.Size([32, 24, 129])\n",
    "        input_mark = torch.arange(0, input_matrix.size(1)).unsqueeze(0).repeat(input_matrix.size(0), 1).cuda()\n",
    "        #torch.Size([32, 24])\n",
    "        input_mask = input_matrix[:, :24, V:]\n",
    "        output_matrix = matrix[:, 24:, :V]\n",
    "        #torch.Size([32, 24, 129])\n",
    "        output_mark = torch.arange(0, output_matrix.size(1)).unsqueeze(0).repeat(input_matrix.size(0), 1).cuda()\n",
    "        #torch.Size([32, 24])\n",
    "        output_mask = matrix[:, 24:, V:]\n",
    "        #torch.Size([32, 24, 129])\n",
    "        #print(var_to_ind[key])\n",
    "        #print(min_dict[key])\n",
    "        #print(max_dict[key])\n",
    "        with torch.no_grad():\n",
    "            output_min = model(input_matrix, input_mark, None, None, trainn=False, change_dict = {var_to_ind[key]-1: min_dict[key]})\n",
    "            output_max = model(input_matrix, input_mark, None, None, trainn=False, change_dict = {var_to_ind[key]-1: max_dict[key]})\n",
    "            \n",
    "            output_25 = model(input_matrix, input_mark, None, None, trainn=False, change_dict = {var_to_ind[key]-1: twentyfive_dict[key]})\n",
    "            output_50 = model(input_matrix, input_mark, None, None, trainn=False, change_dict = {var_to_ind[key]-1: median_dict[key]})\n",
    "            output_75 = model(input_matrix, input_mark, None, None, trainn=False, change_dict = {var_to_ind[key]-1: seventyfive_dict[key]})\n",
    "        #print(output1, output2, output1==output2)\n",
    "        for x in output_min:\n",
    "            mins.append(x.cpu())\n",
    "        for x in output_max:\n",
    "            maxes.append(x.cpu())\n",
    "        for x in output_25:\n",
    "            twentyfives.append(x.cpu())\n",
    "        for x in output_50:\n",
    "            medians.append(x.cpu())\n",
    "        for x in output_75:\n",
    "            seventyfives.append(x.cpu())\n",
    "    print(len(maxes), maxes[0].size())\n",
    "    resultdict[(key, \"min\")]=mins\n",
    "    resultdict[(key, \"max\")]=maxes\n",
    "    resultdict[(key, \"25\")]=twentyfives\n",
    "    resultdict[(key, \"50\")]=medians\n",
    "    resultdict[(key, \"75\")]=seventyfives\n",
    "import pickle\n",
    "\n",
    "with open('crossinteractions.pickle', 'wb') as handle:\n",
    "    pickle.dump([resultdict, var_to_ind, min_dict, max_dict, mean_std_dict], handle)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178f1b03",
   "metadata": {},
   "source": [
    "# 8 Sofa Analysis IMS Informer TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26575f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "d_model, e_layers, d_layers, d_ff = 256, 2, 2, 2048\n",
    "number_of_epochs = 100\n",
    "V, D = 98, 17\n",
    "fore_max_len = 2640\n",
    "print('loading of joblib completed')\n",
    "\n",
    "\n",
    "import argparse\n",
    "parser = argparse.ArgumentParser(description='Autoformer & Transformer family for Time Series Forecasting')\n",
    "\n",
    "# basic config\n",
    "parser.add_argument('--is_training', type=int, required=False, default=1, help='status')\n",
    "parser.add_argument('--train_only', type=bool, required=False, default=False, help='perform training on full input dataset without validation and testing')\n",
    "parser.add_argument('--model_id', type=str, required=False, default='test', help='model id')\n",
    "parser.add_argument('--model', type=str, required=False, default='Autoformer',\n",
    "                    help='model name, options: [Autoformer, Informer, Transformer]')\n",
    "\n",
    "# data loader\n",
    "parser.add_argument('--data', type=str, required=False, default='ETTm1', help='dataset type')\n",
    "parser.add_argument('--root_path', type=str, default='./data/ETT/', help='root path of the data file')\n",
    "parser.add_argument('--data_path', type=str, default='ETTh1.csv', help='data file')\n",
    "parser.add_argument('--features', type=str, default='M',\n",
    "                    help='forecasting task, options:[M, S, MS]; M:multivariate predict multivariate, S:univariate predict univariate, MS:multivariate predict univariate')\n",
    "parser.add_argument('--target', type=str, default='OT', help='target feature in S or MS task')\n",
    "parser.add_argument('--freq', type=str, default='h',\n",
    "                    help='freq for time features encoding, options:[s:secondly, t:minutely, h:hourly, d:daily, b:business days, w:weekly, m:monthly], you can also use more detailed freq like 15min or 3h')\n",
    "parser.add_argument('--checkpoints', type=str, default='./checkpoints/', help='location of model checkpoints')\n",
    "\n",
    "# forecasting task\n",
    "parser.add_argument('--seq_len', type=int, default=24, help='input sequence length')\n",
    "parser.add_argument('--label_len', type=int, default=0, help='start token length')\n",
    "parser.add_argument('--pred_len', type=int, default=24, help='prediction sequence length')\n",
    "\n",
    "\n",
    "# DLinear\n",
    "parser.add_argument('--individual', action='store_true', default=False, help='DLinear: a linear layer for each variate(channel) individually')\n",
    "# Formers \n",
    "#Michi: bisher nur default=3 zum laufen gebracht\n",
    "parser.add_argument('--embed_type', type=int, default=3, help='0: default 1: value embedding + temporal embedding + positional embedding 2: value embedding + temporal embedding 3: value embedding + positional embedding 4: value embedding')\n",
    "parser.add_argument('--enc_in', type=int, default=V*2, help='encoder input size') # DLinear with --individual, use this hyperparameter as the number of channels\n",
    "parser.add_argument('--dec_in', type=int, default=V, help='decoder input size')\n",
    "parser.add_argument('--c_out', type=int, default=V, help='output size')\n",
    "parser.add_argument('--d_model', type=int, default=d_model, help='dimension of model')\n",
    "parser.add_argument('--n_heads', type=int, default=8, help='num of heads')\n",
    "parser.add_argument('--e_layers', type=int, default=e_layers, help='num of encoder layers')\n",
    "parser.add_argument('--d_layers', type=int, default=d_layers, help='num of decoder layers')\n",
    "parser.add_argument('--d_ff', type=int, default=d_ff, help='dimension of fcn')\n",
    "parser.add_argument('--moving_avg', type=int, default=25, help='window size of moving average')\n",
    "parser.add_argument('--factor', type=int, default=1, help='attn factor')\n",
    "parser.add_argument('--distil', action='store_false',\n",
    "                    help='whether to use distilling in encoder, using this argument means not using distilling',\n",
    "                    default=True)\n",
    "parser.add_argument('--dropout', type=float, default=0.05, help='dropout')\n",
    "parser.add_argument('--embed', type=str, default='timeF',\n",
    "                    help='time features encoding, options:[timeF, fixed, learned]')\n",
    "parser.add_argument('--activation', type=str, default='gelu', help='activation')\n",
    "parser.add_argument('--output_attention', action='store_true', help='whether to output attention in ecoder')\n",
    "parser.add_argument('--do_predict', action='store_true', help='whether to predict unseen future data')\n",
    "\n",
    "# optimization\n",
    "parser.add_argument('--num_workers', type=int, default=10, help='data loader num workers')\n",
    "parser.add_argument('--itr', type=int, default=2, help='experiments times')\n",
    "parser.add_argument('--train_epochs', type=int, default=10, help='train epochs')\n",
    "parser.add_argument('--batch_size', type=int, default=32, help='batch size of train input data')\n",
    "parser.add_argument('--patience', type=int, default=3, help='early stopping patience')\n",
    "parser.add_argument('--learning_rate', type=float, default=0.0001, help='optimizer learning rate')\n",
    "parser.add_argument('--des', type=str, default='test', help='exp description')\n",
    "parser.add_argument('--loss', type=str, default='mse', help='loss function')\n",
    "parser.add_argument('--lradj', type=str, default='type1', help='adjust learning rate')\n",
    "parser.add_argument('--use_amp', action='store_true', help='use automatic mixed precision training', default=False)\n",
    "\n",
    "# GPU\n",
    "parser.add_argument('--use_gpu', type=bool, default=True, help='use gpu')\n",
    "parser.add_argument('--gpu', type=int, default=0, help='gpu')\n",
    "parser.add_argument('--use_multi_gpu', action='store_true', help='use multiple gpus', default=False)\n",
    "parser.add_argument('--devices', type=str, default='0,1,2,3', help='device ids of multile gpus')\n",
    "parser.add_argument('--test_flop', action='store_true', default=False, help='See utils/tools for usage')\n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f642a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import models.InformerAutoregressiveFull as autoformer\n",
    "importlib.reload(autoformer)\n",
    "batch_size, lr, patience = 32, 0.0005, 6  # batch_size increased, patience 10 --> 6\n",
    "d, N, he, dropout = 50, 2, 4, 0.2\n",
    "model = autoformer.Model(args).cuda()\n",
    "# Load pretrained weights here.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'max', patience=3)\n",
    "model.load_state_dict(torch.load(\"/home/mitarb/fracarolli/eicu/240401_Experimente/eicu_mod08_Dense_IMS_TF.pytorch\"))\n",
    "model.eval()\n",
    "y_true, y_pred = [], []\n",
    "pbar = tqdm(range(0, len(test_input), batch_size))\n",
    "loss_list = []\n",
    "test_loss = 0\n",
    "test_loss_part1 = 0\n",
    "test_loss_part2 = 0\n",
    "sofa_losses = []\n",
    "test_loss_sofa_variables = 0\n",
    "accuracy_list = []\n",
    "TP, TN, FP, FN = 0, 0, 0, 0\n",
    "test_loss_sum_square_total = 0\n",
    "test_loss_part1_sum_square_total = 0\n",
    "test_loss_part2_sum_square_total = 0\n",
    "test_loss_sofa_sum_square_total = 0\n",
    "for start in pbar:\n",
    "    print(start)\n",
    "    matrix = torch.tensor(test_input[start:start+batch_size], dtype=torch.float32).cuda()\n",
    "    input_matrix = matrix[:, :24]\n",
    "\n",
    "    #torch.Size([32, 24, 129])\n",
    "    input_mark = torch.arange(0, input_matrix.size(1)).unsqueeze(0).repeat(input_matrix.size(0), 1).cuda()\n",
    "    #torch.Size([32, 24])\n",
    "    input_mask = input_matrix[:, :24, V:]\n",
    "    output_matrix = matrix[:, 24:, :V]\n",
    "    #torch.Size([32, 24, 129])\n",
    "    output_mark = torch.arange(0, output_matrix.size(1)).unsqueeze(0).repeat(input_matrix.size(0), 1).cuda()\n",
    "    #torch.Size([32, 24])\n",
    "    output_mask = matrix[:, 24:, V:]\n",
    "    #torch.Size([32, 24, 129])\n",
    "    \n",
    "    output_mark = torch.arange(0, output_matrix.size(1)).unsqueeze(0).repeat(input_matrix.size(0), 1).cuda()\n",
    "\n",
    "    dec_inp = torch.zeros_like(output_matrix[:, -args.pred_len:, :]).float()\n",
    "    with torch.no_grad():\n",
    "        output = model(input_matrix, input_mark, dec_inp, output_mark, trainn=False)\n",
    "        loss = output_mask[:, -args.pred_len:, :]*(\n",
    "        output-output_matrix[:, -args.pred_len:, :])**2\n",
    "    loss_list.extend(loss.sum(axis=-1).mean(axis=-1).detach().cpu().tolist())\n",
    "    for outpu, real, mask, input_mat in zip(output, output_matrix, output_mask, input_matrix):\n",
    "        a = (get_sofa(outpu*mask, var_to_ind))\n",
    "        b = (get_sofa(real, var_to_ind))\n",
    "        c = (get_sofa(input_mat, var_to_ind)) #SOFA Source (First Day)\n",
    "        prediction_sofa_difference = (sum(a)-sum(c)) >= 2\n",
    "        gold_sofa_difference = (sum(b)-sum(c)) >= 2\n",
    "        accuracy_list.append(prediction_sofa_difference == gold_sofa_difference)\n",
    "        if gold_sofa_difference:\n",
    "            if prediction_sofa_difference: TP +=1\n",
    "            else: FP += 1\n",
    "        else:\n",
    "            if prediction_sofa_difference: TN +=1\n",
    "            else: FN += 1\n",
    "        c, d = torch.tensor(a, dtype=torch.float32), torch.tensor(b, dtype=torch.float32)\n",
    "        sofa_losses.append(((c-d)**2).sum(axis=-1).mean().item())\n",
    "    loss_old = loss\n",
    "    loss = loss.sum(axis=-1).mean(axis=-1).sum()\n",
    "    loss_part1 = loss_old[:, :8].sum(axis=-1).mean()\n",
    "    loss_part2 = loss_old[:, 8:].sum(axis=-1).mean()\n",
    "    test_loss+=loss\n",
    "    test_loss_sum_square_total += torch.sum(loss_old.sum(axis=-1).mean(axis=-1)**2)\n",
    "    test_loss_part1_sum_square_total += torch.sum(loss_old[:, :8].sum(axis=-1).mean(axis=-1)**2)\n",
    "    test_loss_part2_sum_square_total += torch.sum(loss_old[:, 8:].sum(axis=-1).mean(axis=-1)**2)\n",
    "    test_loss_part1+=loss_part1\n",
    "    test_loss_part2+=loss_part2\n",
    "    test_loss_sofa_variables+=loss_old[:, :, factor_indices].sum(axis=-1).mean()\n",
    "    test_loss_sofa_sum_square_total += torch.sum(loss_old[:, :, factor_indices].sum(axis=-1).mean(axis=-1)**2)\n",
    "test_loss_p = test_loss/len(test_input)\n",
    "se_loss_p = torch.sqrt(test_loss_sum_square_total / len(test_input) - test_loss_p**2)/len(test_input)\n",
    "test_loss_part1_p = test_loss_part1*batch_size/len(test_input)\n",
    "test_loss_part2_p = test_loss_part2*batch_size/len(test_input)\n",
    "test_loss_sofa_variables_p = test_loss_sofa_variables*batch_size/len(test_input)\n",
    "sofa_loss = sum(sofa_losses)/len(sofa_losses)\n",
    "\n",
    "se_loss_part1_p = torch.sqrt(test_loss_part1_sum_square_total / len(test_input) - test_loss_part1_p**2)/len(test_input)\n",
    "se_loss_part2_p = torch.sqrt(test_loss_part2_sum_square_total / len(test_input) - test_loss_part2_p**2)/len(test_input)\n",
    "se_loss_sofa_p  = torch.sqrt(test_loss_sofa_sum_square_total / len(test_input)  - test_loss_sofa_variables_p**2)/len(test_input)\n",
    "accuracy_sepsis = sum(accuracy_list)/len(accuracy_list)\n",
    "se_accuracy_sepsis = np.sqrt(accuracy_sepsis * (1-accuracy_sepsis)/len(test_input))\n",
    "\n",
    "\n",
    "model_name = '8&Dense&IMS&TF&-&'\n",
    "MSE_string = \"{:.3f}\".format(test_loss_p.item()) + \"$_{[\" + \"{:.3f}\".format(test_loss_p.item()-1.96*se_loss_p.item()) +','+ \"{:.3f}\".format(test_loss_p.item()+1.96*se_loss_p.item()) + ']}$&'\n",
    "SOFA_string = \"{:.3f}\".format(sofa_loss)+ '$_{['+\"{:.3f}\".format(sofa_loss-1.96*se_loss_sofa_p.item())+','+\"{:.3f}\".format(sofa_loss+1.96*se_loss_sofa_p.item())+ ']}$&'\n",
    "MSE8_string = \"{:.3f}\".format(test_loss_part1_p.item()) + \"$_{[\" + \"{:.3f}\".format(test_loss_part1_p.item()-1.96*se_loss_part1_p.item()) +','+ \"{:.3f}\".format(test_loss_part1_p.item()+1.96*se_loss_part1_p.item()) + ']}$&'\n",
    "MSE16_string = \"{:.3f}\".format(test_loss_part2_p.item()) + \"$_{[\" + \"{:.3f}\".format(test_loss_part2_p.item()-1.96*se_loss_part2_p.item()) +','+ \"{:.3f}\".format(test_loss_part2_p.item()+1.96*se_loss_part2_p.item()) + ']}$&'\n",
    "SOFA_acc_string = \"{:.2f}\".format(accuracy_sepsis*100)+ '$_{['+\"{:.2f}\".format(100*(accuracy_sepsis-1.96*se_accuracy_sepsis))+','+\"{:.2f}\".format(100*(accuracy_sepsis+1.96*se_accuracy_sepsis))+ ']}$&'\n",
    "print(model_name,MSE_string, SOFA_string, sep='',file=open('eicu_table_sample_sepsis1.txt','a'))\n",
    "print(model_name,MSE8_string, MSE16_string, sep='',file=open('eicu_table_sample_sepsis2.txt','a'))\n",
    "print(model_name,TP,'&',TN,'&',FP,'&',FN,'&', TP+TN+FP+FN,'&', len(test_input), sep='',file=open('eicu_table_sample_sepsis3.txt','a')) \n",
    "print(model_name,len(test_input),'&',SOFA_acc_string, sep='',file=open('eicu_table_sample_sepsis4.txt','a'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3b7295",
   "metadata": {},
   "source": [
    "# 7 Sofa Analysis DMS Informer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294db61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import models.Informer as autoformer\n",
    "importlib.reload(autoformer)\n",
    "batch_size, lr, patience = 32, 0.0005, 6  # batch_size increased, patience 10 --> 6\n",
    "d, N, he, dropout = 50, 2, 4, 0.2\n",
    "model = autoformer.Model(args).cuda()\n",
    "# Load pretrained weights here.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'max', patience=3)\n",
    "model.load_state_dict(torch.load(\"/home/mitarb/fracarolli/eicu/final/240408_models/eicu_informer.pytorch\"))\n",
    "model.eval()\n",
    "y_true, y_pred = [], []\n",
    "print(test_input)\n",
    "pbar = tqdm(range(0, len(test_input), batch_size))\n",
    "loss_list = []\n",
    "test_loss = 0\n",
    "test_loss_part1 = 0\n",
    "test_loss_part2 = 0\n",
    "sofa_losses = []\n",
    "test_loss_sofa_variables = 0\n",
    "accuracy_list = []\n",
    "TP, TN, FP, FN = 0, 0, 0, 0\n",
    "test_loss_sum_square_total = 0\n",
    "test_loss_part1_sum_square_total = 0\n",
    "test_loss_part2_sum_square_total = 0\n",
    "test_loss_sofa_sum_square_total = 0\n",
    "for start in pbar:\n",
    "    print(start)\n",
    "    matrix = torch.tensor(test_input[start:start+batch_size], dtype=torch.float32).cuda()\n",
    "    input_matrix = matrix[:, :24]\n",
    "\n",
    "    #torch.Size([32, 24, 129])\n",
    "    input_mark = torch.arange(0, input_matrix.size(1)).unsqueeze(0).repeat(input_matrix.size(0), 1).cuda()\n",
    "    #torch.Size([32, 24])\n",
    "    input_mask = input_matrix[:, :24, V:]\n",
    "    output_matrix = matrix[:, 24:, :V]\n",
    "    #torch.Size([32, 24, 129])\n",
    "    output_mark = torch.arange(0, output_matrix.size(1)).unsqueeze(0).repeat(input_matrix.size(0), 1).cuda()\n",
    "    #torch.Size([32, 24])\n",
    "    output_mask = matrix[:, 24:, V:]\n",
    "    #torch.Size([32, 24, 129])\n",
    "    \n",
    "    output_mark = torch.arange(0, output_matrix.size(1)).unsqueeze(0).repeat(input_matrix.size(0), 1).cuda()\n",
    "\n",
    "    dec_inp = torch.zeros_like(output_matrix[:, -args.pred_len:, :]).float()\n",
    "    with torch.no_grad():\n",
    "        output = model(input_matrix, input_mark, dec_inp, output_mark)\n",
    "        loss = output_mask[:, -args.pred_len:, :]*(\n",
    "        output-output_matrix[:, -args.pred_len:, :])**2\n",
    "    loss_list.extend(loss.sum(axis=-1).mean(axis=-1).detach().cpu().tolist())\n",
    "    for outpu, real, mask, input_mat in zip(output, output_matrix, output_mask, input_matrix):\n",
    "        a = (get_sofa(outpu*mask, var_to_ind))\n",
    "        b = (get_sofa(real, var_to_ind))\n",
    "        c = (get_sofa(input_mat, var_to_ind)) #SOFA Source (First Day)\n",
    "        prediction_sofa_difference = (sum(a)-sum(c)) >= 2\n",
    "        gold_sofa_difference = (sum(b)-sum(c)) >= 2\n",
    "        accuracy_list.append(prediction_sofa_difference == gold_sofa_difference)\n",
    "        if gold_sofa_difference:\n",
    "            if prediction_sofa_difference: TP +=1\n",
    "            else: FP += 1\n",
    "        else:\n",
    "            if prediction_sofa_difference: TN +=1\n",
    "            else: FN += 1\n",
    "        c, d = torch.tensor(a, dtype=torch.float32), torch.tensor(b, dtype=torch.float32)\n",
    "        sofa_losses.append(((c-d)**2).sum(axis=-1).mean().item())\n",
    "    loss_old = loss\n",
    "    loss = loss.sum(axis=-1).mean(axis=-1).sum()\n",
    "    loss_part1 = loss_old[:, :8].sum(axis=-1).mean()\n",
    "    loss_part2 = loss_old[:, 8:].sum(axis=-1).mean()\n",
    "    test_loss+=loss\n",
    "    test_loss_part1+=loss_part1\n",
    "    test_loss_part2+=loss_part2\n",
    "    test_loss_sofa_variables+=loss_old[:, :, factor_indices].sum(axis=-1).mean()\n",
    "    test_loss_sum_square_total += torch.sum(loss_old.sum(axis=-1).mean(axis=-1)**2)\n",
    "    test_loss_part1_sum_square_total += torch.sum(loss_old[:, :8].sum(axis=-1).mean(axis=-1)**2)\n",
    "    test_loss_part2_sum_square_total += torch.sum(loss_old[:, 8:].sum(axis=-1).mean(axis=-1)**2)\n",
    "    test_loss_sofa_sum_square_total += torch.sum(loss_old[:, :, factor_indices].sum(axis=-1).mean(axis=-1)**2)\n",
    "test_loss_p = test_loss/len(test_input)\n",
    "se_loss_p = torch.sqrt(test_loss_sum_square_total / len(test_input) - test_loss_p**2)/len(test_input)\n",
    "test_loss_part1_p = test_loss_part1*batch_size/len(test_input)\n",
    "test_loss_part2_p = test_loss_part2*batch_size/len(test_input)\n",
    "test_loss_sofa_variables_p = test_loss_sofa_variables*batch_size/len(test_input)\n",
    "sofa_loss = sum(sofa_losses)/len(sofa_losses)\n",
    "accuracy_sepsis = sum(accuracy_list)/len(accuracy_list)\n",
    "se_accuracy_sepsis = np.sqrt(accuracy_sepsis * (1-accuracy_sepsis)/len(test_input))\n",
    "se_loss_part1_p = torch.sqrt(test_loss_part1_sum_square_total / len(test_input) - test_loss_part1_p**2)/len(test_input)\n",
    "se_loss_part2_p = torch.sqrt(test_loss_part2_sum_square_total / len(test_input) - test_loss_part2_p**2)/len(test_input)\n",
    "se_loss_sofa_p  = torch.sqrt(test_loss_sofa_sum_square_total / len(test_input)  - test_loss_sofa_variables_p**2)/len(test_input)\n",
    "print(test_loss_p, test_loss_part1_p, test_loss_part2_p, sofa_loss, test_loss_sofa_variables_p, accuracy_sepsis)\n",
    "\n",
    "model_name = '7&Dense&DMS&-&-&'\n",
    "MSE_string = \"{:.3f}\".format(test_loss_p.item()) + \"$_{[\" + \"{:.3f}\".format(test_loss_p.item()-1.96*se_loss_p.item()) +','+ \"{:.3f}\".format(test_loss_p.item()+1.96*se_loss_p.item()) + ']}$&'\n",
    "SOFA_string = \"{:.3f}\".format(sofa_loss)+ '$_{['+\"{:.3f}\".format(sofa_loss-1.96*se_loss_sofa_p.item())+','+\"{:.3f}\".format(sofa_loss+1.96*se_loss_sofa_p.item())+ ']}$&'\n",
    "MSE8_string = \"{:.3f}\".format(test_loss_part1_p.item()) + \"$_{[\" + \"{:.3f}\".format(test_loss_part1_p.item()-1.96*se_loss_part1_p.item()) +','+ \"{:.3f}\".format(test_loss_part1_p.item()+1.96*se_loss_part1_p.item()) + ']}$&'\n",
    "MSE16_string = \"{:.3f}\".format(test_loss_part2_p.item()) + \"$_{[\" + \"{:.3f}\".format(test_loss_part2_p.item()-1.96*se_loss_part2_p.item()) +','+ \"{:.3f}\".format(test_loss_part2_p.item()+1.96*se_loss_part2_p.item()) + ']}$&'\n",
    "SOFA_acc_string = \"{:.2f}\".format(accuracy_sepsis*100)+ '$_{['+\"{:.2f}\".format(100*(accuracy_sepsis-1.96*se_accuracy_sepsis))+','+\"{:.2f}\".format(100*(accuracy_sepsis+1.96*se_accuracy_sepsis))+ ']}$&'\n",
    "print(model_name,MSE_string, SOFA_string, sep='',file=open('eicu_table_sample_sepsis1.txt','a'))\n",
    "print(model_name,MSE8_string, MSE16_string, sep='',file=open('eicu_table_sample_sepsis2.txt','a'))\n",
    "print(model_name,TP,'&',TN,'&',FP,'&',FN,'&', TP+TN+FP+FN,'&', len(test_input), sep='',file=open('eicu_table_sample_sepsis3.txt','a')) \n",
    "print(model_name,len(test_input),'&',SOFA_acc_string, sep='',file=open('eicu_table_sample_sepsis4.txt','a'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a553eb",
   "metadata": {},
   "source": [
    "# 10 Sofa Analysis IMS Informer Backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12355c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import models.InformerAutoregressiveFull as autoformer\n",
    "importlib.reload(autoformer)\n",
    "batch_size, lr, patience = 32, 0.0005, 6  # batch_size increased, patience 10 --> 6\n",
    "d, N, he, dropout = 50, 2, 4, 0.2\n",
    "model = autoformer.Model(args).cuda()\n",
    "# Load pretrained weights here.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'max', patience=3)\n",
    "model.load_state_dict(torch.load(\"/home/mitarb/fracarolli/eicu/240401_Experimente/eicu_mod10_Dense_IMS_SF_BP.pytorch\"))\n",
    "model.eval()\n",
    "y_true, y_pred = [], []\n",
    "print(test_input)\n",
    "pbar = tqdm(range(0, len(test_input), batch_size))\n",
    "loss_list = []\n",
    "test_loss = 0\n",
    "test_loss_part1 = 0\n",
    "test_loss_part2 = 0\n",
    "sofa_losses = []\n",
    "test_loss_sofa_variables = 0\n",
    "accuracy_list = []\n",
    "TP, TN, FP, FN = 0, 0, 0, 0\n",
    "test_loss_sum_square_total = 0\n",
    "test_loss_part1_sum_square_total = 0\n",
    "test_loss_part2_sum_square_total = 0\n",
    "test_loss_sofa_sum_square_total = 0\n",
    "for start in pbar:\n",
    "    print(start)\n",
    "    matrix = torch.tensor(test_input[start:start+batch_size], dtype=torch.float32).cuda()\n",
    "    input_matrix = matrix[:, :24]\n",
    "\n",
    "    #torch.Size([32, 24, 129])\n",
    "    input_mark = torch.arange(0, input_matrix.size(1)).unsqueeze(0).repeat(input_matrix.size(0), 1).cuda()\n",
    "    #torch.Size([32, 24])\n",
    "    input_mask = input_matrix[:, :24, V:]\n",
    "    output_matrix = matrix[:, 24:, :V]\n",
    "    #torch.Size([32, 24, 129])\n",
    "    output_mark = torch.arange(0, output_matrix.size(1)).unsqueeze(0).repeat(input_matrix.size(0), 1).cuda()\n",
    "    #torch.Size([32, 24])\n",
    "    output_mask = matrix[:, 24:, V:]\n",
    "    #torch.Size([32, 24, 129])\n",
    "    output_mark = torch.arange(0, output_matrix.size(1)).unsqueeze(0).repeat(input_matrix.size(0), 1).cuda()\n",
    "    dec_inp = torch.zeros_like(output_matrix[:, -args.pred_len:, :]).float()\n",
    "    with torch.no_grad():\n",
    "        output = model(input_matrix, input_mark, dec_inp, output_mark, trainn=False)\n",
    "        loss = output_mask[:, -args.pred_len:, :]*(\n",
    "        output-output_matrix[:, -args.pred_len:, :])**2\n",
    "    loss_list.extend(loss.sum(axis=-1).mean(axis=-1).detach().cpu().tolist())\n",
    "    for outpu, real, mask, input_mat in zip(output, output_matrix, output_mask, input_matrix):\n",
    "        a = (get_sofa(outpu*mask, var_to_ind))\n",
    "        b = (get_sofa(real, var_to_ind))\n",
    "        c = (get_sofa(input_mat, var_to_ind)) #SOFA Source (First Day)\n",
    "        prediction_sofa_difference = (sum(a)-sum(c)) >= 2\n",
    "        gold_sofa_difference = (sum(b)-sum(c)) >= 2\n",
    "        accuracy_list.append(prediction_sofa_difference == gold_sofa_difference)\n",
    "        if gold_sofa_difference:\n",
    "            if prediction_sofa_difference: TP +=1\n",
    "            else: FP += 1\n",
    "        else:\n",
    "            if prediction_sofa_difference: TN +=1\n",
    "            else: FN += 1\n",
    "        c, d = torch.tensor(a, dtype=torch.float32), torch.tensor(b, dtype=torch.float32)\n",
    "        sofa_losses.append(((c-d)**2).sum(axis=-1).mean().item())\n",
    "    loss_old = loss\n",
    "    loss = loss.sum(axis=-1).mean(axis=-1).sum()\n",
    "    loss_part1 = loss_old[:, :8].sum(axis=-1).mean()\n",
    "    loss_part2 = loss_old[:, 8:].sum(axis=-1).mean()\n",
    "    test_loss+=loss\n",
    "    test_loss_part1+=loss_part1\n",
    "    test_loss_part2+=loss_part2\n",
    "    test_loss_sofa_variables+=loss_old[:, :, factor_indices].sum(axis=-1).mean()\n",
    "    test_loss_sum_square_total += torch.sum(loss_old.sum(axis=-1).mean(axis=-1)**2)\n",
    "    test_loss_part1_sum_square_total += torch.sum(loss_old[:, :8].sum(axis=-1).mean(axis=-1)**2)\n",
    "    test_loss_part2_sum_square_total += torch.sum(loss_old[:, 8:].sum(axis=-1).mean(axis=-1)**2)\n",
    "    test_loss_sofa_sum_square_total += torch.sum(loss_old[:, :, factor_indices].sum(axis=-1).mean(axis=-1)**2)\n",
    "test_loss_p = test_loss/len(test_input)\n",
    "se_loss_p = torch.sqrt(test_loss_sum_square_total / len(test_input) - test_loss_p**2)/len(test_input)\n",
    "test_loss_part1_p = test_loss_part1*batch_size/len(test_input)\n",
    "test_loss_part2_p = test_loss_part2*batch_size/len(test_input)\n",
    "test_loss_sofa_variables_p = test_loss_sofa_variables*batch_size/len(test_input)\n",
    "sofa_loss = sum(sofa_losses)/len(sofa_losses)\n",
    "accuracy_sepsis = sum(accuracy_list)/len(accuracy_list)\n",
    "se_accuracy_sepsis = np.sqrt(accuracy_sepsis * (1-accuracy_sepsis)/len(test_input))\n",
    "se_loss_part1_p = torch.sqrt(test_loss_part1_sum_square_total / len(test_input) - test_loss_part1_p**2)/len(test_input)\n",
    "se_loss_part2_p = torch.sqrt(test_loss_part2_sum_square_total / len(test_input) - test_loss_part2_p**2)/len(test_input)\n",
    "se_loss_sofa_p  = torch.sqrt(test_loss_sofa_sum_square_total / len(test_input)  - test_loss_sofa_variables_p**2)/len(test_input)\n",
    "print(test_loss_p, test_loss_part1_p, test_loss_part2_p, sofa_loss, test_loss_sofa_variables_p, accuracy_sepsis)\n",
    "print('Informer_IMS_SF_B',test_loss_p.item(), test_loss_part1_p.item(), test_loss_part2_p.item(), sofa_loss, test_loss_sofa_variables_p.item(), accuracy_sepsis, sep=',',file=open('results.txt','a'))\n",
    "\n",
    "model_name = '10&Dense&IMS&SF&Yes&'\n",
    "MSE_string = \"{:.3f}\".format(test_loss_p.item()) + \"$_{[\" + \"{:.3f}\".format(test_loss_p.item()-1.96*se_loss_p.item()) +','+ \"{:.3f}\".format(test_loss_p.item()+1.96*se_loss_p.item()) + ']}$&'\n",
    "SOFA_string = \"{:.3f}\".format(sofa_loss)+ '$_{['+\"{:.3f}\".format(sofa_loss-1.96*se_loss_sofa_p.item())+','+\"{:.3f}\".format(sofa_loss+1.96*se_loss_sofa_p.item())+ ']}$&'\n",
    "MSE8_string = \"{:.3f}\".format(test_loss_part1_p.item()) + \"$_{[\" + \"{:.3f}\".format(test_loss_part1_p.item()-1.96*se_loss_part1_p.item()) +','+ \"{:.3f}\".format(test_loss_part1_p.item()+1.96*se_loss_part1_p.item()) + ']}$&'\n",
    "MSE16_string = \"{:.3f}\".format(test_loss_part2_p.item()) + \"$_{[\" + \"{:.3f}\".format(test_loss_part2_p.item()-1.96*se_loss_part2_p.item()) +','+ \"{:.3f}\".format(test_loss_part2_p.item()+1.96*se_loss_part2_p.item()) + ']}$&'\n",
    "SOFA_acc_string = \"{:.2f}\".format(accuracy_sepsis*100)+ '$_{['+\"{:.2f}\".format(100*(accuracy_sepsis-1.96*se_accuracy_sepsis))+','+\"{:.2f}\".format(100*(accuracy_sepsis+1.96*se_accuracy_sepsis))+ ']}$&'\n",
    "print(model_name,MSE_string, SOFA_string, sep='',file=open('eicu_table_sample_sepsis1.txt','a'))\n",
    "print(model_name,MSE8_string, MSE16_string, sep='',file=open('eicu_table_sample_sepsis2.txt','a'))\n",
    "print(model_name,TP,'&',TN,'&',FP,'&',FN,'&', TP+TN+FP+FN,'&', len(test_input), sep='',file=open('eicu_table_sample_sepsis3.txt','a')) \n",
    "print(model_name,len(test_input),'&',SOFA_acc_string, sep='',file=open('eicu_table_sample_sepsis4.txt','a'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38621f04",
   "metadata": {},
   "source": [
    "# 9 Sofa Analysis IMS Informer Detach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87034366",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import models.InformerAutoregressiveFull as autoformer\n",
    "importlib.reload(autoformer)\n",
    "batch_size, lr, patience = 32, 0.0005, 6  # batch_size increased, patience 10 --> 6\n",
    "d, N, he, dropout = 50, 2, 4, 0.2\n",
    "model = autoformer.Model(args).cuda()\n",
    "# Load pretrained weights here.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "#fore_savepath = 'mimic_iii_24h_strats_no_interp_with_ss_fore_blubdiblub.pytorch'\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'max', patience=3)\n",
    "model.load_state_dict(torch.load(\"/home/mitarb/fracarolli/eicu/240401_Experimente/eicu_mod09_Dense_IMS_SF_D.pytorch\"))\n",
    "#model.load_state_dict(torch.load(fore_savepath))\n",
    "model.eval()\n",
    "y_true, y_pred = [], []\n",
    "print(test_input)\n",
    "pbar = tqdm(range(0, len(test_input), batch_size))\n",
    "loss_list = []\n",
    "test_loss = 0\n",
    "test_loss_part1 = 0\n",
    "test_loss_part2 = 0\n",
    "test_loss_sofa_variables = 0\n",
    "sofa_losses = []\n",
    "accuracy_list = []\n",
    "TP, TN, FP, FN = 0, 0, 0, 0\n",
    "test_loss_sum_square_total = 0\n",
    "test_loss_part1_sum_square_total = 0\n",
    "test_loss_part2_sum_square_total = 0\n",
    "test_loss_sofa_sum_square_total = 0\n",
    "for start in pbar:\n",
    "    print(start)\n",
    "    matrix = torch.tensor(test_input[start:start+batch_size], dtype=torch.float32).cuda()\n",
    "    input_matrix = matrix[:, :24]\n",
    "\n",
    "    #torch.Size([32, 24, 129])\n",
    "    input_mark = torch.arange(0, input_matrix.size(1)).unsqueeze(0).repeat(input_matrix.size(0), 1).cuda()\n",
    "    #torch.Size([32, 24])\n",
    "    input_mask = input_matrix[:, :24, V:]\n",
    "    output_matrix = matrix[:, 24:, :V]\n",
    "    #torch.Size([32, 24, 129])\n",
    "    output_mark = torch.arange(0, output_matrix.size(1)).unsqueeze(0).repeat(input_matrix.size(0), 1).cuda()\n",
    "    #torch.Size([32, 24])\n",
    "    output_mask = matrix[:, 24:, V:]\n",
    "    \n",
    "    output_mark = torch.arange(0, output_matrix.size(1)).unsqueeze(0).repeat(input_matrix.size(0), 1).cuda()\n",
    "    dec_inp = torch.zeros_like(output_matrix[:, -args.pred_len:, :]).float()\n",
    "    #torch.Size([32, 24, 129])\n",
    "    with torch.no_grad():\n",
    "        output = model(input_matrix, input_mark, dec_inp, output_mark, trainn=False)\n",
    "        loss = output_mask[:, -args.pred_len:, :]*(\n",
    "        output-output_matrix[:, -args.pred_len:, :])**2\n",
    "    loss_list.extend(loss.sum(axis=-1).mean(axis=-1).detach().cpu().tolist())\n",
    "    for outpu, real, mask, input_mat in zip(output, output_matrix, output_mask, input_matrix):\n",
    "        a = (get_sofa(outpu*mask, var_to_ind))\n",
    "        b = (get_sofa(real, var_to_ind))\n",
    "        c = (get_sofa(input_mat, var_to_ind)) #SOFA Source (First Day)\n",
    "        prediction_sofa_difference = (sum(a)-sum(c)) >= 2\n",
    "        gold_sofa_difference = (sum(b)-sum(c)) >= 2\n",
    "        accuracy_list.append(prediction_sofa_difference == gold_sofa_difference)\n",
    "        if gold_sofa_difference:\n",
    "            if prediction_sofa_difference: TP +=1\n",
    "            else: FP += 1\n",
    "        else:\n",
    "            if prediction_sofa_difference: TN +=1\n",
    "            else: FN += 1\n",
    "        c, d = torch.tensor(a, dtype=torch.float32), torch.tensor(b, dtype=torch.float32)\n",
    "        sofa_losses.append(((c-d)**2).sum(axis=-1).mean().item())\n",
    "    loss_old = loss\n",
    "    loss = loss.sum(axis=-1).mean(axis=-1).sum()\n",
    "    loss_part1 = loss_old[:, :8].sum(axis=-1).mean()\n",
    "    loss_part2 = loss_old[:, 8:].sum(axis=-1).mean()\n",
    "    test_loss+=loss\n",
    "    test_loss_part1+=loss_part1\n",
    "    test_loss_part2+=loss_part2\n",
    "    test_loss_sofa_variables+=loss_old[:, :, factor_indices].sum(axis=-1).mean()\n",
    "    test_loss_sum_square_total += torch.sum(loss_old.sum(axis=-1).mean(axis=-1)**2)\n",
    "    test_loss_part1_sum_square_total += torch.sum(loss_old[:, :8].sum(axis=-1).mean(axis=-1)**2)\n",
    "    test_loss_part2_sum_square_total += torch.sum(loss_old[:, 8:].sum(axis=-1).mean(axis=-1)**2)\n",
    "    test_loss_sofa_sum_square_total += torch.sum(loss_old[:, :, factor_indices].sum(axis=-1).mean(axis=-1)**2)\n",
    "test_loss_p = test_loss/len(test_input)\n",
    "se_loss_p = torch.sqrt(test_loss_sum_square_total / len(test_input) - test_loss_p**2)/len(test_input)\n",
    "test_loss_part1_p = test_loss_part1*batch_size/len(test_input)\n",
    "test_loss_part2_p = test_loss_part2*batch_size/len(test_input)\n",
    "test_loss_sofa_variables_p = test_loss_sofa_variables*batch_size/len(test_input)\n",
    "sofa_loss = sum(sofa_losses)/len(sofa_losses)\n",
    "accuracy_sepsis = sum(accuracy_list)/len(accuracy_list)\n",
    "se_accuracy_sepsis = np.sqrt(accuracy_sepsis * (1-accuracy_sepsis)/len(test_input))\n",
    "se_loss_part1_p = torch.sqrt(test_loss_part1_sum_square_total / len(test_input) - test_loss_part1_p**2)/len(test_input)\n",
    "se_loss_part2_p = torch.sqrt(test_loss_part2_sum_square_total / len(test_input) - test_loss_part2_p**2)/len(test_input)\n",
    "se_loss_sofa_p  = torch.sqrt(test_loss_sofa_sum_square_total / len(test_input)  - test_loss_sofa_variables_p**2)/len(test_input)\n",
    "print(test_loss_p, test_loss_part1_p, test_loss_part2_p, sofa_loss, test_loss_sofa_variables_p, accuracy_sepsis)\n",
    "print('Informer_IMS_SF_D',test_loss_p.item(), test_loss_part1_p.item(), test_loss_part2_p.item(), sofa_loss, test_loss_sofa_variables_p.item(), accuracy_sepsis, sep=',',file=open('results.txt','a'))\n",
    "\n",
    "\n",
    "\n",
    "model_name = '9&Dense&IMS&SF&No&'\n",
    "MSE_string = \"{:.3f}\".format(test_loss_p.item()) + \"$_{[\" + \"{:.3f}\".format(test_loss_p.item()-1.96*se_loss_p.item()) +','+ \"{:.3f}\".format(test_loss_p.item()+1.96*se_loss_p.item()) + ']}$&'\n",
    "SOFA_string = \"{:.3f}\".format(sofa_loss)+ '$_{['+\"{:.3f}\".format(sofa_loss-1.96*se_loss_sofa_p.item())+','+\"{:.3f}\".format(sofa_loss+1.96*se_loss_sofa_p.item())+ ']}$&'\n",
    "MSE8_string = \"{:.3f}\".format(test_loss_part1_p.item()) + \"$_{[\" + \"{:.3f}\".format(test_loss_part1_p.item()-1.96*se_loss_part1_p.item()) +','+ \"{:.3f}\".format(test_loss_part1_p.item()+1.96*se_loss_part1_p.item()) + ']}$&'\n",
    "MSE16_string = \"{:.3f}\".format(test_loss_part2_p.item()) + \"$_{[\" + \"{:.3f}\".format(test_loss_part2_p.item()-1.96*se_loss_part2_p.item()) +','+ \"{:.3f}\".format(test_loss_part2_p.item()+1.96*se_loss_part2_p.item()) + ']}$&'\n",
    "SOFA_acc_string = \"{:.2f}\".format(accuracy_sepsis*100)+ '$_{['+\"{:.2f}\".format(100*(accuracy_sepsis-1.96*se_accuracy_sepsis))+','+\"{:.2f}\".format(100*(accuracy_sepsis+1.96*se_accuracy_sepsis))+ ']}$&'\n",
    "print(model_name,MSE_string, SOFA_string, sep='',file=open('eicu_table_sample_sepsis1.txt','a'))\n",
    "print(model_name,MSE8_string, MSE16_string, sep='',file=open('eicu_table_sample_sepsis2.txt','a'))\n",
    "print(model_name,TP,'&',TN,'&',FP,'&',FN,'&', TP+TN+FP+FN,'&', len(test_input), sep='',file=open('eicu_table_sample_sepsis3.txt','a')) \n",
    "print(model_name,len(test_input),'&',SOFA_acc_string, sep='',file=open('eicu_table_sample_sepsis4.txt','a'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07239a61",
   "metadata": {},
   "source": [
    "# Autof SOFA Analysis Autoformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfc91fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Michi: die ganzen Argparses kÃ¶nnte man noch anders implementieren, wollte es nur schnell zum laufen bringen.\n",
    "import argparse\n",
    "parser = argparse.ArgumentParser(description='Autoformer & Transformer family for Time Series Forecasting')\n",
    "\n",
    "parser.add_argument('--is_training', type=int, required=False, default=1, help='status')\n",
    "parser.add_argument('--train_only', type=bool, required=False, default=False, help='perform training on full input dataset without validation and testing')\n",
    "parser.add_argument('--model_id', type=str, required=False, default='test', help='model id')\n",
    "parser.add_argument('--model', type=str, required=False, default='Autoformer',\n",
    "                    help='model name, options: [Autoformer, Informer, Transformer]')\n",
    "\n",
    "# data loader\n",
    "parser.add_argument('--data', type=str, required=False, default='ETTm1', help='dataset type')\n",
    "parser.add_argument('--root_path', type=str, default='./data/ETT/', help='root path of the data file')\n",
    "parser.add_argument('--data_path', type=str, default='ETTh1.csv', help='data file')\n",
    "parser.add_argument('--features', type=str, default='M',\n",
    "                    help='forecasting task, options:[M, S, MS]; M:multivariate predict multivariate, S:univariate predict univariate, MS:multivariate predict univariate')\n",
    "parser.add_argument('--target', type=str, default='OT', help='target feature in S or MS task')\n",
    "parser.add_argument('--freq', type=str, default='h',\n",
    "                    help='freq for time features encoding, options:[s:secondly, t:minutely, h:hourly, d:daily, b:business days, w:weekly, m:monthly], you can also use more detailed freq like 15min or 3h')\n",
    "parser.add_argument('--checkpoints', type=str, default='./checkpoints/', help='location of model checkpoints')\n",
    "\n",
    "# forecasting task\n",
    "parser.add_argument('--seq_len', type=int, default=24, help='input sequence length')\n",
    "parser.add_argument('--label_len', type=int, default=0, help='start token length')\n",
    "parser.add_argument('--pred_len', type=int, default=24, help='prediction sequence length')\n",
    "\n",
    "\n",
    "# DLinear\n",
    "parser.add_argument('--individual', action='store_true', default=False, help='DLinear: a linear layer for each variate(channel) individually')\n",
    "# Formers \n",
    "\n",
    "parser.add_argument('--embed_type', type=int, default=3, help='0: default 1: value embedding + temporal embedding + positional embedding 2: value embedding + temporal embedding 3: value embedding + positional embedding 4: value embedding')\n",
    "parser.add_argument('--enc_in', type=int, default=V, help='encoder input size') # DLinear with --individual, use this hyperparameter as the number of channels\n",
    "parser.add_argument('--dec_in', type=int, default=V, help='decoder input size')\n",
    "parser.add_argument('--c_out', type=int, default=V, help='output size')\n",
    "parser.add_argument('--d_model', type=int, default=512, help='dimension of model')\n",
    "parser.add_argument('--n_heads', type=int, default=8, help='num of heads')\n",
    "parser.add_argument('--e_layers', type=int, default=2, help='num of encoder layers')\n",
    "parser.add_argument('--d_layers', type=int, default=1, help='num of decoder layers')\n",
    "parser.add_argument('--d_ff', type=int, default=2048, help='dimension of fcn')\n",
    "parser.add_argument('--moving_avg', type=int, default=25, help='window size of moving average')\n",
    "parser.add_argument('--factor', type=int, default=1, help='attn factor')\n",
    "parser.add_argument('--distil', action='store_false',\n",
    "                    help='whether to use distilling in encoder, using this argument means not using distilling',\n",
    "                    default=True)\n",
    "parser.add_argument('--dropout', type=float, default=0.05, help='dropout')\n",
    "parser.add_argument('--embed', type=str, default='timeF',\n",
    "                    help='time features encoding, options:[timeF, fixed, learned]')\n",
    "parser.add_argument('--activation', type=str, default='gelu', help='activation')\n",
    "parser.add_argument('--output_attention', action='store_true', help='whether to output attention in ecoder')\n",
    "parser.add_argument('--do_predict', action='store_true', help='whether to predict unseen future data')\n",
    "\n",
    "# optimization\n",
    "parser.add_argument('--num_workers', type=int, default=10, help='data loader num workers')\n",
    "parser.add_argument('--itr', type=int, default=2, help='experiments times')\n",
    "parser.add_argument('--train_epochs', type=int, default=10, help='train epochs')\n",
    "parser.add_argument('--batch_size', type=int, default=32, help='batch size of train input data')\n",
    "parser.add_argument('--patience', type=int, default=3, help='early stopping patience')\n",
    "parser.add_argument('--learning_rate', type=float, default=0.0001, help='optimizer learning rate')\n",
    "parser.add_argument('--des', type=str, default='test', help='exp description')\n",
    "parser.add_argument('--loss', type=str, default='mse', help='loss function')\n",
    "parser.add_argument('--lradj', type=str, default='type1', help='adjust learning rate')\n",
    "parser.add_argument('--use_amp', action='store_true', help='use automatic mixed precision training', default=False)\n",
    "\n",
    "# GPU\n",
    "parser.add_argument('--use_gpu', type=bool, default=True, help='use gpu')\n",
    "parser.add_argument('--gpu', type=int, default=0, help='gpu')\n",
    "parser.add_argument('--use_multi_gpu', action='store_true', help='use multiple gpus', default=False)\n",
    "parser.add_argument('--devices', type=str, default='0,1,2,3', help='device ids of multile gpus')\n",
    "parser.add_argument('--test_flop', action='store_true', default=False, help='See utils/tools for usage')\n",
    "\n",
    "\n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "import importlib\n",
    "\n",
    "\n",
    "import models.Autoformer as autoformer\n",
    "importlib.reload(autoformer)\n",
    "batch_size, lr, patience = 32, 0.0005, 6  # batch_size increased, patience 10 --> 6\n",
    "d, N, he, dropout = 50, 2, 4, 0.2\n",
    "model = autoformer.Model(args).cuda()\n",
    "# Load pretrained weights here.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "#fore_savepath = 'mimic_iii_24h_strats_no_interp_with_ss_fore_blubdiblub.pytorch'\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'max', patience=3)\n",
    "model.load_state_dict(torch.load(\"/home/mitarb/fracarolli/eicu/240401_Experimente/eicu_autoformer.pytorch\"))\n",
    "#model.load_state_dict(torch.load(fore_savepath))\n",
    "model.eval()\n",
    "y_true, y_pred = [], []\n",
    "print(test_input)\n",
    "pbar = tqdm(range(0, len(test_input), batch_size))\n",
    "loss_list = []\n",
    "test_loss = 0\n",
    "test_loss_part1 = 0\n",
    "test_loss_part2 = 0\n",
    "sofa_losses = []\n",
    "test_loss_sofa_variables = 0\n",
    "accuracy_list = []\n",
    "TP, TN, FP, FN = 0, 0, 0, 0\n",
    "test_loss_sum_square_total = 0\n",
    "test_loss_part1_sum_square_total = 0\n",
    "test_loss_part2_sum_square_total = 0\n",
    "test_loss_sofa_sum_square_total = 0\n",
    "for start in pbar:\n",
    "    print(start)\n",
    "    matrix = torch.tensor(test_input[start:start+batch_size], dtype=torch.float32).cuda()\n",
    "    input_matrix = matrix[:, :24]\n",
    "\n",
    "    #torch.Size([32, 24, 129])\n",
    "    input_mark = torch.arange(0, input_matrix.size(1)).unsqueeze(0).repeat(input_matrix.size(0), 1).cuda()\n",
    "    #torch.Size([32, 24])\n",
    "    input_mask = input_matrix[:, :24, V:]\n",
    "    output_matrix = matrix[:, 24:, :V]\n",
    "    #torch.Size([32, 24, 129])\n",
    "    output_mark = torch.arange(0, output_matrix.size(1)).unsqueeze(0).repeat(input_matrix.size(0), 1).cuda()\n",
    "    #torch.Size([32, 24])\n",
    "    output_mask = matrix[:, 24:, V:]\n",
    "    #torch.Size([32, 24, 129])\n",
    "\n",
    "    dec_inp = torch.zeros_like(output_matrix[:, -args.pred_len:, :]).float()\n",
    "    with torch.no_grad():\n",
    "        output = model(input_matrix[:, :, :V], input_mark, dec_inp, output_mark)\n",
    "        loss = output_mask[:, -args.pred_len:, :]*(\n",
    "        output-output_matrix[:, -args.pred_len:, :])**2\n",
    "    loss_list.extend(loss.sum(axis=-1).mean(axis=-1).detach().cpu().tolist())\n",
    "    for outpu, real, mask, input_mat in zip(output, output_matrix, output_mask, input_matrix):\n",
    "        a = (get_sofa(outpu*mask, var_to_ind))\n",
    "        b = (get_sofa(real, var_to_ind))\n",
    "        c = (get_sofa(input_mat, var_to_ind)) #SOFA Source (First Day)\n",
    "        prediction_sofa_difference = (sum(a)-sum(c)) >= 2\n",
    "        gold_sofa_difference = (sum(b)-sum(c)) >= 2\n",
    "        accuracy_list.append(prediction_sofa_difference == gold_sofa_difference)\n",
    "        if gold_sofa_difference:\n",
    "            if prediction_sofa_difference: TP +=1\n",
    "            else: FP += 1\n",
    "        else:\n",
    "            if prediction_sofa_difference: TN +=1\n",
    "            else: FN += 1\n",
    "        c, d = torch.tensor(a, dtype=torch.float32), torch.tensor(b, dtype=torch.float32)\n",
    "        sofa_losses.append(((c-d)**2).sum(axis=-1).mean().item())\n",
    "    loss_old = loss\n",
    "    loss = loss.sum(axis=-1).mean(axis=-1).sum()\n",
    "    loss_part1 = loss_old[:, :8].sum(axis=-1).mean()\n",
    "    loss_part2 = loss_old[:, 8:].sum(axis=-1).mean()\n",
    "    test_loss+=loss\n",
    "    test_loss_part1+=loss_part1\n",
    "    test_loss_part2+=loss_part2\n",
    "    test_loss_sofa_variables+=loss_old[:, :, factor_indices].sum(axis=-1).mean()\n",
    "    test_loss_sum_square_total += torch.sum(loss_old.sum(axis=-1).mean(axis=-1)**2)\n",
    "    test_loss_part1_sum_square_total += torch.sum(loss_old[:, :8].sum(axis=-1).mean(axis=-1)**2)\n",
    "    test_loss_part2_sum_square_total += torch.sum(loss_old[:, 8:].sum(axis=-1).mean(axis=-1)**2)\n",
    "    test_loss_sofa_sum_square_total += torch.sum(loss_old[:, :, factor_indices].sum(axis=-1).mean(axis=-1)**2)\n",
    "test_loss_p = test_loss/len(test_input)\n",
    "se_loss_p = torch.sqrt(test_loss_sum_square_total / len(test_input) - test_loss_p**2)/len(test_input)\n",
    "test_loss_part1_p = test_loss_part1*batch_size/len(test_input)\n",
    "test_loss_part2_p = test_loss_part2*batch_size/len(test_input)\n",
    "test_loss_sofa_variables_p = test_loss_sofa_variables*batch_size/len(test_input)\n",
    "sofa_loss = sum(sofa_losses)/len(sofa_losses)\n",
    "accuracy_sepsis = sum(accuracy_list)/len(accuracy_list)\n",
    "se_accuracy_sepsis = np.sqrt(accuracy_sepsis * (1-accuracy_sepsis)/len(test_input))\n",
    "se_loss_part1_p = torch.sqrt(test_loss_part1_sum_square_total / len(test_input) - test_loss_part1_p**2)/len(test_input)\n",
    "se_loss_part2_p = torch.sqrt(test_loss_part2_sum_square_total / len(test_input) - test_loss_part2_p**2)/len(test_input)\n",
    "se_loss_sofa_p  = torch.sqrt(test_loss_sofa_sum_square_total / len(test_input)  - test_loss_sofa_variables_p**2)/len(test_input)\n",
    "print(test_loss_p, test_loss_part1_p, test_loss_part2_p, sofa_loss, test_loss_sofa_variables_p, accuracy_sepsis)\n",
    "print('Autoformer_DMS',test_loss_p.item(), test_loss_part1_p.item(), test_loss_part2_p.item(), sofa_loss, test_loss_sofa_variables_p.item(), accuracy_sepsis, sep=',',file=open('results.txt','a'))\n",
    "\n",
    "\n",
    "\n",
    "model_name = 'Autoformer&Dense&DMS&-&-&'\n",
    "MSE_string = \"{:.3f}\".format(test_loss_p.item()) + \"$_{[\" + \"{:.3f}\".format(test_loss_p.item()-1.96*se_loss_p.item()) +','+ \"{:.3f}\".format(test_loss_p.item()+1.96*se_loss_p.item()) + ']}$&'\n",
    "SOFA_string = \"{:.3f}\".format(sofa_loss)+ '$_{['+\"{:.3f}\".format(sofa_loss-1.96*se_loss_sofa_p.item())+','+\"{:.3f}\".format(sofa_loss+1.96*se_loss_sofa_p.item())+ ']}$&'\n",
    "MSE8_string = \"{:.3f}\".format(test_loss_part1_p.item()) + \"$_{[\" + \"{:.3f}\".format(test_loss_part1_p.item()-1.96*se_loss_part1_p.item()) +','+ \"{:.3f}\".format(test_loss_part1_p.item()+1.96*se_loss_part1_p.item()) + ']}$&'\n",
    "MSE16_string = \"{:.3f}\".format(test_loss_part2_p.item()) + \"$_{[\" + \"{:.3f}\".format(test_loss_part2_p.item()-1.96*se_loss_part2_p.item()) +','+ \"{:.3f}\".format(test_loss_part2_p.item()+1.96*se_loss_part2_p.item()) + ']}$&'\n",
    "SOFA_acc_string = \"{:.2f}\".format(accuracy_sepsis*100)+ '$_{['+\"{:.2f}\".format(100*(accuracy_sepsis-1.96*se_accuracy_sepsis))+','+\"{:.2f}\".format(100*(accuracy_sepsis+1.96*se_accuracy_sepsis))+ ']}$&'\n",
    "print(model_name,MSE_string, SOFA_string, sep='',file=open('eicu_table_sample_sepsis1.txt','a'))\n",
    "print(model_name,MSE8_string, MSE16_string, sep='',file=open('eicu_table_sample_sepsis2.txt','a'))\n",
    "print(model_name,TP,'&',TN,'&',FP,'&',FN,'&', TP+TN+FP+FN,'&', len(test_input), sep='',file=open('eicu_table_sample_sepsis3.txt','a')) \n",
    "print(model_name,len(test_input),'&',SOFA_acc_string, sep='',file=open('eicu_table_sample_sepsis4.txt','a'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0de2c09",
   "metadata": {},
   "source": [
    "# DLinear SOFA Analysis DLINEAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578ab241",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Michi: die ganzen Argparses kÃ¶nnte man noch anders implementieren, wollte es nur schnell zum laufen bringen.\n",
    "import argparse\n",
    "parser = argparse.ArgumentParser(description='Autoformer & Transformer family for Time Series Forecasting')\n",
    "\n",
    "# basic config\n",
    "parser.add_argument('--is_training', type=int, required=False, default=1, help='status')\n",
    "parser.add_argument('--train_only', type=bool, required=False, default=False, help='perform training on full input dataset without validation and testing')\n",
    "parser.add_argument('--model_id', type=str, required=False, default='test', help='model id')\n",
    "parser.add_argument('--model', type=str, required=False, default='Autoformer',\n",
    "                    help='model name, options: [Autoformer, Informer, Transformer]')\n",
    "\n",
    "# data loader\n",
    "parser.add_argument('--data', type=str, required=False, default='ETTm1', help='dataset type')\n",
    "parser.add_argument('--root_path', type=str, default='./data/ETT/', help='root path of the data file')\n",
    "parser.add_argument('--data_path', type=str, default='ETTh1.csv', help='data file')\n",
    "parser.add_argument('--features', type=str, default='M',\n",
    "                    help='forecasting task, options:[M, S, MS]; M:multivariate predict multivariate, S:univariate predict univariate, MS:multivariate predict univariate')\n",
    "parser.add_argument('--target', type=str, default='OT', help='target feature in S or MS task')\n",
    "parser.add_argument('--freq', type=str, default='h',\n",
    "                    help='freq for time features encoding, options:[s:secondly, t:minutely, h:hourly, d:daily, b:business days, w:weekly, m:monthly], you can also use more detailed freq like 15min or 3h')\n",
    "parser.add_argument('--checkpoints', type=str, default='./checkpoints/', help='location of model checkpoints')\n",
    "\n",
    "# forecasting task\n",
    "parser.add_argument('--seq_len', type=int, default=24, help='input sequence length')\n",
    "parser.add_argument('--label_len', type=int, default=0, help='start token length')\n",
    "parser.add_argument('--pred_len', type=int, default=24, help='prediction sequence length')\n",
    "\n",
    "\n",
    "# DLinear\n",
    "parser.add_argument('--individual', action='store_true', default=False, help='DLinear: a linear layer for each variate(channel) individually')\n",
    "# Formers \n",
    "#Michi: bisher nur default=3 zum laufen gebracht\n",
    "parser.add_argument('--embed_type', type=int, default=3, help='0: default 1: value embedding + temporal embedding + positional embedding 2: value embedding + temporal embedding 3: value embedding + positional embedding 4: value embedding')\n",
    "parser.add_argument('--enc_in', type=int, default=262, help='encoder input size') # DLinear with --individual, use this hyperparameter as the number of channels\n",
    "parser.add_argument('--dec_in', type=int, default=V, help='decoder input size')\n",
    "parser.add_argument('--c_out', type=int, default=V, help='output size')\n",
    "parser.add_argument('--d_model', type=int, default=512, help='dimension of model')\n",
    "parser.add_argument('--n_heads', type=int, default=8, help='num of heads')\n",
    "parser.add_argument('--e_layers', type=int, default=2, help='num of encoder layers')\n",
    "parser.add_argument('--d_layers', type=int, default=1, help='num of decoder layers')\n",
    "parser.add_argument('--d_ff', type=int, default=2048, help='dimension of fcn')\n",
    "parser.add_argument('--moving_avg', type=int, default=25, help='window size of moving average')\n",
    "parser.add_argument('--factor', type=int, default=1, help='attn factor')\n",
    "parser.add_argument('--distil', action='store_false',\n",
    "                    help='whether to use distilling in encoder, using this argument means not using distilling',\n",
    "                    default=True)\n",
    "parser.add_argument('--dropout', type=float, default=0.05, help='dropout')\n",
    "parser.add_argument('--embed', type=str, default='timeF',\n",
    "                    help='time features encoding, options:[timeF, fixed, learned]')\n",
    "parser.add_argument('--activation', type=str, default='gelu', help='activation')\n",
    "parser.add_argument('--output_attention', action='store_true', help='whether to output attention in ecoder')\n",
    "parser.add_argument('--do_predict', action='store_true', help='whether to predict unseen future data')\n",
    "\n",
    "# optimization\n",
    "parser.add_argument('--num_workers', type=int, default=10, help='data loader num workers')\n",
    "parser.add_argument('--itr', type=int, default=2, help='experiments times')\n",
    "parser.add_argument('--train_epochs', type=int, default=10, help='train epochs')\n",
    "parser.add_argument('--batch_size', type=int, default=32, help='batch size of train input data')\n",
    "parser.add_argument('--patience', type=int, default=3, help='early stopping patience')\n",
    "parser.add_argument('--learning_rate', type=float, default=0.0001, help='optimizer learning rate')\n",
    "parser.add_argument('--des', type=str, default='test', help='exp description')\n",
    "parser.add_argument('--loss', type=str, default='mse', help='loss function')\n",
    "parser.add_argument('--lradj', type=str, default='type1', help='adjust learning rate')\n",
    "parser.add_argument('--use_amp', action='store_true', help='use automatic mixed precision training', default=False)\n",
    "\n",
    "# GPU\n",
    "parser.add_argument('--use_gpu', type=bool, default=True, help='use gpu')\n",
    "parser.add_argument('--gpu', type=int, default=0, help='gpu')\n",
    "parser.add_argument('--use_multi_gpu', action='store_true', help='use multiple gpus', default=False)\n",
    "parser.add_argument('--devices', type=str, default='0,1,2,3', help='device ids of multile gpus')\n",
    "parser.add_argument('--test_flop', action='store_true', default=False, help='See utils/tools for usage')\n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "import importlib\n",
    "\n",
    "\n",
    "\n",
    "import models.DLinear as autoformer\n",
    "importlib.reload(autoformer)\n",
    "batch_size, lr, patience = 32, 0.0005, 6  # batch_size increased, patience 10 --> 6\n",
    "d, N, he, dropout = 50, 2, 4, 0.2\n",
    "model = autoformer.Model(args).cuda()\n",
    "# Load pretrained weights here.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'max', patience=3)\n",
    "model.load_state_dict(torch.load(\"/home/mitarb/fracarolli/eicu/240401_Experimente/eicu_dlinear.pytorch\"))\n",
    "model.eval()\n",
    "y_true, y_pred = [], []\n",
    "print(test_input)\n",
    "pbar = tqdm(range(0, len(test_input), batch_size))\n",
    "loss_list = []\n",
    "test_loss = 0\n",
    "test_loss_part1 = 0\n",
    "test_loss_part2 = 0\n",
    "sofa_losses = []\n",
    "test_loss_sofa_variables = 0\n",
    "accuracy_list = []\n",
    "TP, TN, FP, FN = 0, 0, 0, 0\n",
    "test_loss_sum_square_total = 0\n",
    "test_loss_part1_sum_square_total = 0\n",
    "test_loss_part2_sum_square_total = 0\n",
    "test_loss_sofa_sum_square_total = 0\n",
    "for start in pbar:\n",
    "    print(start)\n",
    "    matrix = torch.tensor(test_input[start:start+batch_size], dtype=torch.float32).cuda()\n",
    "    input_matrix = matrix[:, :24, :V]\n",
    "\n",
    "    #torch.Size([32, 24, 129])\n",
    "    input_mark = torch.arange(0, input_matrix.size(1)).unsqueeze(0).repeat(input_matrix.size(0), 1).cuda()\n",
    "    #torch.Size([32, 24])\n",
    "    input_mask = input_matrix[:, :24, V:]\n",
    "    output_matrix = matrix[:, 24:, :V]\n",
    "    #torch.Size([32, 24, 129])\n",
    "    output_mark = torch.arange(0, output_matrix.size(1)).unsqueeze(0).repeat(input_matrix.size(0), 1).cuda()\n",
    "    #torch.Size([32, 24])\n",
    "    output_mask = matrix[:, 24:, V:]\n",
    "    #torch.Size([32, 24, 129])\n",
    "    with torch.no_grad():\n",
    "        output = model(input_matrix)\n",
    "        loss = output_mask[:, -args.pred_len:, :]*(\n",
    "        output-output_matrix[:, -args.pred_len:, :])**2\n",
    "    loss_list.extend(loss.sum(axis=-1).mean(axis=-1).detach().cpu().tolist())\n",
    "    for outpu, real, mask, input_mat in zip(output, output_matrix, output_mask, input_matrix):\n",
    "        a = (get_sofa(outpu*mask, var_to_ind))\n",
    "        b = (get_sofa(real, var_to_ind))\n",
    "        c = (get_sofa(input_mat, var_to_ind)) #SOFA Source (First Day)\n",
    "        prediction_sofa_difference = (sum(a)-sum(c)) >= 2\n",
    "        gold_sofa_difference = (sum(b)-sum(c)) >= 2\n",
    "        accuracy_list.append(prediction_sofa_difference == gold_sofa_difference)\n",
    "        if gold_sofa_difference:\n",
    "            if prediction_sofa_difference: TP +=1\n",
    "            else: FP += 1\n",
    "        else:\n",
    "            if prediction_sofa_difference: TN +=1\n",
    "            else: FN += 1\n",
    "        c, d = torch.tensor(a, dtype=torch.float32), torch.tensor(b, dtype=torch.float32)\n",
    "        sofa_losses.append(((c-d)**2).sum(axis=-1).mean().item())\n",
    "    loss_old = loss\n",
    "    loss = loss.sum(axis=-1).mean(axis=-1).sum()\n",
    "    loss_part1 = loss_old[:, :8].sum(axis=-1).mean()\n",
    "    loss_part2 = loss_old[:, 8:].sum(axis=-1).mean()\n",
    "    test_loss+=loss\n",
    "    test_loss_part1+=loss_part1\n",
    "    test_loss_part2+=loss_part2\n",
    "    test_loss_sofa_variables+=loss_old[:, :, factor_indices].sum(axis=-1).mean()\n",
    "    test_loss_sum_square_total += torch.sum(loss_old.sum(axis=-1).mean(axis=-1)**2)\n",
    "    test_loss_part1_sum_square_total += torch.sum(loss_old[:, :8].sum(axis=-1).mean(axis=-1)**2)\n",
    "    test_loss_part2_sum_square_total += torch.sum(loss_old[:, 8:].sum(axis=-1).mean(axis=-1)**2)\n",
    "    test_loss_sofa_sum_square_total += torch.sum(loss_old[:, :, factor_indices].sum(axis=-1).mean(axis=-1)**2)\n",
    "test_loss_p = test_loss/len(test_input)\n",
    "se_loss_p = torch.sqrt(test_loss_sum_square_total / len(test_input) - test_loss_p**2)/len(test_input)\n",
    "test_loss_part1_p = test_loss_part1*batch_size/len(test_input)\n",
    "test_loss_part2_p = test_loss_part2*batch_size/len(test_input)\n",
    "test_loss_sofa_variables_p = test_loss_sofa_variables*batch_size/len(test_input)\n",
    "sofa_loss = sum(sofa_losses)/len(sofa_losses)\n",
    "accuracy_sepsis = sum(accuracy_list)/len(accuracy_list)\n",
    "se_accuracy_sepsis = np.sqrt(accuracy_sepsis * (1-accuracy_sepsis)/len(test_input))\n",
    "se_loss_part1_p = torch.sqrt(test_loss_part1_sum_square_total / len(test_input) - test_loss_part1_p**2)/len(test_input)\n",
    "se_loss_part2_p = torch.sqrt(test_loss_part2_sum_square_total / len(test_input) - test_loss_part2_p**2)/len(test_input)\n",
    "se_loss_sofa_p  = torch.sqrt(test_loss_sofa_sum_square_total / len(test_input)  - test_loss_sofa_variables_p**2)/len(test_input)\n",
    "print(test_loss_p, test_loss_part1_p, test_loss_part2_p, sofa_loss, test_loss_sofa_variables_p, accuracy_sepsis)\n",
    "print('DLinear_DMS',test_loss_p.item(), test_loss_part1_p.item(), test_loss_part2_p.item(), sofa_loss, test_loss_sofa_variables_p.item(), accuracy_sepsis, sep=',',file=open('results.txt','a'))\n",
    "\n",
    "\n",
    "model_name = 'DLinear&Dense&DMS&-&-&'\n",
    "MSE_string = \"{:.3f}\".format(test_loss_p.item()) + \"$_{[\" + \"{:.3f}\".format(test_loss_p.item()-1.96*se_loss_p.item()) +','+ \"{:.3f}\".format(test_loss_p.item()+1.96*se_loss_p.item()) + ']}$&'\n",
    "SOFA_string = \"{:.3f}\".format(sofa_loss)+ '$_{['+\"{:.3f}\".format(sofa_loss-1.96*se_loss_sofa_p.item())+','+\"{:.3f}\".format(sofa_loss+1.96*se_loss_sofa_p.item())+ ']}$&'\n",
    "MSE8_string = \"{:.3f}\".format(test_loss_part1_p.item()) + \"$_{[\" + \"{:.3f}\".format(test_loss_part1_p.item()-1.96*se_loss_part1_p.item()) +','+ \"{:.3f}\".format(test_loss_part1_p.item()+1.96*se_loss_part1_p.item()) + ']}$&'\n",
    "MSE16_string = \"{:.3f}\".format(test_loss_part2_p.item()) + \"$_{[\" + \"{:.3f}\".format(test_loss_part2_p.item()-1.96*se_loss_part2_p.item()) +','+ \"{:.3f}\".format(test_loss_part2_p.item()+1.96*se_loss_part2_p.item()) + ']}$&'\n",
    "SOFA_acc_string = \"{:.2f}\".format(accuracy_sepsis*100)+ '$_{['+\"{:.2f}\".format(100*(accuracy_sepsis-1.96*se_accuracy_sepsis))+','+\"{:.2f}\".format(100*(accuracy_sepsis+1.96*se_accuracy_sepsis))+ ']}$&'\n",
    "print(model_name,MSE_string, SOFA_string, sep='',file=open('eicu_table_sample_sepsis1.txt','a'))\n",
    "print(model_name,MSE8_string, MSE16_string, sep='',file=open('eicu_table_sample_sepsis2.txt','a'))\n",
    "print(model_name,TP,'&',TN,'&',FP,'&',FN,'&', TP+TN+FP+FN,'&', len(test_input), sep='',file=open('eicu_table_sample_sepsis3.txt','a')) \n",
    "print(model_name,len(test_input),'&',SOFA_acc_string, sep='',file=open('eicu_table_sample_sepsis4.txt','a'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a51563",
   "metadata": {},
   "source": [
    "# Linear SOFA Analysis Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6a3d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import models.Linear as autoformer\n",
    "importlib.reload(autoformer)\n",
    "batch_size, lr, patience = 32, 0.0005, 6  # batch_size increased, patience 10 --> 6\n",
    "d, N, he, dropout = 50, 2, 4, 0.2\n",
    "model = autoformer.Model(args).cuda()\n",
    "# Load pretrained weights here.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "#fore_savepath = 'mimic_iii_24h_strats_no_interp_with_ss_fore_blubdiblub.pytorch'\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'max', patience=3)\n",
    "model.load_state_dict(torch.load(\"/home/mitarb/fracarolli/eicu/240401_Experimente/eicu_linear.pytorch\"))\n",
    "#model.load_state_dict(torch.load(fore_savepath))\n",
    "model.eval()\n",
    "y_true, y_pred = [], []\n",
    "print(test_input)\n",
    "pbar = tqdm(range(0, len(test_input), batch_size))\n",
    "loss_list = []\n",
    "test_loss = 0\n",
    "test_loss_part1 = 0\n",
    "test_loss_part2 = 0\n",
    "sofa_losses = []\n",
    "test_loss_sofa_variables = 0\n",
    "accuracy_list = []\n",
    "TP, TN, FP, FN = 0, 0, 0, 0\n",
    "test_loss_sum_square_total = 0\n",
    "test_loss_part1_sum_square_total = 0\n",
    "test_loss_part2_sum_square_total = 0\n",
    "test_loss_sofa_sum_square_total = 0\n",
    "for start in pbar:\n",
    "    print(start)\n",
    "    matrix = torch.tensor(test_input[start:start+batch_size], dtype=torch.float32).cuda()\n",
    "    input_matrix = matrix[:, :24, :V]\n",
    "\n",
    "    #torch.Size([32, 24, 129])\n",
    "    input_mark = torch.arange(0, input_matrix.size(1)).unsqueeze(0).repeat(input_matrix.size(0), 1).cuda()\n",
    "    #torch.Size([32, 24])\n",
    "    input_mask = input_matrix[:, :24, V:]\n",
    "    output_matrix = matrix[:, 24:, :V]\n",
    "    #torch.Size([32, 24, 129])\n",
    "    output_mark = torch.arange(0, output_matrix.size(1)).unsqueeze(0).repeat(input_matrix.size(0), 1).cuda()\n",
    "    #torch.Size([32, 24])\n",
    "    output_mask = matrix[:, 24:, V:]\n",
    "    #torch.Size([32, 24, 129])\n",
    "    with torch.no_grad():\n",
    "        output = model(input_matrix)\n",
    "        loss = output_mask[:, -args.pred_len:, :]*(\n",
    "        output-output_matrix[:, -args.pred_len:, :])**2\n",
    "    loss_list.extend(loss.sum(axis=-1).mean(axis=-1).detach().cpu().tolist())\n",
    "    for outpu, real, mask, input_mat in zip(output, output_matrix, output_mask, input_matrix):\n",
    "        a = (get_sofa(outpu*mask, var_to_ind))\n",
    "        b = (get_sofa(real, var_to_ind))\n",
    "        c = (get_sofa(input_mat, var_to_ind)) #SOFA Source (First Day)\n",
    "        prediction_sofa_difference = (sum(a)-sum(c)) >= 2\n",
    "        gold_sofa_difference = (sum(b)-sum(c)) >= 2\n",
    "        accuracy_list.append(prediction_sofa_difference == gold_sofa_difference)\n",
    "        if gold_sofa_difference:\n",
    "            if prediction_sofa_difference: TP +=1\n",
    "            else: FP += 1\n",
    "        else:\n",
    "            if prediction_sofa_difference: TN +=1\n",
    "            else: FN += 1\n",
    "        c, d = torch.tensor(a, dtype=torch.float32), torch.tensor(b, dtype=torch.float32)\n",
    "        sofa_losses.append(((c-d)**2).sum(axis=-1).mean().item())\n",
    "    loss_old = loss\n",
    "    loss = loss.sum(axis=-1).mean(axis=-1).sum()\n",
    "    loss_part1 = loss_old[:, :8].sum(axis=-1).mean()\n",
    "    loss_part2 = loss_old[:, 8:].sum(axis=-1).mean()\n",
    "    test_loss+=loss\n",
    "    test_loss_part1+=loss_part1\n",
    "    test_loss_part2+=loss_part2\n",
    "    test_loss_sofa_variables+=loss_old[:, :, factor_indices].sum(axis=-1).mean()\n",
    "    test_loss_sum_square_total += torch.sum(loss_old.sum(axis=-1).mean(axis=-1)**2)\n",
    "    test_loss_part1_sum_square_total += torch.sum(loss_old[:, :8].sum(axis=-1).mean(axis=-1)**2)\n",
    "    test_loss_part2_sum_square_total += torch.sum(loss_old[:, 8:].sum(axis=-1).mean(axis=-1)**2)\n",
    "    test_loss_sofa_sum_square_total += torch.sum(loss_old[:, :, factor_indices].sum(axis=-1).mean(axis=-1)**2)\n",
    "test_loss_p = test_loss/len(test_input)\n",
    "se_loss_p = torch.sqrt(test_loss_sum_square_total / len(test_input) - test_loss_p**2)/len(test_input)\n",
    "test_loss_part1_p = test_loss_part1*batch_size/len(test_input)\n",
    "test_loss_part2_p = test_loss_part2*batch_size/len(test_input)\n",
    "test_loss_sofa_variables_p = test_loss_sofa_variables*batch_size/len(test_input)\n",
    "sofa_loss = sum(sofa_losses)/len(sofa_losses)\n",
    "accuracy_sepsis = sum(accuracy_list)/len(accuracy_list)\n",
    "se_accuracy_sepsis = np.sqrt(accuracy_sepsis * (1-accuracy_sepsis)/len(test_input))\n",
    "se_loss_part1_p = torch.sqrt(test_loss_part1_sum_square_total / len(test_input) - test_loss_part1_p**2)/len(test_input)\n",
    "se_loss_part2_p = torch.sqrt(test_loss_part2_sum_square_total / len(test_input) - test_loss_part2_p**2)/len(test_input)\n",
    "se_loss_sofa_p  = torch.sqrt(test_loss_sofa_sum_square_total / len(test_input)  - test_loss_sofa_variables_p**2)/len(test_input)\n",
    "print(test_loss_p, test_loss_part1_p, test_loss_part2_p, sofa_loss, test_loss_sofa_variables_p, accuracy_sepsis)\n",
    "print('Linear_DMS',test_loss_p.item(), test_loss_part1_p.item(), test_loss_part2_p.item(), sofa_loss, test_loss_sofa_variables_p.item(), accuracy_sepsis, sep=',',file=open('results.txt','a'))\n",
    "\n",
    "\n",
    "model_name = 'Linear&Dense&DMS&-&-&'\n",
    "MSE_string = \"{:.3f}\".format(test_loss_p.item()) + \"$_{[\" + \"{:.3f}\".format(test_loss_p.item()-1.96*se_loss_p.item()) +','+ \"{:.3f}\".format(test_loss_p.item()+1.96*se_loss_p.item()) + ']}$&'\n",
    "SOFA_string = \"{:.3f}\".format(sofa_loss)+ '$_{['+\"{:.3f}\".format(sofa_loss-1.96*se_loss_sofa_p.item())+','+\"{:.3f}\".format(sofa_loss+1.96*se_loss_sofa_p.item())+ ']}$&'\n",
    "MSE8_string = \"{:.3f}\".format(test_loss_part1_p.item()) + \"$_{[\" + \"{:.3f}\".format(test_loss_part1_p.item()-1.96*se_loss_part1_p.item()) +','+ \"{:.3f}\".format(test_loss_part1_p.item()+1.96*se_loss_part1_p.item()) + ']}$&'\n",
    "MSE16_string = \"{:.3f}\".format(test_loss_part2_p.item()) + \"$_{[\" + \"{:.3f}\".format(test_loss_part2_p.item()-1.96*se_loss_part2_p.item()) +','+ \"{:.3f}\".format(test_loss_part2_p.item()+1.96*se_loss_part2_p.item()) + ']}$&'\n",
    "SOFA_acc_string = \"{:.2f}\".format(accuracy_sepsis*100)+ '$_{['+\"{:.2f}\".format(100*(accuracy_sepsis-1.96*se_accuracy_sepsis))+','+\"{:.2f}\".format(100*(accuracy_sepsis+1.96*se_accuracy_sepsis))+ ']}$&'\n",
    "print(model_name,MSE_string, SOFA_string, sep='',file=open('eicu_table_sample_sepsis1.txt','a'))\n",
    "print(model_name,MSE8_string, MSE16_string, sep='',file=open('eicu_table_sample_sepsis2.txt','a'))\n",
    "print(model_name,TP,'&',TN,'&',FP,'&',FN,'&', TP+TN+FP+FN,'&', len(test_input), sep='',file=open('eicu_table_sample_sepsis3.txt','a')) \n",
    "print(model_name,len(test_input),'&',SOFA_acc_string, sep='',file=open('eicu_table_sample_sepsis4.txt','a'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba396efe",
   "metadata": {},
   "source": [
    "# 1 STRATS DMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca8c648",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_divisor = 1\n",
    "V=98\n",
    "fore_max_len=880*3\n",
    "class CVE(nn.Module):\n",
    "    def __init__(self, hid_units, output_dim):\n",
    "        super(CVE, self).__init__()\n",
    "        self.hid_units = hid_units\n",
    "        self.output_dim = output_dim\n",
    "        self.W1 = nn.Parameter(torch.Tensor(1, self.hid_units))\n",
    "        self.b1 = nn.Parameter(torch.Tensor(self.hid_units))\n",
    "        self.W2 = nn.Parameter(torch.Tensor(self.hid_units, self.output_dim))\n",
    "        nn.init.xavier_uniform_(self.W1)\n",
    "        nn.init.zeros_(self.b1)\n",
    "        nn.init.xavier_uniform_(self.W2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(-1)\n",
    "        x = torch.matmul(torch.tanh(torch.add(torch.matmul(x, self.W1), self.b1)), self.W2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hid_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.hid_dim = hid_dim\n",
    "        self.W = nn.Parameter(torch.Tensor(50, self.hid_dim))\n",
    "        self.b = nn.Parameter(torch.Tensor(self.hid_dim))\n",
    "        self.u = nn.Parameter(torch.Tensor(self.hid_dim, 1))\n",
    "        nn.init.xavier_uniform_(self.W)\n",
    "        nn.init.zeros_(self.b)\n",
    "        nn.init.xavier_uniform_(self.u)\n",
    "\n",
    "    def forward(self, x, mask, mask_value=-1e30):\n",
    "        attn_weights = torch.matmul(torch.tanh(torch.add(torch.matmul(x, self.W), self.b)), self.u)\n",
    "        mask = mask.unsqueeze(-1)\n",
    "        attn_weights = mask * attn_weights + (1 - mask) * mask_value\n",
    "        attn_weights = F.softmax(attn_weights, dim=-2)\n",
    "        return attn_weights\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, N=2, h=8, dk=None, dv=None, dff=None, dropout=0, d=8):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.N, self.h, self.dk, self.dv, self.dff, self.dropout = N, h, dk, dv, dff, dropout\n",
    "        self.epsilon = torch.finfo(torch.float32).eps * torch.finfo(torch.float32).eps\n",
    "        self.Wq = nn.Parameter(torch.Tensor(N, h, d, dk))\n",
    "        self.Wk = nn.Parameter(torch.Tensor(N, h, d, dk))\n",
    "        self.Wv = nn.Parameter(torch.Tensor(N, h, d, dv))\n",
    "        self.Wo = nn.Parameter(torch.Tensor(N, dv * h, d))\n",
    "        self.W1 = nn.Parameter(torch.Tensor(N, d, dff))\n",
    "        self.b1 = nn.Parameter(torch.Tensor(N, dff))\n",
    "        self.W2 = nn.Parameter(torch.Tensor(N, dff, d))\n",
    "        self.b2 = nn.Parameter(torch.Tensor(N, d))\n",
    "        self.gamma = nn.Parameter(torch.Tensor(2 * N))\n",
    "        self.beta = nn.Parameter(torch.Tensor(2 * N))\n",
    "        nn.init.xavier_uniform_(self.Wq)\n",
    "        nn.init.xavier_uniform_(self.Wk)\n",
    "        nn.init.xavier_uniform_(self.Wv)\n",
    "        nn.init.xavier_uniform_(self.Wo)\n",
    "        nn.init.xavier_uniform_(self.W1)\n",
    "        nn.init.zeros_(self.b1)\n",
    "        nn.init.xavier_uniform_(self.W2)\n",
    "        nn.init.zeros_(self.b2)\n",
    "        nn.init.ones_(self.gamma)\n",
    "        nn.init.zeros_(self.beta)\n",
    "        self.training = False\n",
    "        \n",
    "    def forward(self, x, mask, mask_value=-1e-30):\n",
    "        mask = mask.unsqueeze(-2)\n",
    "        for i in range(self.N):\n",
    "            # MHA\n",
    "            mha_ops = []\n",
    "            for j in range(self.h):\n",
    "                q = torch.matmul(x, self.Wq[i, j, :, :])          \n",
    "                k = torch.matmul(x, self.Wk[i, j, :, :]).permute(0, 2, 1)\n",
    "                v = torch.matmul(x, self.Wv[i, j, :, :])\n",
    "                A = torch.matmul(q, k)\n",
    "                # Mask unobserved steps.\n",
    "                A = mask * A + (1 - mask) * mask_value\n",
    "                # Mask for attention dropout.\n",
    "                if self.training:\n",
    "                    dp_mask = (torch.rand(A.shape, device=A.device) >= self.dropout).float()\n",
    "                    A = A * dp_mask + (1 - dp_mask) * mask_value\n",
    "                A = F.softmax(A, dim=-1)\n",
    "                mha_ops.append(torch.matmul(A, v))\n",
    "            conc = torch.cat(mha_ops, dim=-1)\n",
    "            proj = torch.matmul(conc, self.Wo[i, :, :])\n",
    "            # Dropout.\n",
    "            if self.training:\n",
    "                proj = F.dropout(proj, p=self.dropout)\n",
    "            # Add & LN\n",
    "            x = x + proj\n",
    "            mean = x.mean(dim=-1, keepdim=True)\n",
    "            variance = torch.mean(torch.square(x - mean), dim=-1, keepdim=True)\n",
    "            std = torch.sqrt(variance + self.epsilon)\n",
    "            x = (x - mean) / std\n",
    "            x = x * self.gamma[2 * i] + self.beta[2 * i]\n",
    "            # FFN\n",
    "            ffn_op = torch.add(torch.matmul(F.relu(torch.add(\n",
    "                torch.matmul(x, self.W1[i, :, :]), self.b1[i, :])),\n",
    "                                            self.W2[i, :, :]), self.b2[i, :])\n",
    "            # Dropout.\n",
    "            if self.training:\n",
    "                ffn_op = F.dropout(ffn_op, p=self.dropout)\n",
    "            # Add & LN\n",
    "            x = x + ffn_op\n",
    "            mean = x.mean(dim=-1, keepdim=True)\n",
    "            variance = torch.mean(torch.square(x - mean), dim=-1, keepdim=True)\n",
    "            std = torch.sqrt(variance + self.epsilon)\n",
    "            x = (x - mean) / std\n",
    "            x = x * self.gamma[2 * i + 1] + self.beta[2 * i + 1]\n",
    "        return x\n",
    "\n",
    "\n",
    "class StratsModel(nn.Module):\n",
    "    def __init__(self, D, max_len, V, d, N, he, dropout, forecast=False, autoregressive=False):\n",
    "        super(StratsModel, self).__init__()\n",
    "        self.forecast = forecast\n",
    "        self.autoregressive=autoregressive\n",
    "        self.demo_enc = nn.Sequential(nn.Linear(D, D * d),\n",
    "                                      nn.Tanh(),\n",
    "                                      nn.Linear(D * d, d),\n",
    "                                      nn.Tanh())\n",
    "        self.varis_emb = nn.Embedding(V + 1, d)\n",
    "        nn.init.uniform_(self.varis_emb.weight, a=-0.05, b=0.05)\n",
    "        self.values_emb = CVE(int(np.sqrt(d)), d)\n",
    "        self.times_emb = CVE(int(np.sqrt(d)), d)\n",
    "        # self.comb_emb = nn.LayerNorm(d)\n",
    "        self.transformer = Transformer(N, he, dk=d//he, dv=d//he, dff=2*d, dropout=dropout, d=d)\n",
    "\n",
    "        if autoregressive:\n",
    "            self.linearize = nn.Linear(50, V)\n",
    "            self.dec = nn.TransformerDecoderLayer(d_model=V, dim_feedforward=V, nhead=1, batch_first=True)\n",
    "        else:\n",
    "            self.attention = Attention(2 * d)\n",
    "            self.attentions = nn.ModuleList([Attention(2*d) for i in range(24)])\n",
    "            # self.concat = nn.Linear(2 * d, 1)\n",
    "            self.fore_op_calc = nn.Linear(2 * d, V)\n",
    "            self.op_calc = nn.Linear(V, 1)\n",
    "            self.sig = nn.Sigmoid()        \n",
    "        \n",
    "\n",
    "    def forward(self, demo, times, values, varis, trainn=False, tgt=torch.zeros(32, 1, V).cuda()):\n",
    "        demo_enc = self.demo_enc(demo)\n",
    "        varis_emb = self.varis_emb(varis)\n",
    "        values_emb = self.values_emb(values)\n",
    "        times_emb = self.times_emb(times)\n",
    "\n",
    "        comb_emb = (torch.sum(torch.stack([varis_emb, values_emb, times_emb]), dim=0))\n",
    "        self.trainn = trainn\n",
    "        mask = torch.clip(varis, 0, 1)\n",
    "        cont_emb = self.transformer(comb_emb, mask)\n",
    "        \n",
    "        \n",
    "        \n",
    "        if self.autoregressive:\n",
    "            if self.trainn:\n",
    "                #print(cont_emb.size())\n",
    "                \n",
    "                mask_new = torch.nn.Transformer.generate_square_subsequent_mask(25).cuda()\n",
    "                linear_memory = self.linearize(cont_emb)\n",
    "                start_input = torch.zeros(linear_memory.size(0), 1, V).cuda()\n",
    "                tgt = torch.concat([start_input, tgt], dim=1)\n",
    "                result=self.dec(tgt, linear_memory, mask_new)\n",
    "                return result[:, 1:, :]\n",
    "            if not self.trainn:\n",
    "                output = torch.zeros(32, 1, V).cuda()\n",
    "                linear_memory  = self.linearize(cont_emb)\n",
    "                for i in range(24):\n",
    "                    #print(i)\n",
    "                    #print(linear_memory.size(), output.size())\n",
    "                    res = self.dec(output.detach(), linear_memory)\n",
    "                    #print(res.size(), output.size())\n",
    "                    output = torch.concat([output, res[:,-1:,:]], dim=1)\n",
    "                return output[:, 1:, :]\n",
    "                \n",
    "        if not self.autoregressive:\n",
    "            fused_embs = []\n",
    "            for attention in self.attentions:\n",
    "                attn_weights = attention(cont_emb, mask)\n",
    "                fused_emb = torch.matmul(attn_weights.transpose(-1, -2), cont_emb).squeeze(-2)\n",
    "                # fused_emb torch.Size([32, 50])\n",
    "                fused_embs.append(fused_emb)\n",
    "            # fused_embs Liste mit 6 torch.Size([32, 50])\n",
    "            fused_emb = torch.stack(fused_embs, axis=1)\n",
    "            # altes fused_emb: torch.Size([32, 50])\n",
    "            # neues fused_emb: torch.Size([32, 6, 50])\n",
    "            # demo_enc torch.Size([32, 50])\n",
    "\n",
    "            # demo_enc.unsqueeze(axis=-2).repeat(1, 6, 1):  torch.Size([32, 6, 50])\n",
    "            #print(fused_emb.size())\n",
    "            concat = torch.cat([fused_emb, demo_enc.unsqueeze(axis=-2).repeat(1, 24, 1)], axis=-1)\n",
    "            #concat torch.Size([32, 6, 100])\n",
    "\n",
    "            x = self.fore_op_calc(concat)\n",
    "            # neues x torch.Size([32, 6, 258])\n",
    "            # altes x: torch.Size([32, 258])\n",
    "            if self.forecast:\n",
    "                return x\n",
    "            else:\n",
    "                x = self.sig(self.op_calc(x)).mean(axis=-2)\n",
    "                return x\n",
    "\n",
    "lr, batch_size, samples_per_epoch, patience = 0.0005, 32, int(102400/sample_divisor), 6\n",
    "d, N, he, dropout = 50, 2, 4, 0.2\n",
    "print('number of parameters: ', V)\n",
    "\n",
    "model = StratsModel(17, fore_max_len, V, d, N, he, dropout, forecast=True).cuda()\n",
    "\n",
    "# raise Exception\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=3)\n",
    "# Pretrain fore_model\n",
    "best_val_loss = np.inf\n",
    "\n",
    "fore_savepath = '/home/mitarb/fracarolli/eicu/240401_Experimente/eicu_mod01_Triplet_DMS.pytorch'\n",
    "loss_func = torch.nn.MSELoss(reduction=\"none\")\n",
    "model.load_state_dict(torch.load(fore_savepath))\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "test_loss_part1 = 0\n",
    "test_loss_part2 = 0\n",
    "sofa_losses = []\n",
    "test_loss_sofa_variables = 0\n",
    "accuracy_list = []\n",
    "TP, TN, FP, FN = 0, 0, 0, 0\n",
    "test_loss_sum_square_total = 0\n",
    "test_loss_part1_sum_square_total = 0\n",
    "test_loss_part2_sum_square_total = 0\n",
    "test_loss_sofa_sum_square_total = 0\n",
    "counterblub = 0\n",
    "for start in tqdm(range(0, len(test_ip[0]), 32)):\n",
    "    with torch.no_grad():\n",
    "        output = model(*[torch.tensor(ip[start:start+batch_size], dtype=torch.float32).cuda() \n",
    "                         if i < 3 else torch.tensor(ip[start:start+batch_size], dtype=torch.int32).cuda() \n",
    "                         for i, ip in enumerate(test_ip)], trainn=False)#, tgt=torch.tensor(test_ip[start:start+batch_size], dtype=torch.float32).cuda())\n",
    "    matrix = torch.tensor(test_input[start:start+batch_size], dtype=torch.float32).cuda()\n",
    "    input_matrix = matrix[:, :24, :V]\n",
    "    output_matrix = matrix[:, 24:, :V]\n",
    "    #torch.Size([32, 24, 129])\n",
    "    output_mark = torch.arange(0, output_matrix.size(1)).unsqueeze(0).repeat(batch_size, 1).cuda()\n",
    "    #torch.Size([32, 24])\n",
    "    output_mask = matrix[:, 24:, V:]\n",
    "    #torch.Size([32, 24, 129])\n",
    "    print(output_mask.size(), output.size(), output_matrix.size())\n",
    "    loss = output_mask[:, -24:, :]*(\n",
    "        output-output_matrix[:, -24:, :])**2\n",
    "    for outpu, real, mask, input_mat in zip(output, output_matrix, output_mask, input_matrix):\n",
    "        a = (get_sofa(outpu*mask, var_to_ind))\n",
    "        b = (get_sofa(real, var_to_ind))\n",
    "        c = (get_sofa(input_mat, var_to_ind)) #SOFA Source (First Day)\n",
    "        prediction_sofa_difference = (sum(a)-sum(c)) >= 2\n",
    "        gold_sofa_difference = (sum(b)-sum(c)) >= 2\n",
    "        accuracy_list.append(prediction_sofa_difference == gold_sofa_difference)\n",
    "        if gold_sofa_difference:\n",
    "            counterblub+=1\n",
    "            if prediction_sofa_difference: TP +=1\n",
    "            else: FP += 1\n",
    "        else:\n",
    "            if prediction_sofa_difference: TN +=1\n",
    "            else: FN += 1\n",
    "        c, d = torch.tensor(a, dtype=torch.float32), torch.tensor(b, dtype=torch.float32)\n",
    "        sofa_losses.append(((c-d)**2).sum(axis=-1).mean().item())\n",
    "    loss_old = loss\n",
    "    loss = loss.sum(axis=-1).mean(axis=-1).sum()\n",
    "    loss_part1 = loss_old[:, :8].sum(axis=-1).mean()\n",
    "    loss_part2 = loss_old[:, 8:].sum(axis=-1).mean()\n",
    "    test_loss+=loss\n",
    "    test_loss_part1+=loss_part1\n",
    "    test_loss_part2+=loss_part2\n",
    "    test_loss_sofa_variables+=loss_old[:, :, factor_indices].sum(axis=-1).mean()\n",
    "    test_loss_sum_square_total += torch.sum(loss_old.sum(axis=-1).mean(axis=-1)**2)\n",
    "    test_loss_part1_sum_square_total += torch.sum(loss_old[:, :8].sum(axis=-1).mean(axis=-1)**2)\n",
    "    test_loss_part2_sum_square_total += torch.sum(loss_old[:, 8:].sum(axis=-1).mean(axis=-1)**2)\n",
    "    test_loss_sofa_sum_square_total += torch.sum(loss_old[:, :, factor_indices].sum(axis=-1).mean(axis=-1)**2)\n",
    "test_loss_p = test_loss/len(test_input)\n",
    "se_loss_p = torch.sqrt(test_loss_sum_square_total / len(test_input) - test_loss_p**2)/len(test_input)\n",
    "test_loss_part1_p = test_loss_part1*batch_size/len(test_input)\n",
    "test_loss_part2_p = test_loss_part2*batch_size/len(test_input)\n",
    "test_loss_sofa_variables_p = test_loss_sofa_variables*batch_size/len(test_input)\n",
    "sofa_loss = sum(sofa_losses)/len(sofa_losses)\n",
    "accuracy_sepsis = sum(accuracy_list)/len(accuracy_list)\n",
    "se_accuracy_sepsis = np.sqrt(accuracy_sepsis * (1-accuracy_sepsis)/len(test_input))\n",
    "se_loss_part1_p = torch.sqrt(test_loss_part1_sum_square_total / len(test_input) - test_loss_part1_p**2)/len(test_input)\n",
    "se_loss_part2_p = torch.sqrt(test_loss_part2_sum_square_total / len(test_input) - test_loss_part2_p**2)/len(test_input)\n",
    "se_loss_sofa_p  = torch.sqrt(test_loss_sofa_sum_square_total / len(test_input)  - test_loss_sofa_variables_p**2)/len(test_input)\n",
    "print(test_loss_p, test_loss_part1_p, test_loss_part2_p, sofa_loss, test_loss_sofa_variables_p, accuracy_sepsis)\n",
    "print('Triplet_DMS',test_loss_p.item(), test_loss_part1_p.item(), test_loss_part2_p.item(), sofa_loss, test_loss_sofa_variables_p.item(), accuracy_sepsis, sep=',',file=open('results.txt','a'))\n",
    "\n",
    "model_name = '1&Triplet&DMS&-&-&'\n",
    "MSE_string = \"{:.3f}\".format(test_loss_p.item()) + \"$_{[\" + \"{:.3f}\".format(test_loss_p.item()-1.96*se_loss_p.item()) +','+ \"{:.3f}\".format(test_loss_p.item()+1.96*se_loss_p.item()) + ']}$&'\n",
    "SOFA_string = \"{:.3f}\".format(sofa_loss)+ '$_{['+\"{:.3f}\".format(sofa_loss-1.96*se_loss_sofa_p.item())+','+\"{:.3f}\".format(sofa_loss+1.96*se_loss_sofa_p.item())+ ']}$&'\n",
    "MSE8_string = \"{:.3f}\".format(test_loss_part1_p.item()) + \"$_{[\" + \"{:.3f}\".format(test_loss_part1_p.item()-1.96*se_loss_part1_p.item()) +','+ \"{:.3f}\".format(test_loss_part1_p.item()+1.96*se_loss_part1_p.item()) + ']}$&'\n",
    "MSE16_string = \"{:.3f}\".format(test_loss_part2_p.item()) + \"$_{[\" + \"{:.3f}\".format(test_loss_part2_p.item()-1.96*se_loss_part2_p.item()) +','+ \"{:.3f}\".format(test_loss_part2_p.item()+1.96*se_loss_part2_p.item()) + ']}$&'\n",
    "SOFA_acc_string = \"{:.2f}\".format(accuracy_sepsis*100)+ '$_{['+\"{:.2f}\".format(100*(accuracy_sepsis-1.96*se_accuracy_sepsis))+','+\"{:.2f}\".format(100*(accuracy_sepsis+1.96*se_accuracy_sepsis))+ ']}$&'\n",
    "\n",
    "print(model_name,TP,'&',TN,'&',FP,'&',FN,'&', TP+TN+FP+FN,'&', len(test_input), sep='') \n",
    "print(counterblub)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf29db2",
   "metadata": {},
   "source": [
    "# 2 STRATS IMS Teacher Forcing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824fbd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CVE(nn.Module):\n",
    "    def __init__(self, hid_units, output_dim):\n",
    "        super(CVE, self).__init__()\n",
    "        self.hid_units = hid_units\n",
    "        self.output_dim = output_dim\n",
    "        self.W1 = nn.Parameter(torch.Tensor(1, self.hid_units))\n",
    "        self.b1 = nn.Parameter(torch.Tensor(self.hid_units))\n",
    "        self.W2 = nn.Parameter(torch.Tensor(self.hid_units, self.output_dim))\n",
    "        nn.init.xavier_uniform_(self.W1)\n",
    "        nn.init.zeros_(self.b1)\n",
    "        nn.init.xavier_uniform_(self.W2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(-1)\n",
    "        x = torch.matmul(torch.tanh(torch.add(torch.matmul(x, self.W1), self.b1)), self.W2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hid_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.hid_dim = hid_dim\n",
    "        self.W = nn.Parameter(torch.Tensor(50, self.hid_dim))\n",
    "        self.b = nn.Parameter(torch.Tensor(self.hid_dim))\n",
    "        self.u = nn.Parameter(torch.Tensor(self.hid_dim, 1))\n",
    "        nn.init.xavier_uniform_(self.W)\n",
    "        nn.init.zeros_(self.b)\n",
    "        nn.init.xavier_uniform_(self.u)\n",
    "\n",
    "    def forward(self, x, mask, mask_value=-1e30):\n",
    "        attn_weights = torch.matmul(torch.tanh(torch.add(torch.matmul(x, self.W), self.b)), self.u)\n",
    "        mask = mask.unsqueeze(-1)\n",
    "        attn_weights = mask * attn_weights + (1 - mask) * mask_value\n",
    "        attn_weights = F.softmax(attn_weights, dim=-2)\n",
    "        return attn_weights\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, N=2, h=8, dk=None, dv=None, dff=None, dropout=0, d=8):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.N, self.h, self.dk, self.dv, self.dff, self.dropout = N, h, dk, dv, dff, dropout\n",
    "        self.epsilon = torch.finfo(torch.float32).eps * torch.finfo(torch.float32).eps\n",
    "        self.Wq = nn.Parameter(torch.Tensor(N, h, d, dk))\n",
    "        self.Wk = nn.Parameter(torch.Tensor(N, h, d, dk))\n",
    "        self.Wv = nn.Parameter(torch.Tensor(N, h, d, dv))\n",
    "        self.Wo = nn.Parameter(torch.Tensor(N, dv * h, d))\n",
    "        self.W1 = nn.Parameter(torch.Tensor(N, d, dff))\n",
    "        self.b1 = nn.Parameter(torch.Tensor(N, dff))\n",
    "        self.W2 = nn.Parameter(torch.Tensor(N, dff, d))\n",
    "        self.b2 = nn.Parameter(torch.Tensor(N, d))\n",
    "        self.gamma = nn.Parameter(torch.Tensor(2 * N))\n",
    "        self.beta = nn.Parameter(torch.Tensor(2 * N))\n",
    "        nn.init.xavier_uniform_(self.Wq)\n",
    "        nn.init.xavier_uniform_(self.Wk)\n",
    "        nn.init.xavier_uniform_(self.Wv)\n",
    "        nn.init.xavier_uniform_(self.Wo)\n",
    "        nn.init.xavier_uniform_(self.W1)\n",
    "        nn.init.zeros_(self.b1)\n",
    "        nn.init.xavier_uniform_(self.W2)\n",
    "        nn.init.zeros_(self.b2)\n",
    "        nn.init.ones_(self.gamma)\n",
    "        nn.init.zeros_(self.beta)\n",
    "        self.training = False\n",
    "        \n",
    "    def forward(self, x, mask, mask_value=-1e-30):\n",
    "        mask = mask.unsqueeze(-2)\n",
    "        for i in range(self.N):\n",
    "            # MHA\n",
    "            mha_ops = []\n",
    "            for j in range(self.h):\n",
    "                q = torch.matmul(x, self.Wq[i, j, :, :])          \n",
    "                k = torch.matmul(x, self.Wk[i, j, :, :]).permute(0, 2, 1)\n",
    "                v = torch.matmul(x, self.Wv[i, j, :, :])\n",
    "                A = torch.matmul(q, k)\n",
    "                # Mask unobserved steps.\n",
    "                A = mask * A + (1 - mask) * mask_value\n",
    "                # Mask for attention dropout.\n",
    "                if self.training:\n",
    "                    dp_mask = (torch.rand(A.shape, device=A.device) >= self.dropout).float()\n",
    "                    A = A * dp_mask + (1 - dp_mask) * mask_value\n",
    "                A = F.softmax(A, dim=-1)\n",
    "                mha_ops.append(torch.matmul(A, v))\n",
    "            conc = torch.cat(mha_ops, dim=-1)\n",
    "            proj = torch.matmul(conc, self.Wo[i, :, :])\n",
    "            # Dropout.\n",
    "            if self.training:\n",
    "                proj = F.dropout(proj, p=self.dropout)\n",
    "            # Add & LN\n",
    "            x = x + proj\n",
    "            mean = x.mean(dim=-1, keepdim=True)\n",
    "            variance = torch.mean(torch.square(x - mean), dim=-1, keepdim=True)\n",
    "            std = torch.sqrt(variance + self.epsilon)\n",
    "            x = (x - mean) / std\n",
    "            x = x * self.gamma[2 * i] + self.beta[2 * i]\n",
    "            # FFN\n",
    "            ffn_op = torch.add(torch.matmul(F.relu(torch.add(\n",
    "                torch.matmul(x, self.W1[i, :, :]), self.b1[i, :])),\n",
    "                                            self.W2[i, :, :]), self.b2[i, :])\n",
    "            # Dropout.\n",
    "            if self.training:\n",
    "                ffn_op = F.dropout(ffn_op, p=self.dropout)\n",
    "            # Add & LN\n",
    "            x = x + ffn_op\n",
    "            mean = x.mean(dim=-1, keepdim=True)\n",
    "            variance = torch.mean(torch.square(x - mean), dim=-1, keepdim=True)\n",
    "            std = torch.sqrt(variance + self.epsilon)\n",
    "            x = (x - mean) / std\n",
    "            x = x * self.gamma[2 * i + 1] + self.beta[2 * i + 1]\n",
    "        return x\n",
    "\n",
    "\n",
    "class StratsModel(nn.Module):\n",
    "    def __init__(self, D, max_len, V, d, N, he, dropout, forecast=False, autoregressive=True):\n",
    "        super(StratsModel, self).__init__()\n",
    "        self.forecast = forecast\n",
    "        self.autoregressive=autoregressive\n",
    "        self.demo_enc = nn.Sequential(nn.Linear(D, D * d),\n",
    "                                      nn.Tanh(),\n",
    "                                      nn.Linear(D * d, d),\n",
    "                                      nn.Tanh())\n",
    "        self.varis_emb = nn.Embedding(V + 1, d)\n",
    "        nn.init.uniform_(self.varis_emb.weight, a=-0.05, b=0.05)\n",
    "        self.values_emb = CVE(int(np.sqrt(d)), d)\n",
    "        self.times_emb = CVE(int(np.sqrt(d)), d)\n",
    "        # self.comb_emb = nn.LayerNorm(d)\n",
    "        self.transformer = Transformer(N, he, dk=d//he, dv=d//he, dff=2*d, dropout=dropout, d=d)\n",
    "\n",
    "        if autoregressive:\n",
    "            self.linearize = nn.Linear(50, V)\n",
    "            self.dec = nn.TransformerDecoderLayer(d_model=V, dim_feedforward=V, nhead=1, batch_first=True)\n",
    "        else:\n",
    "            self.attention = Attention(2 * d)\n",
    "            self.attentions = nn.ModuleList([Attention(2*d) for i in range(24)])\n",
    "            # self.concat = nn.Linear(2 * d, 1)\n",
    "            self.fore_op_calc = nn.Linear(2 * d, V)\n",
    "            self.op_calc = nn.Linear(V, 1)\n",
    "            self.sig = nn.Sigmoid()        \n",
    "        \n",
    "\n",
    "    def forward(self, demo, times, values, varis, trainn=False, tgt=torch.zeros(32, 1, V).cuda()):\n",
    "        demo_enc = self.demo_enc(demo)\n",
    "        varis_emb = self.varis_emb(varis)\n",
    "        values_emb = self.values_emb(values)\n",
    "        times_emb = self.times_emb(times)\n",
    "\n",
    "        comb_emb = (torch.sum(torch.stack([varis_emb, values_emb, times_emb]), dim=0))\n",
    "        self.trainn = trainn\n",
    "        mask = torch.clip(varis, 0, 1)\n",
    "        cont_emb = self.transformer(comb_emb, mask)\n",
    "        \n",
    "        \n",
    "        \n",
    "        if self.autoregressive:\n",
    "            if self.trainn:\n",
    "                #print(cont_emb.size())\n",
    "                \n",
    "                mask_new = torch.nn.Transformer.generate_square_subsequent_mask(25).cuda()\n",
    "                linear_memory = self.linearize(cont_emb)\n",
    "                start_input = torch.zeros(linear_memory.size(0), 1, V).cuda()\n",
    "                tgt = torch.concat([start_input, tgt], dim=1)\n",
    "                result=self.dec(tgt, linear_memory, mask_new)\n",
    "                return result[:, 1:, :]\n",
    "            if not self.trainn:\n",
    "                output = torch.zeros(cont_emb.size(0), 1, V).cuda()\n",
    "                linear_memory  = self.linearize(cont_emb)\n",
    "                for i in range(24):\n",
    "                    #print(i)\n",
    "                    #print(linear_memory.size(), output.size())\n",
    "                    res = self.dec(output.detach(), linear_memory)\n",
    "                    #print(res.size(), output.size())\n",
    "                    output = torch.concat([output, res[:,-1:,:]], dim=1)\n",
    "                return output[:, 1:, :]\n",
    "                \n",
    "        if not self.autoregressive:\n",
    "            fused_embs = []\n",
    "            for attention in self.attentions:\n",
    "                attn_weights = attention(cont_emb, mask)\n",
    "                fused_emb = torch.matmul(attn_weights.transpose(-1, -2), cont_emb).squeeze(-2)\n",
    "                # fused_emb torch.Size([32, 50])\n",
    "                fused_embs.append(fused_emb)\n",
    "            # fused_embs Liste mit 6 torch.Size([32, 50])\n",
    "            fused_emb = torch.stack(fused_embs, axis=1)\n",
    "            # altes fused_emb: torch.Size([32, 50])\n",
    "            # neues fused_emb: torch.Size([32, 6, 50])\n",
    "            # demo_enc torch.Size([32, 50])\n",
    "\n",
    "            # demo_enc.unsqueeze(axis=-2).repeat(1, 6, 1):  torch.Size([32, 6, 50])\n",
    "            #print(fused_emb.size())\n",
    "            concat = torch.cat([fused_emb, demo_enc.unsqueeze(axis=-2).repeat(1, 24, 1)], axis=-1)\n",
    "            #concat torch.Size([32, 6, 100])\n",
    "\n",
    "            x = self.fore_op_calc(concat)\n",
    "            # neues x torch.Size([32, 6, 258])\n",
    "            # altes x: torch.Size([32, 258])\n",
    "            if self.forecast:\n",
    "                return x\n",
    "            else:\n",
    "                x = self.sig(self.op_calc(x)).mean(axis=-2)\n",
    "                return x\n",
    "\n",
    "\n",
    "lr, batch_size, samples_per_epoch, patience = 0.0005, 32, int(102400/sample_divisor), 6\n",
    "d, N, he, dropout = 50, 2, 4, 0.2\n",
    "print('number of parameters: ', V)\n",
    "\n",
    "model = StratsModel(17, fore_max_len, V, d, N, he, dropout, forecast=True).cuda()\n",
    "\n",
    "# raise Exception\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=3)\n",
    "# Pretrain fore_model.\n",
    "best_val_loss = np.inf\n",
    "\n",
    "fore_savepath = '/home/mitarb/fracarolli/eicu/240401_Experimente/eicu_mod02_Triplet_IMS_TF.pytorch'\n",
    "loss_func = torch.nn.MSELoss(reduction=\"none\")\n",
    "model.load_state_dict(torch.load(fore_savepath))\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "test_loss_part1 = 0\n",
    "test_loss_part2 = 0\n",
    "sofa_losses = []\n",
    "test_loss_sofa_variables = 0\n",
    "accuracy_list = []\n",
    "TP, TN, FP, FN = 0, 0, 0, 0\n",
    "test_loss_sum_square_total = 0\n",
    "test_loss_part1_sum_square_total = 0\n",
    "test_loss_part2_sum_square_total = 0\n",
    "test_loss_sofa_sum_square_total = 0\n",
    "for start in tqdm(range(0, len(test_ip[0]), 32)):\n",
    "    #print(start, start+batch_size)\n",
    "    with torch.no_grad():\n",
    "        output = model(*[torch.tensor(ip[start:start+batch_size], dtype=torch.float32).cuda() \n",
    "                         if i < 3 else torch.tensor(ip[start:start+batch_size], dtype=torch.int32).cuda() \n",
    "                         for i, ip in enumerate(test_ip)], trainn=False)#, tgt=torch.tensor(test_ip[start:start+batch_size], dtype=torch.float32).cuda())\n",
    "    \n",
    "    matrix = torch.tensor(test_input[start:start+batch_size], dtype=torch.float32).cuda()\n",
    "    input_matrix = matrix[:, :24]\n",
    "    output_matrix = matrix[:, 24:, :V]\n",
    "    #torch.Size([32, 24, 129])\n",
    "    output_mark = torch.arange(0, output_matrix.size(1)).unsqueeze(0).repeat(batch_size, 1).cuda()\n",
    "    #torch.Size([32, 24])\n",
    "    output_mask = matrix[:, 24:, V:]\n",
    "    #torch.Size([32, 24, 129])\n",
    "    loss = output_mask[:, -args.pred_len:, :]*(\n",
    "        output-output_matrix[:, -args.pred_len:, :])**2\n",
    "    for outpu, real, mask, input_mat in zip(output, output_matrix, output_mask, input_matrix):\n",
    "        a = (get_sofa(outpu*mask, var_to_ind))\n",
    "        b = (get_sofa(real, var_to_ind))\n",
    "        c = (get_sofa(input_mat, var_to_ind)) #SOFA Source (First Day)\n",
    "        prediction_sofa_difference = (sum(a)-sum(c)) >= 2\n",
    "        gold_sofa_difference = (sum(b)-sum(c)) >= 2\n",
    "        accuracy_list.append(prediction_sofa_difference == gold_sofa_difference)\n",
    "        if gold_sofa_difference:\n",
    "            if prediction_sofa_difference: TP +=1\n",
    "            else: FP += 1\n",
    "        else:\n",
    "            if prediction_sofa_difference: TN +=1\n",
    "            else: FN += 1\n",
    "                \n",
    "            TP => TP\n",
    "            FP => FN\n",
    "            FN => TN\n",
    "            TN => FP\n",
    "        c, d = torch.tensor(a, dtype=torch.float32), torch.tensor(b, dtype=torch.float32)\n",
    "        sofa_losses.append(((c-d)**2).sum(axis=-1).mean().item())\n",
    "    loss_old = loss\n",
    "    loss = loss.sum(axis=-1).mean(axis=-1).sum()\n",
    "    loss_part1 = loss_old[:, :8].sum(axis=-1).mean()\n",
    "    loss_part2 = loss_old[:, 8:].sum(axis=-1).mean()\n",
    "    test_loss+=loss\n",
    "    test_loss_part1+=loss_part1\n",
    "    test_loss_part2+=loss_part2\n",
    "    test_loss_sofa_variables+=loss_old[:, :, factor_indices].sum(axis=-1).mean()\n",
    "    test_loss_sum_square_total += torch.sum(loss_old.sum(axis=-1).mean(axis=-1)**2)\n",
    "    test_loss_part1_sum_square_total += torch.sum(loss_old[:, :8].sum(axis=-1).mean(axis=-1)**2)\n",
    "    test_loss_part2_sum_square_total += torch.sum(loss_old[:, 8:].sum(axis=-1).mean(axis=-1)**2)\n",
    "    test_loss_sofa_sum_square_total += torch.sum(loss_old[:, :, factor_indices].sum(axis=-1).mean(axis=-1)**2)\n",
    "test_loss_p = test_loss/len(test_input)\n",
    "se_loss_p = torch.sqrt(test_loss_sum_square_total / len(test_input) - test_loss_p**2)/len(test_input)\n",
    "test_loss_part1_p = test_loss_part1*batch_size/len(test_input)\n",
    "test_loss_part2_p = test_loss_part2*batch_size/len(test_input)\n",
    "test_loss_sofa_variables_p = test_loss_sofa_variables*batch_size/len(test_input)\n",
    "sofa_loss = sum(sofa_losses)/len(sofa_losses)\n",
    "accuracy_sepsis = sum(accuracy_list)/len(accuracy_list)\n",
    "se_accuracy_sepsis = np.sqrt(accuracy_sepsis * (1-accuracy_sepsis)/len(test_input))\n",
    "se_loss_part1_p = torch.sqrt(test_loss_part1_sum_square_total / len(test_input) - test_loss_part1_p**2)/len(test_input)\n",
    "se_loss_part2_p = torch.sqrt(test_loss_part2_sum_square_total / len(test_input) - test_loss_part2_p**2)/len(test_input)\n",
    "se_loss_sofa_p  = torch.sqrt(test_loss_sofa_sum_square_total / len(test_input)  - test_loss_sofa_variables_p**2)/len(test_input)\n",
    "print(test_loss_p, test_loss_part1_p, test_loss_part2_p, sofa_loss, test_loss_sofa_variables_p, accuracy_sepsis)\n",
    "print('Triplet_IMS_TF',test_loss_p.item(), test_loss_part1_p.item(), test_loss_part2_p.item(), sofa_loss, test_loss_sofa_variables_p.item(), accuracy_sepsis, sep=',',file=open('results.txt','a'))\n",
    "\n",
    "model_name = '2&Triplet&IMS&TF&-&'\n",
    "MSE_string = \"{:.3f}\".format(test_loss_p.item()) + \"$_{[\" + \"{:.3f}\".format(test_loss_p.item()-1.96*se_loss_p.item()) +','+ \"{:.3f}\".format(test_loss_p.item()+1.96*se_loss_p.item()) + ']}$&'\n",
    "SOFA_string = \"{:.3f}\".format(sofa_loss)+ '$_{['+\"{:.3f}\".format(sofa_loss-1.96*se_loss_sofa_p.item())+','+\"{:.3f}\".format(sofa_loss+1.96*se_loss_sofa_p.item())+ ']}$&'\n",
    "MSE8_string = \"{:.3f}\".format(test_loss_part1_p.item()) + \"$_{[\" + \"{:.3f}\".format(test_loss_part1_p.item()-1.96*se_loss_part1_p.item()) +','+ \"{:.3f}\".format(test_loss_part1_p.item()+1.96*se_loss_part1_p.item()) + ']}$&'\n",
    "MSE16_string = \"{:.3f}\".format(test_loss_part2_p.item()) + \"$_{[\" + \"{:.3f}\".format(test_loss_part2_p.item()-1.96*se_loss_part2_p.item()) +','+ \"{:.3f}\".format(test_loss_part2_p.item()+1.96*se_loss_part2_p.item()) + ']}$&'\n",
    "SOFA_acc_string = \"{:.2f}\".format(accuracy_sepsis*100)+ '$_{['+\"{:.2f}\".format(100*(accuracy_sepsis-1.96*se_accuracy_sepsis))+','+\"{:.2f}\".format(100*(accuracy_sepsis+1.96*se_accuracy_sepsis))+ ']}$&'\n",
    "print(model_name,MSE_string, SOFA_string, sep='',file=open('eicu_table_sample_sepsis1.txt','a'))\n",
    "print(model_name,MSE8_string, MSE16_string, sep='',file=open('eicu_table_sample_sepsis2.txt','a'))\n",
    "print(model_name,TP,'&',TN,'&',FP,'&',FN,'&', TP+TN+FP+FN,'&', len(test_input), sep='',file=open('eicu_table_sample_sepsis3.txt','a')) \n",
    "print(model_name,len(test_input),'&',SOFA_acc_string, sep='',file=open('eicu_table_sample_sepsis4.txt','a'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60e1daf",
   "metadata": {},
   "source": [
    "# 4 STRATS IMS Backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadd5d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CVE(nn.Module):\n",
    "    def __init__(self, hid_units, output_dim):\n",
    "        super(CVE, self).__init__()\n",
    "        self.hid_units = hid_units\n",
    "        self.output_dim = output_dim\n",
    "        self.W1 = nn.Parameter(torch.Tensor(1, self.hid_units))\n",
    "        self.b1 = nn.Parameter(torch.Tensor(self.hid_units))\n",
    "        self.W2 = nn.Parameter(torch.Tensor(self.hid_units, self.output_dim))\n",
    "        nn.init.xavier_uniform_(self.W1)\n",
    "        nn.init.zeros_(self.b1)\n",
    "        nn.init.xavier_uniform_(self.W2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(-1)\n",
    "        x = torch.matmul(torch.tanh(torch.add(torch.matmul(x, self.W1), self.b1)), self.W2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hid_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.hid_dim = hid_dim\n",
    "        self.W = nn.Parameter(torch.Tensor(50, self.hid_dim))\n",
    "        self.b = nn.Parameter(torch.Tensor(self.hid_dim))\n",
    "        self.u = nn.Parameter(torch.Tensor(self.hid_dim, 1))\n",
    "        nn.init.xavier_uniform_(self.W)\n",
    "        nn.init.zeros_(self.b)\n",
    "        nn.init.xavier_uniform_(self.u)\n",
    "\n",
    "    def forward(self, x, mask, mask_value=-1e30):\n",
    "        attn_weights = torch.matmul(torch.tanh(torch.add(torch.matmul(x, self.W), self.b)), self.u)\n",
    "        mask = mask.unsqueeze(-1)\n",
    "        attn_weights = mask * attn_weights + (1 - mask) * mask_value\n",
    "        attn_weights = F.softmax(attn_weights, dim=-2)\n",
    "        return attn_weights\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, N=2, h=8, dk=None, dv=None, dff=None, dropout=0, d=8):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.N, self.h, self.dk, self.dv, self.dff, self.dropout = N, h, dk, dv, dff, dropout\n",
    "        self.epsilon = torch.finfo(torch.float32).eps * torch.finfo(torch.float32).eps\n",
    "        self.Wq = nn.Parameter(torch.Tensor(N, h, d, dk))\n",
    "        self.Wk = nn.Parameter(torch.Tensor(N, h, d, dk))\n",
    "        self.Wv = nn.Parameter(torch.Tensor(N, h, d, dv))\n",
    "        self.Wo = nn.Parameter(torch.Tensor(N, dv * h, d))\n",
    "        self.W1 = nn.Parameter(torch.Tensor(N, d, dff))\n",
    "        self.b1 = nn.Parameter(torch.Tensor(N, dff))\n",
    "        self.W2 = nn.Parameter(torch.Tensor(N, dff, d))\n",
    "        self.b2 = nn.Parameter(torch.Tensor(N, d))\n",
    "        self.gamma = nn.Parameter(torch.Tensor(2 * N))\n",
    "        self.beta = nn.Parameter(torch.Tensor(2 * N))\n",
    "        nn.init.xavier_uniform_(self.Wq)\n",
    "        nn.init.xavier_uniform_(self.Wk)\n",
    "        nn.init.xavier_uniform_(self.Wv)\n",
    "        nn.init.xavier_uniform_(self.Wo)\n",
    "        nn.init.xavier_uniform_(self.W1)\n",
    "        nn.init.zeros_(self.b1)\n",
    "        nn.init.xavier_uniform_(self.W2)\n",
    "        nn.init.zeros_(self.b2)\n",
    "        nn.init.ones_(self.gamma)\n",
    "        nn.init.zeros_(self.beta)\n",
    "        self.training = False\n",
    "        \n",
    "    def forward(self, x, mask, mask_value=-1e-30):\n",
    "        mask = mask.unsqueeze(-2)\n",
    "        for i in range(self.N):\n",
    "            # MHA\n",
    "            mha_ops = []\n",
    "            for j in range(self.h):\n",
    "                q = torch.matmul(x, self.Wq[i, j, :, :])          \n",
    "                k = torch.matmul(x, self.Wk[i, j, :, :]).permute(0, 2, 1)\n",
    "                v = torch.matmul(x, self.Wv[i, j, :, :])\n",
    "                A = torch.matmul(q, k)\n",
    "                # Mask unobserved steps.\n",
    "                A = mask * A + (1 - mask) * mask_value\n",
    "                # Mask for attention dropout.\n",
    "                if self.training:\n",
    "                    dp_mask = (torch.rand(A.shape, device=A.device) >= self.dropout).float()\n",
    "                    A = A * dp_mask + (1 - dp_mask) * mask_value\n",
    "                A = F.softmax(A, dim=-1)\n",
    "                mha_ops.append(torch.matmul(A, v))\n",
    "            conc = torch.cat(mha_ops, dim=-1)\n",
    "            proj = torch.matmul(conc, self.Wo[i, :, :])\n",
    "            # Dropout.\n",
    "            if self.training:\n",
    "                proj = F.dropout(proj, p=self.dropout)\n",
    "            # Add & LN\n",
    "            x = x + proj\n",
    "            mean = x.mean(dim=-1, keepdim=True)\n",
    "            variance = torch.mean(torch.square(x - mean), dim=-1, keepdim=True)\n",
    "            std = torch.sqrt(variance + self.epsilon)\n",
    "            x = (x - mean) / std\n",
    "            x = x * self.gamma[2 * i] + self.beta[2 * i]\n",
    "            # FFN\n",
    "            ffn_op = torch.add(torch.matmul(F.relu(torch.add(\n",
    "                torch.matmul(x, self.W1[i, :, :]), self.b1[i, :])),\n",
    "                                            self.W2[i, :, :]), self.b2[i, :])\n",
    "            # Dropout.\n",
    "            if self.training:\n",
    "                ffn_op = F.dropout(ffn_op, p=self.dropout)\n",
    "            # Add & LN\n",
    "            x = x + ffn_op\n",
    "            mean = x.mean(dim=-1, keepdim=True)\n",
    "            variance = torch.mean(torch.square(x - mean), dim=-1, keepdim=True)\n",
    "            std = torch.sqrt(variance + self.epsilon)\n",
    "            x = (x - mean) / std\n",
    "            x = x * self.gamma[2 * i + 1] + self.beta[2 * i + 1]\n",
    "        return x\n",
    "\n",
    "\n",
    "class StratsModel(nn.Module):\n",
    "    def __init__(self, D, max_len, V, d, N, he, dropout, forecast=False, autoregressive=True):\n",
    "        super(StratsModel, self).__init__()\n",
    "        self.forecast = forecast\n",
    "        self.autoregressive=autoregressive\n",
    "        self.demo_enc = nn.Sequential(nn.Linear(D, D * d),\n",
    "                                      nn.Tanh(),\n",
    "                                      nn.Linear(D * d, d),\n",
    "                                      nn.Tanh())\n",
    "        self.varis_emb = nn.Embedding(V + 1, d)\n",
    "        nn.init.uniform_(self.varis_emb.weight, a=-0.05, b=0.05)\n",
    "        self.values_emb = CVE(int(np.sqrt(d)), d)\n",
    "        self.times_emb = CVE(int(np.sqrt(d)), d)\n",
    "        # self.comb_emb = nn.LayerNorm(d)\n",
    "        self.transformer = Transformer(N, he, dk=d//he, dv=d//he, dff=2*d, dropout=dropout, d=d)\n",
    "\n",
    "        if autoregressive:\n",
    "            self.linearize = nn.Linear(50, V)\n",
    "            self.dec = nn.TransformerDecoderLayer(d_model=V, dim_feedforward=V, nhead=1, batch_first=True)\n",
    "        else:\n",
    "            self.attention = Attention(2 * d)\n",
    "            self.attentions = nn.ModuleList([Attention(2*d) for i in range(24)])\n",
    "            # self.concat = nn.Linear(2 * d, 1)\n",
    "            self.fore_op_calc = nn.Linear(2 * d, V)\n",
    "            self.op_calc = nn.Linear(V, 1)\n",
    "            self.sig = nn.Sigmoid()        \n",
    "        \n",
    "\n",
    "    def forward(self, demo, times, values, varis, trainn=False, tgt=torch.zeros(32, 1, V).cuda()):\n",
    "        demo_enc = self.demo_enc(demo)\n",
    "        varis_emb = self.varis_emb(varis)\n",
    "        values_emb = self.values_emb(values)\n",
    "        times_emb = self.times_emb(times)\n",
    "\n",
    "        comb_emb = (torch.sum(torch.stack([varis_emb, values_emb, times_emb]), dim=0))\n",
    "        self.trainn = trainn\n",
    "        mask = torch.clip(varis, 0, 1)\n",
    "        cont_emb = self.transformer(comb_emb, mask)\n",
    "        \n",
    "        \n",
    "        \n",
    "        if self.autoregressive:\n",
    "            if self.trainn:\n",
    "                #print(cont_emb.size())\n",
    "                \n",
    "                mask_new = torch.nn.Transformer.generate_square_subsequent_mask(25).cuda()\n",
    "                linear_memory = self.linearize(cont_emb)\n",
    "                start_input = torch.zeros(cont_emb.size(0), 1, V).cuda()\n",
    "                tgt = torch.concat([start_input, tgt], dim=1)\n",
    "                result=self.dec(tgt, linear_memory, mask_new)\n",
    "                return result[:, 1:, :]\n",
    "            if not self.trainn:\n",
    "                output = torch.zeros(cont_emb.size(0), 1, V).cuda()\n",
    "                linear_memory  = self.linearize(cont_emb)\n",
    "                for i in range(24):\n",
    "                    #print(i)\n",
    "                    #print(linear_memory.size(), output.size())\n",
    "                    res = self.dec(output, linear_memory)\n",
    "                    #print(res.size(), output.size())\n",
    "                    output = torch.concat([output, res[:,-1:,:]], dim=1)\n",
    "                return output[:, 1:, :]\n",
    "                \n",
    "        if not self.autoregressive:\n",
    "            fused_embs = []\n",
    "            for attention in self.attentions:\n",
    "                attn_weights = attention(cont_emb, mask)\n",
    "                fused_emb = torch.matmul(attn_weights.transpose(-1, -2), cont_emb).squeeze(-2)\n",
    "                # fused_emb torch.Size([32, 50])\n",
    "                fused_embs.append(fused_emb)\n",
    "            # fused_embs Liste mit 6 torch.Size([32, 50])\n",
    "            fused_emb = torch.stack(fused_embs, axis=1)\n",
    "            # altes fused_emb: torch.Size([32, 50])\n",
    "            # neues fused_emb: torch.Size([32, 6, 50])\n",
    "            # demo_enc torch.Size([32, 50])\n",
    "\n",
    "            # demo_enc.unsqueeze(axis=-2).repeat(1, 6, 1):  torch.Size([32, 6, 50])\n",
    "            #print(fused_emb.size())\n",
    "            concat = torch.cat([fused_emb, demo_enc.unsqueeze(axis=-2).repeat(1, 24, 1)], axis=-1)\n",
    "            #concat torch.Size([32, 6, 100])\n",
    "\n",
    "            x = self.fore_op_calc(concat)\n",
    "            # neues x torch.Size([32, 6, 258])\n",
    "            # altes x: torch.Size([32, 258])\n",
    "            if self.forecast:\n",
    "                return x\n",
    "            else:\n",
    "                x = self.sig(self.op_calc(x)).mean(axis=-2)\n",
    "                return x\n",
    "\n",
    "\n",
    "lr, batch_size, samples_per_epoch, patience = 0.0005, 32, int(102400/sample_divisor), 6\n",
    "d, N, he, dropout = 50, 2, 4, 0.2\n",
    "print('number of parameters: ', V)\n",
    "\n",
    "model = StratsModel(17, fore_max_len, V, d, N, he, dropout, forecast=True).cuda()\n",
    "\n",
    "# raise Exception\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=3)\n",
    "# Pretrain fore_model.\n",
    "best_val_loss = np.inf\n",
    "\n",
    "fore_savepath = '/home/mitarb/fracarolli/eicu/240401_Experimente/eicu_mod04_Triplet_IMS_SF_BP.pytorch'\n",
    "loss_func = torch.nn.MSELoss(reduction=\"none\")\n",
    "model.load_state_dict(torch.load(fore_savepath))\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "test_loss_part1 = 0\n",
    "test_loss_part2 = 0\n",
    "sofa_losses = []\n",
    "test_loss_sofa_variables = 0\n",
    "accuracy_list = []\n",
    "TP, TN, FP, FN = 0, 0, 0, 0\n",
    "test_loss_sum_square_total = 0\n",
    "test_loss_part1_sum_square_total = 0\n",
    "test_loss_part2_sum_square_total = 0\n",
    "test_loss_sofa_sum_square_total = 0\n",
    "for start in tqdm(range(0, len(test_ip[0]), 32)):\n",
    "    with torch.no_grad():\n",
    "        output = model(*[torch.tensor(ip[start:start+batch_size], dtype=torch.float32).cuda() \n",
    "                         if i < 3 else torch.tensor(ip[start:start+batch_size], dtype=torch.int32).cuda() \n",
    "                         for i, ip in enumerate(test_ip)], trainn=False)#, tgt=torch.tensor(test_ip[start:start+batch_size], dtype=torch.float32).cuda())\n",
    "    \n",
    "    matrix = torch.tensor(test_input[start:start+batch_size], dtype=torch.float32).cuda()\n",
    "    input_matrix = matrix[:, :24]\n",
    "    output_matrix = matrix[:, 24:, :V]\n",
    "    #torch.Size([32, 24, 129])\n",
    "    output_mark = torch.arange(0, output_matrix.size(1)).unsqueeze(0).repeat(batch_size, 1).cuda()\n",
    "    #torch.Size([32, 24])\n",
    "    output_mask = matrix[:, 24:, V:]\n",
    "    #torch.Size([32, 24, 129])\n",
    "    loss = output_mask[:, -args.pred_len:, :]*(\n",
    "        output-output_matrix[:, -args.pred_len:, :])**2\n",
    "    for outpu, real, mask, input_mat in zip(output, output_matrix, output_mask, input_matrix):\n",
    "        a = (get_sofa(outpu*mask, var_to_ind))\n",
    "        b = (get_sofa(real, var_to_ind))\n",
    "        c = (get_sofa(input_mat, var_to_ind)) #SOFA Source (First Day)\n",
    "        prediction_sofa_difference = (sum(a)-sum(c)) >= 2\n",
    "        gold_sofa_difference = (sum(b)-sum(c)) >= 2\n",
    "        accuracy_list.append(prediction_sofa_difference == gold_sofa_difference)\n",
    "        if gold_sofa_difference:\n",
    "            if prediction_sofa_difference: TP +=1\n",
    "            else: FP += 1\n",
    "        else:\n",
    "            if prediction_sofa_difference: TN +=1\n",
    "            else: FN += 1\n",
    "        c, d = torch.tensor(a, dtype=torch.float32), torch.tensor(b, dtype=torch.float32)\n",
    "        sofa_losses.append(((c-d)**2).sum(axis=-1).mean().item())\n",
    "    loss_old = loss\n",
    "    loss = loss.sum(axis=-1).mean(axis=-1).sum()\n",
    "    loss_part1 = loss_old[:, :8].sum(axis=-1).mean()\n",
    "    loss_part2 = loss_old[:, 8:].sum(axis=-1).mean()\n",
    "    test_loss+=loss\n",
    "    test_loss_part1+=loss_part1\n",
    "    test_loss_part2+=loss_part2\n",
    "    test_loss_sofa_variables+=loss_old[:, :, factor_indices].sum(axis=-1).mean()\n",
    "    test_loss_sum_square_total += torch.sum(loss_old.sum(axis=-1).mean(axis=-1)**2)\n",
    "    test_loss_part1_sum_square_total += torch.sum(loss_old[:, :8].sum(axis=-1).mean(axis=-1)**2)\n",
    "    test_loss_part2_sum_square_total += torch.sum(loss_old[:, 8:].sum(axis=-1).mean(axis=-1)**2)\n",
    "    test_loss_sofa_sum_square_total += torch.sum(loss_old[:, :, factor_indices].sum(axis=-1).mean(axis=-1)**2)\n",
    "test_loss_p = test_loss/len(test_input)\n",
    "se_loss_p = torch.sqrt(test_loss_sum_square_total / len(test_input) - test_loss_p**2)/len(test_input)\n",
    "test_loss_part1_p = test_loss_part1*batch_size/len(test_input)\n",
    "test_loss_part2_p = test_loss_part2*batch_size/len(test_input)\n",
    "test_loss_sofa_variables_p = test_loss_sofa_variables*batch_size/len(test_input)\n",
    "sofa_loss = sum(sofa_losses)/len(sofa_losses)\n",
    "accuracy_sepsis = sum(accuracy_list)/len(accuracy_list)\n",
    "se_accuracy_sepsis = np.sqrt(accuracy_sepsis * (1-accuracy_sepsis)/len(test_input))\n",
    "se_loss_part1_p = torch.sqrt(test_loss_part1_sum_square_total / len(test_input) - test_loss_part1_p**2)/len(test_input)\n",
    "se_loss_part2_p = torch.sqrt(test_loss_part2_sum_square_total / len(test_input) - test_loss_part2_p**2)/len(test_input)\n",
    "se_loss_sofa_p  = torch.sqrt(test_loss_sofa_sum_square_total / len(test_input)  - test_loss_sofa_variables_p**2)/len(test_input)\n",
    "print(test_loss_p, test_loss_part1_p, test_loss_part2_p, sofa_loss, test_loss_sofa_variables_p, accuracy_sepsis)\n",
    "print('Triplet_IMS_SF_B',test_loss_p.item(), test_loss_part1_p.item(), test_loss_part2_p.item(), sofa_loss, test_loss_sofa_variables_p.item(), accuracy_sepsis, sep=',',file=open('results.txt','a'))\n",
    "\n",
    "\n",
    "model_name = '4&Triplet&IMS&SF&Yes&'\n",
    "MSE_string = \"{:.3f}\".format(test_loss_p.item()) + \"$_{[\" + \"{:.3f}\".format(test_loss_p.item()-1.96*se_loss_p.item()) +','+ \"{:.3f}\".format(test_loss_p.item()+1.96*se_loss_p.item()) + ']}$&'\n",
    "SOFA_string = \"{:.3f}\".format(sofa_loss)+ '$_{['+\"{:.3f}\".format(sofa_loss-1.96*se_loss_sofa_p.item())+','+\"{:.3f}\".format(sofa_loss+1.96*se_loss_sofa_p.item())+ ']}$&'\n",
    "MSE8_string = \"{:.3f}\".format(test_loss_part1_p.item()) + \"$_{[\" + \"{:.3f}\".format(test_loss_part1_p.item()-1.96*se_loss_part1_p.item()) +','+ \"{:.3f}\".format(test_loss_part1_p.item()+1.96*se_loss_part1_p.item()) + ']}$&'\n",
    "MSE16_string = \"{:.3f}\".format(test_loss_part2_p.item()) + \"$_{[\" + \"{:.3f}\".format(test_loss_part2_p.item()-1.96*se_loss_part2_p.item()) +','+ \"{:.3f}\".format(test_loss_part2_p.item()+1.96*se_loss_part2_p.item()) + ']}$&'\n",
    "SOFA_acc_string = \"{:.2f}\".format(accuracy_sepsis*100)+ '$_{['+\"{:.2f}\".format(100*(accuracy_sepsis-1.96*se_accuracy_sepsis))+','+\"{:.2f}\".format(100*(accuracy_sepsis+1.96*se_accuracy_sepsis))+ ']}$&'\n",
    "print(model_name,MSE_string, SOFA_string, sep='',file=open('eicu_table_sample_sepsis1.txt','a'))\n",
    "print(model_name,MSE8_string, MSE16_string, sep='',file=open('eicu_table_sample_sepsis2.txt','a'))\n",
    "print(model_name,TP,'&',TN,'&',FP,'&',FN,'&', TP+TN+FP+FN,'&', len(test_input), sep='',file=open('eicu_table_sample_sepsis3.txt','a')) \n",
    "print(model_name,len(test_input),'&',SOFA_acc_string, sep='',file=open('eicu_table_sample_sepsis4.txt','a'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd85b64",
   "metadata": {},
   "source": [
    "# 3 STRATS IMS Detach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b26da09",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CVE(nn.Module):\n",
    "    def __init__(self, hid_units, output_dim):\n",
    "        super(CVE, self).__init__()\n",
    "        self.hid_units = hid_units\n",
    "        self.output_dim = output_dim\n",
    "        self.W1 = nn.Parameter(torch.Tensor(1, self.hid_units))\n",
    "        self.b1 = nn.Parameter(torch.Tensor(self.hid_units))\n",
    "        self.W2 = nn.Parameter(torch.Tensor(self.hid_units, self.output_dim))\n",
    "        nn.init.xavier_uniform_(self.W1)\n",
    "        nn.init.zeros_(self.b1)\n",
    "        nn.init.xavier_uniform_(self.W2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(-1)\n",
    "        x = torch.matmul(torch.tanh(torch.add(torch.matmul(x, self.W1), self.b1)), self.W2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hid_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.hid_dim = hid_dim\n",
    "        self.W = nn.Parameter(torch.Tensor(50, self.hid_dim))\n",
    "        self.b = nn.Parameter(torch.Tensor(self.hid_dim))\n",
    "        self.u = nn.Parameter(torch.Tensor(self.hid_dim, 1))\n",
    "        nn.init.xavier_uniform_(self.W)\n",
    "        nn.init.zeros_(self.b)\n",
    "        nn.init.xavier_uniform_(self.u)\n",
    "\n",
    "    def forward(self, x, mask, mask_value=-1e30):\n",
    "        attn_weights = torch.matmul(torch.tanh(torch.add(torch.matmul(x, self.W), self.b)), self.u)\n",
    "        mask = mask.unsqueeze(-1)\n",
    "        attn_weights = mask * attn_weights + (1 - mask) * mask_value\n",
    "        attn_weights = F.softmax(attn_weights, dim=-2)\n",
    "        return attn_weights\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, N=2, h=8, dk=None, dv=None, dff=None, dropout=0, d=8):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.N, self.h, self.dk, self.dv, self.dff, self.dropout = N, h, dk, dv, dff, dropout\n",
    "        self.epsilon = torch.finfo(torch.float32).eps * torch.finfo(torch.float32).eps\n",
    "        self.Wq = nn.Parameter(torch.Tensor(N, h, d, dk))\n",
    "        self.Wk = nn.Parameter(torch.Tensor(N, h, d, dk))\n",
    "        self.Wv = nn.Parameter(torch.Tensor(N, h, d, dv))\n",
    "        self.Wo = nn.Parameter(torch.Tensor(N, dv * h, d))\n",
    "        self.W1 = nn.Parameter(torch.Tensor(N, d, dff))\n",
    "        self.b1 = nn.Parameter(torch.Tensor(N, dff))\n",
    "        self.W2 = nn.Parameter(torch.Tensor(N, dff, d))\n",
    "        self.b2 = nn.Parameter(torch.Tensor(N, d))\n",
    "        self.gamma = nn.Parameter(torch.Tensor(2 * N))\n",
    "        self.beta = nn.Parameter(torch.Tensor(2 * N))\n",
    "        nn.init.xavier_uniform_(self.Wq)\n",
    "        nn.init.xavier_uniform_(self.Wk)\n",
    "        nn.init.xavier_uniform_(self.Wv)\n",
    "        nn.init.xavier_uniform_(self.Wo)\n",
    "        nn.init.xavier_uniform_(self.W1)\n",
    "        nn.init.zeros_(self.b1)\n",
    "        nn.init.xavier_uniform_(self.W2)\n",
    "        nn.init.zeros_(self.b2)\n",
    "        nn.init.ones_(self.gamma)\n",
    "        nn.init.zeros_(self.beta)\n",
    "        self.training = False\n",
    "        \n",
    "    def forward(self, x, mask, mask_value=-1e-30):\n",
    "        mask = mask.unsqueeze(-2)\n",
    "        for i in range(self.N):\n",
    "            # MHA\n",
    "            mha_ops = []\n",
    "            for j in range(self.h):\n",
    "                q = torch.matmul(x, self.Wq[i, j, :, :])          \n",
    "                k = torch.matmul(x, self.Wk[i, j, :, :]).permute(0, 2, 1)\n",
    "                v = torch.matmul(x, self.Wv[i, j, :, :])\n",
    "                A = torch.matmul(q, k)\n",
    "                # Mask unobserved steps.\n",
    "                A = mask * A + (1 - mask) * mask_value\n",
    "                # Mask for attention dropout.\n",
    "                if self.training:\n",
    "                    dp_mask = (torch.rand(A.shape, device=A.device) >= self.dropout).float()\n",
    "                    A = A * dp_mask + (1 - dp_mask) * mask_value\n",
    "                A = F.softmax(A, dim=-1)\n",
    "                mha_ops.append(torch.matmul(A, v))\n",
    "            conc = torch.cat(mha_ops, dim=-1)\n",
    "            proj = torch.matmul(conc, self.Wo[i, :, :])\n",
    "            # Dropout.\n",
    "            if self.training:\n",
    "                proj = F.dropout(proj, p=self.dropout)\n",
    "            # Add & LN\n",
    "            x = x + proj\n",
    "            mean = x.mean(dim=-1, keepdim=True)\n",
    "            variance = torch.mean(torch.square(x - mean), dim=-1, keepdim=True)\n",
    "            std = torch.sqrt(variance + self.epsilon)\n",
    "            x = (x - mean) / std\n",
    "            x = x * self.gamma[2 * i] + self.beta[2 * i]\n",
    "            # FFN\n",
    "            ffn_op = torch.add(torch.matmul(F.relu(torch.add(\n",
    "                torch.matmul(x, self.W1[i, :, :]), self.b1[i, :])),\n",
    "                                            self.W2[i, :, :]), self.b2[i, :])\n",
    "            # Dropout.\n",
    "            if self.training:\n",
    "                ffn_op = F.dropout(ffn_op, p=self.dropout)\n",
    "            # Add & LN\n",
    "            x = x + ffn_op\n",
    "            mean = x.mean(dim=-1, keepdim=True)\n",
    "            variance = torch.mean(torch.square(x - mean), dim=-1, keepdim=True)\n",
    "            std = torch.sqrt(variance + self.epsilon)\n",
    "            x = (x - mean) / std\n",
    "            x = x * self.gamma[2 * i + 1] + self.beta[2 * i + 1]\n",
    "        return x\n",
    "\n",
    "\n",
    "class StratsModel(nn.Module):\n",
    "    def __init__(self, D, max_len, V, d, N, he, dropout, forecast=False, autoregressive=True):\n",
    "        super(StratsModel, self).__init__()\n",
    "        self.forecast = forecast\n",
    "        self.autoregressive=autoregressive\n",
    "        self.demo_enc = nn.Sequential(nn.Linear(D, D * d),\n",
    "                                      nn.Tanh(),\n",
    "                                      nn.Linear(D * d, d),\n",
    "                                      nn.Tanh())\n",
    "        self.varis_emb = nn.Embedding(V + 1, d)\n",
    "        nn.init.uniform_(self.varis_emb.weight, a=-0.05, b=0.05)\n",
    "        self.values_emb = CVE(int(np.sqrt(d)), d)\n",
    "        self.times_emb = CVE(int(np.sqrt(d)), d)\n",
    "        # self.comb_emb = nn.LayerNorm(d)\n",
    "        self.transformer = Transformer(N, he, dk=d//he, dv=d//he, dff=2*d, dropout=dropout, d=d)\n",
    "\n",
    "        if autoregressive:\n",
    "            self.linearize = nn.Linear(50, V)\n",
    "            self.dec = nn.TransformerDecoderLayer(d_model=V, dim_feedforward=V, nhead=1, batch_first=True)\n",
    "        else:\n",
    "            self.attention = Attention(2 * d)\n",
    "            self.attentions = nn.ModuleList([Attention(2*d) for i in range(24)])\n",
    "            # self.concat = nn.Linear(2 * d, 1)\n",
    "            self.fore_op_calc = nn.Linear(2 * d, V)\n",
    "            self.op_calc = nn.Linear(V, 1)\n",
    "            self.sig = nn.Sigmoid()        \n",
    "        \n",
    "\n",
    "    def forward(self, demo, times, values, varis, trainn=False, tgt=torch.zeros(32, 1, V).cuda()):\n",
    "        demo_enc = self.demo_enc(demo)\n",
    "        varis_emb = self.varis_emb(varis)\n",
    "        values_emb = self.values_emb(values)\n",
    "        times_emb = self.times_emb(times)\n",
    "\n",
    "        comb_emb = (torch.sum(torch.stack([varis_emb, values_emb, times_emb]), dim=0))\n",
    "        self.trainn = trainn\n",
    "        mask = torch.clip(varis, 0, 1)\n",
    "        cont_emb = self.transformer(comb_emb, mask)\n",
    "        \n",
    "        \n",
    "        \n",
    "        if self.autoregressive:\n",
    "            if self.trainn:\n",
    "                #print(cont_emb.size())\n",
    "                \n",
    "                mask_new = torch.nn.Transformer.generate_square_subsequent_mask(25).cuda()\n",
    "                linear_memory = self.linearize(cont_emb)\n",
    "                start_input = torch.zeros(cont_emb.size(0), 1, V).cuda()\n",
    "                tgt = torch.concat([start_input, tgt], dim=1)\n",
    "                result=self.dec(tgt, linear_memory, mask_new)\n",
    "                return result[:, 1:, :]\n",
    "            if not self.trainn:\n",
    "                output = torch.zeros(cont_emb.size(0), 1, V).cuda()\n",
    "                linear_memory  = self.linearize(cont_emb)\n",
    "                for i in range(24):\n",
    "                    #print(i)\n",
    "                    #print(linear_memory.size(), output.size())\n",
    "                    res = self.dec(output.detach(), linear_memory)\n",
    "                    #print(res.size(), output.size())\n",
    "                    output = torch.concat([output, res[:,-1:,:]], dim=1)\n",
    "                return output[:, 1:, :]\n",
    "                \n",
    "        if not self.autoregressive:\n",
    "            fused_embs = []\n",
    "            for attention in self.attentions:\n",
    "                attn_weights = attention(cont_emb, mask)\n",
    "                fused_emb = torch.matmul(attn_weights.transpose(-1, -2), cont_emb).squeeze(-2)\n",
    "                # fused_emb torch.Size([32, 50])\n",
    "                fused_embs.append(fused_emb)\n",
    "            # fused_embs Liste mit 6 torch.Size([32, 50])\n",
    "            fused_emb = torch.stack(fused_embs, axis=1)\n",
    "            # altes fused_emb: torch.Size([32, 50])\n",
    "            # neues fused_emb: torch.Size([32, 6, 50])\n",
    "            # demo_enc torch.Size([32, 50])\n",
    "\n",
    "            # demo_enc.unsqueeze(axis=-2).repeat(1, 6, 1):  torch.Size([32, 6, 50])\n",
    "            #print(fused_emb.size())\n",
    "            concat = torch.cat([fused_emb, demo_enc.unsqueeze(axis=-2).repeat(1, 24, 1)], axis=-1)\n",
    "            #concat torch.Size([32, 6, 100])\n",
    "\n",
    "            x = self.fore_op_calc(concat)\n",
    "            # neues x torch.Size([32, 6, 258])\n",
    "            # altes x: torch.Size([32, 258])\n",
    "            if self.forecast:\n",
    "                return x\n",
    "            else:\n",
    "                x = self.sig(self.op_calc(x)).mean(axis=-2)\n",
    "                return x\n",
    "\n",
    "lr, batch_size, samples_per_epoch, patience = 0.0005, 32, int(102400/sample_divisor), 6\n",
    "d, N, he, dropout = 50, 2, 4, 0.2\n",
    "print('number of parameters: ', V)\n",
    "\n",
    "model = StratsModel(17, fore_max_len, V, d, N, he, dropout, forecast=True).cuda()\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=3)\n",
    "# Pretrain fore_model\n",
    "best_val_loss = np.inf\n",
    "\n",
    "fore_savepath = '/home/mitarb/fracarolli/eicu/240401_Experimente/eicu_24h_strats_no_interp_with_ss_fore_strats_ims_sampling_detach.pytorch'\n",
    "loss_func = torch.nn.MSELoss(reduction=\"none\")\n",
    "model.load_state_dict(torch.load(fore_savepath))\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "test_loss_part1 = 0\n",
    "test_loss_part2 = 0\n",
    "sofa_losses = []\n",
    "test_loss_sofa_variables = 0\n",
    "accuracy_list = []\n",
    "TP, TN, FP, FN = 0, 0, 0, 0\n",
    "test_loss_sum_square_total = 0\n",
    "test_loss_part1_sum_square_total = 0\n",
    "test_loss_part2_sum_square_total = 0\n",
    "test_loss_sofa_sum_square_total = 0\n",
    "for start in tqdm(range(0, len(test_ip[0]), 32)):\n",
    "    #print(start, start+batch_size)\n",
    "    with torch.no_grad():\n",
    "        output = model(*[torch.tensor(ip[start:start+batch_size], dtype=torch.float32).cuda() \n",
    "                         if i < 3 else torch.tensor(ip[start:start+batch_size], dtype=torch.int32).cuda() \n",
    "                         for i, ip in enumerate(test_ip)], trainn=False)#, tgt=torch.tensor(test_ip[start:start+batch_size], dtype=torch.float32).cuda())\n",
    "    \n",
    "    matrix = torch.tensor(test_input[start:start+batch_size], dtype=torch.float32).cuda()\n",
    "    input_matrix = matrix[:, :24]\n",
    "    output_matrix = matrix[:, 24:, :V]\n",
    "    #torch.Size([32, 24, 129])\n",
    "    output_mark = torch.arange(0, output_matrix.size(1)).unsqueeze(0).repeat(batch_size, 1).cuda()\n",
    "    #torch.Size([32, 24])\n",
    "    output_mask = matrix[:, 24:, V:]\n",
    "    #torch.Size([32, 24, 129])\n",
    "    loss = output_mask[:, -args.pred_len:, :]*(\n",
    "        output-output_matrix[:, -args.pred_len:, :])**2\n",
    "    for outpu, real, mask, input_mat in zip(output, output_matrix, output_mask, input_matrix):\n",
    "        a = (get_sofa(outpu*mask, var_to_ind))\n",
    "        b = (get_sofa(real, var_to_ind))\n",
    "        c = (get_sofa(input_mat, var_to_ind)) #SOFA Source (First Day)\n",
    "        prediction_sofa_difference = (sum(a)-sum(c)) >= 2\n",
    "        gold_sofa_difference = (sum(b)-sum(c)) >= 2\n",
    "        accuracy_list.append(prediction_sofa_difference == gold_sofa_difference)\n",
    "        if gold_sofa_difference:\n",
    "            if prediction_sofa_difference: TP +=1\n",
    "            else: FP += 1\n",
    "        else:\n",
    "            if prediction_sofa_difference: TN +=1\n",
    "            else: FN += 1\n",
    "        c, d = torch.tensor(a, dtype=torch.float32), torch.tensor(b, dtype=torch.float32)\n",
    "        sofa_losses.append(((c-d)**2).sum(axis=-1).mean().item())\n",
    "    loss_old = loss\n",
    "    loss = loss.sum(axis=-1).mean(axis=-1).sum()\n",
    "    loss_part1 = loss_old[:, :8].sum(axis=-1).mean()\n",
    "    loss_part2 = loss_old[:, 8:].sum(axis=-1).mean()\n",
    "    test_loss+=loss\n",
    "    test_loss_part1+=loss_part1\n",
    "    test_loss_part2+=loss_part2\n",
    "    test_loss_sofa_variables+=loss_old[:, :, factor_indices].sum(axis=-1).mean()\n",
    "    test_loss_sum_square_total += torch.sum(loss_old.sum(axis=-1).mean(axis=-1)**2)\n",
    "    test_loss_part1_sum_square_total += torch.sum(loss_old[:, :8].sum(axis=-1).mean(axis=-1)**2)\n",
    "    test_loss_part2_sum_square_total += torch.sum(loss_old[:, 8:].sum(axis=-1).mean(axis=-1)**2)\n",
    "    test_loss_sofa_sum_square_total += torch.sum(loss_old[:, :, factor_indices].sum(axis=-1).mean(axis=-1)**2)\n",
    "test_loss_p = test_loss/len(test_input)\n",
    "se_loss_p = torch.sqrt(test_loss_sum_square_total / len(test_input) - test_loss_p**2)/len(test_input)\n",
    "test_loss_part1_p = test_loss_part1*batch_size/len(test_input)\n",
    "test_loss_part2_p = test_loss_part2*batch_size/len(test_input)\n",
    "test_loss_sofa_variables_p = test_loss_sofa_variables*batch_size/len(test_input)\n",
    "sofa_loss = sum(sofa_losses)/len(sofa_losses)\n",
    "accuracy_sepsis = sum(accuracy_list)/len(accuracy_list)\n",
    "se_accuracy_sepsis = np.sqrt(accuracy_sepsis * (1-accuracy_sepsis)/len(test_input))\n",
    "se_loss_part1_p = torch.sqrt(test_loss_part1_sum_square_total / len(test_input) - test_loss_part1_p**2)/len(test_input)\n",
    "se_loss_part2_p = torch.sqrt(test_loss_part2_sum_square_total / len(test_input) - test_loss_part2_p**2)/len(test_input)\n",
    "se_loss_sofa_p  = torch.sqrt(test_loss_sofa_sum_square_total / len(test_input)  - test_loss_sofa_variables_p**2)/len(test_input)\n",
    "print(test_loss_p, test_loss_part1_p, test_loss_part2_p, sofa_loss, test_loss_sofa_variables_p, accuracy_sepsis)\n",
    "print('Triplet_IMS_SF_D',test_loss_p.item(), test_loss_part1_p.item(), test_loss_part2_p.item(), sofa_loss, test_loss_sofa_variables_p.item(), accuracy_sepsis, sep=',',file=open('results.txt','a'))\n",
    "\n",
    "model_name = '3&Triplet&IMS&SF&No&'\n",
    "MSE_string = \"{:.3f}\".format(test_loss_p.item()) + \"$_{[\" + \"{:.3f}\".format(test_loss_p.item()-1.96*se_loss_p.item()) +','+ \"{:.3f}\".format(test_loss_p.item()+1.96*se_loss_p.item()) + ']}$&'\n",
    "SOFA_string = \"{:.3f}\".format(sofa_loss)+ '$_{['+\"{:.3f}\".format(sofa_loss-1.96*se_loss_sofa_p.item())+','+\"{:.3f}\".format(sofa_loss+1.96*se_loss_sofa_p.item())+ ']}$&'\n",
    "MSE8_string = \"{:.3f}\".format(test_loss_part1_p.item()) + \"$_{[\" + \"{:.3f}\".format(test_loss_part1_p.item()-1.96*se_loss_part1_p.item()) +','+ \"{:.3f}\".format(test_loss_part1_p.item()+1.96*se_loss_part1_p.item()) + ']}$&'\n",
    "MSE16_string = \"{:.3f}\".format(test_loss_part2_p.item()) + \"$_{[\" + \"{:.3f}\".format(test_loss_part2_p.item()-1.96*se_loss_part2_p.item()) +','+ \"{:.3f}\".format(test_loss_part2_p.item()+1.96*se_loss_part2_p.item()) + ']}$&'\n",
    "SOFA_acc_string = \"{:.2f}\".format(accuracy_sepsis*100)+ '$_{['+\"{:.2f}\".format(100*(accuracy_sepsis-1.96*se_accuracy_sepsis))+','+\"{:.2f}\".format(100*(accuracy_sepsis+1.96*se_accuracy_sepsis))+ ']}$&'\n",
    "print(model_name,MSE_string, SOFA_string, sep='',file=open('eicu_table_sample_sepsis1.txt','a'))\n",
    "print(model_name,MSE8_string, MSE16_string, sep='',file=open('eicu_table_sample_sepsis2.txt','a'))\n",
    "print(model_name,TP,'&',TN,'&',FP,'&',FN,'&', TP+TN+FP+FN,'&', len(test_input), sep='',file=open('eicu_table_sample_sepsis3.txt','a')) \n",
    "print(model_name,len(test_input),'&',SOFA_acc_string, sep='',file=open('eicu_table_sample_sepsis4.txt','a'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257e3b68",
   "metadata": {},
   "source": [
    "# Informer with Attention-DMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624e3141",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Michi: die ganzen Argparses kÃ¶nnte man noch anders implementieren, wollte es nur schnell zum laufen bringen.\n",
    "import argparse\n",
    "parser = argparse.ArgumentParser(description='Autoformer & Transformer family for Time Series Forecasting')\n",
    "\n",
    "parser.add_argument('--is_training', type=int, required=False, default=1, help='status')\n",
    "parser.add_argument('--train_only', type=bool, required=False, default=False, help='perform training on full input dataset without validation and testing')\n",
    "parser.add_argument('--model_id', type=str, required=False, default='test', help='model id')\n",
    "parser.add_argument('--model', type=str, required=False, default='Autoformer',\n",
    "                    help='model name, options: [Autoformer, Informer, Transformer]')\n",
    "\n",
    "# data loader\n",
    "parser.add_argument('--data', type=str, required=False, default='ETTm1', help='dataset type')\n",
    "parser.add_argument('--root_path', type=str, default='./data/ETT/', help='root path of the data file')\n",
    "parser.add_argument('--data_path', type=str, default='ETTh1.csv', help='data file')\n",
    "parser.add_argument('--features', type=str, default='M',\n",
    "                    help='forecasting task, options:[M, S, MS]; M:multivariate predict multivariate, S:univariate predict univariate, MS:multivariate predict univariate')\n",
    "parser.add_argument('--target', type=str, default='OT', help='target feature in S or MS task')\n",
    "parser.add_argument('--freq', type=str, default='h',\n",
    "                    help='freq for time features encoding, options:[s:secondly, t:minutely, h:hourly, d:daily, b:business days, w:weekly, m:monthly], you can also use more detailed freq like 15min or 3h')\n",
    "parser.add_argument('--checkpoints', type=str, default='./checkpoints/', help='location of model checkpoints')\n",
    "\n",
    "# forecasting task\n",
    "parser.add_argument('--seq_len', type=int, default=24, help='input sequence length')\n",
    "parser.add_argument('--label_len', type=int, default=0, help='start token length')\n",
    "parser.add_argument('--pred_len', type=int, default=24, help='prediction sequence length')\n",
    "\n",
    "\n",
    "# DLinear\n",
    "parser.add_argument('--individual', action='store_true', default=False, help='DLinear: a linear layer for each variate(channel) individually')\n",
    "# Formers \n",
    "#Michi: bisher nur default=3 zum laufen gebracht\n",
    "parser.add_argument('--embed_type', type=int, default=3, help='0: default 1: value embedding + temporal embedding + positional embedding 2: value embedding + temporal embedding 3: value embedding + positional embedding 4: value embedding')\n",
    "parser.add_argument('--enc_in', type=int, default=V*2, help='encoder input size') # DLinear with --individual, use this hyperparameter as the number of channels\n",
    "parser.add_argument('--dec_in', type=int, default=V, help='decoder input size')\n",
    "parser.add_argument('--c_out', type=int, default=V, help='output size')\n",
    "parser.add_argument('--d_model', type=int, default=d_model, help='dimension of model')\n",
    "parser.add_argument('--n_heads', type=int, default=8, help='num of heads')\n",
    "parser.add_argument('--e_layers', type=int, default=e_layers, help='num of encoder layers')\n",
    "parser.add_argument('--d_layers', type=int, default=d_layers, help='num of decoder layers')\n",
    "parser.add_argument('--d_ff', type=int, default=d_ff, help='dimension of fcn')\n",
    "parser.add_argument('--moving_avg', type=int, default=25, help='window size of moving average')\n",
    "parser.add_argument('--factor', type=int, default=1, help='attn factor')\n",
    "parser.add_argument('--distil', action='store_false',\n",
    "                    help='whether to use distilling in encoder, using this argument means not using distilling',\n",
    "                    default=True)\n",
    "parser.add_argument('--dropout', type=float, default=0.05, help='dropout')\n",
    "parser.add_argument('--embed', type=str, default='timeF',\n",
    "                    help='time features encoding, options:[timeF, fixed, learned]')\n",
    "parser.add_argument('--activation', type=str, default='gelu', help='activation')\n",
    "parser.add_argument('--output_attention', action='store_true', help='whether to output attention in ecoder')\n",
    "parser.add_argument('--do_predict', action='store_true', help='whether to predict unseen future data')\n",
    "\n",
    "# optimization\n",
    "parser.add_argument('--num_workers', type=int, default=10, help='data loader num workers')\n",
    "parser.add_argument('--itr', type=int, default=2, help='experiments times')\n",
    "parser.add_argument('--train_epochs', type=int, default=10, help='train epochs')\n",
    "parser.add_argument('--batch_size', type=int, default=32, help='batch size of train input data')\n",
    "parser.add_argument('--patience', type=int, default=3, help='early stopping patience')\n",
    "parser.add_argument('--learning_rate', type=float, default=0.0001, help='optimizer learning rate')\n",
    "parser.add_argument('--des', type=str, default='test', help='exp description')\n",
    "parser.add_argument('--loss', type=str, default='mse', help='loss function')\n",
    "parser.add_argument('--lradj', type=str, default='type1', help='adjust learning rate')\n",
    "parser.add_argument('--use_amp', action='store_true', help='use automatic mixed precision training', default=False)\n",
    "\n",
    "# GPU\n",
    "parser.add_argument('--use_gpu', type=bool, default=True, help='use gpu')\n",
    "parser.add_argument('--gpu', type=int, default=0, help='gpu')\n",
    "parser.add_argument('--use_multi_gpu', action='store_true', help='use multiple gpus', default=False)\n",
    "parser.add_argument('--devices', type=str, default='0,1,2,3', help='device ids of multile gpus')\n",
    "parser.add_argument('--test_flop', action='store_true', default=False, help='See utils/tools for usage')\n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "import importlib\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da38f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import models.InformerFull as autoformer\n",
    "importlib.reload(autoformer)\n",
    "batch_size, lr, patience = 32, 0.0005, 6  # batch_size increased, patience 10 --> 6\n",
    "d, N, he, dropout = 50, 2, 4, 0.2\n",
    "model = autoformer.Model(args).cuda()\n",
    "# Load pretrained weights here.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "#fore_savepath = 'mimic_iii_24h_strats_no_interp_with_ss_fore_blubdiblub.pytorch'\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'max', patience=3)\n",
    "model.load_state_dict(torch.load(\"/home/mitarb/fracarolli/eicu/240401_Experimente/eicu_mod07_Dense_DMS_cont.pytorch\"))\n",
    "#model.load_state_dict(torch.load(fore_savepath))\n",
    "model.eval()\n",
    "y_true, y_pred = [], []\n",
    "print(test_input)\n",
    "pbar = tqdm(range(0, len(test_input), batch_size))\n",
    "loss_list = []\n",
    "test_loss = 0\n",
    "test_loss = 0\n",
    "test_loss_part1 = 0\n",
    "test_loss_part2 = 0\n",
    "sofa_losses = []\n",
    "test_loss_sofa_variables = 0\n",
    "accuracy_list = []\n",
    "TP, TN, FP, FN = 0, 0, 0, 0\n",
    "test_loss_sum_square_total = 0\n",
    "test_loss_part1_sum_square_total = 0\n",
    "test_loss_part2_sum_square_total = 0\n",
    "test_loss_sofa_sum_square_total = 0\n",
    "for start in pbar:\n",
    "    print(start)\n",
    "    matrix = torch.tensor(test_input[start:start+batch_size], dtype=torch.float32).cuda()\n",
    "    input_matrix = matrix[:, :24]\n",
    "\n",
    "    #torch.Size([32, 24, 129])\n",
    "    input_mark = torch.arange(0, input_matrix.size(1)).unsqueeze(0).repeat(input_matrix.size(0), 1).cuda()\n",
    "    #torch.Size([32, 24])\n",
    "    input_mask = input_matrix[:, :24, V:]\n",
    "    output_matrix = matrix[:, 24:, :V]\n",
    "    #torch.Size([32, 24, 129])\n",
    "    output_mark = torch.arange(0, output_matrix.size(1)).unsqueeze(0).repeat(input_matrix.size(0), 1).cuda()\n",
    "    #torch.Size([32, 24])\n",
    "    output_mask = matrix[:, 24:, V:]\n",
    "    #torch.Size([32, 24, 129])\n",
    "    \n",
    "    output_mark = torch.arange(0, output_matrix.size(1)).unsqueeze(0).repeat(input_matrix.size(0), 1).cuda()\n",
    "\n",
    "    dec_inp = torch.zeros_like(output_matrix[:, -args.pred_len:, :]).float()\n",
    "    with torch.no_grad():\n",
    "        output = model(input_matrix, input_mark, dec_inp, output_mark)\n",
    "        loss = output_mask[:, -args.pred_len:, :]*(\n",
    "        output-output_matrix[:, -args.pred_len:, :])**2\n",
    "    loss_list.extend(loss.sum(axis=-1).mean(axis=-1).detach().cpu().tolist())\n",
    "    for outpu, real, mask, input_mat in zip(output, output_matrix, output_mask, input_matrix):\n",
    "        a=(get_sofa(outpu*mask, var_to_ind))\n",
    "        b=(get_sofa(real, var_to_ind))\n",
    "        c = (get_sofa(input_mat, var_to_ind)) #SOFA Source (First Day)\n",
    "        prediction_sofa_difference = (sum(a)-sum(c)) >= 2\n",
    "        gold_sofa_difference = (sum(b)-sum(c)) >= 2\n",
    "        accuracy_list.append(prediction_sofa_difference == gold_sofa_difference)\n",
    "        if gold_sofa_difference:\n",
    "            if prediction_sofa_difference: TP +=1\n",
    "            else: FP += 1\n",
    "        else:\n",
    "            if prediction_sofa_difference: TN +=1\n",
    "            else: FN += 1\n",
    "        c, d = torch.tensor(a, dtype=torch.float32), torch.tensor(b, dtype=torch.float32)\n",
    "        sofa_losses.append(((c-d)**2).sum(axis=-1).mean().item())\n",
    "    loss_old = loss\n",
    "    loss = loss.sum(axis=-1).mean(axis=-1).sum()\n",
    "    loss_part1 = loss_old[:, :8].sum(axis=-1).mean()\n",
    "    loss_part2 = loss_old[:, 8:].sum(axis=-1).mean()\n",
    "    test_loss+=loss\n",
    "    test_loss_part1+=loss_part1\n",
    "    test_loss_part2+=loss_part2\n",
    "    test_loss_sofa_variables+=loss_old[:, :, factor_indices].sum(axis=-1).mean()\n",
    "    test_loss_sum_square_total += torch.sum(loss_old.sum(axis=-1).mean(axis=-1)**2)\n",
    "    test_loss_part1_sum_square_total += torch.sum(loss_old[:, :8].sum(axis=-1).mean(axis=-1)**2)\n",
    "    test_loss_part2_sum_square_total += torch.sum(loss_old[:, 8:].sum(axis=-1).mean(axis=-1)**2)\n",
    "    test_loss_sofa_sum_square_total += torch.sum(loss_old[:, :, factor_indices].sum(axis=-1).mean(axis=-1)**2)\n",
    "test_loss_p = test_loss/len(test_input)\n",
    "se_loss_p = torch.sqrt(test_loss_sum_square_total / len(test_input) - test_loss_p**2)/len(test_input)\n",
    "test_loss_part1_p = test_loss_part1*batch_size/len(test_input)\n",
    "test_loss_part2_p = test_loss_part2*batch_size/len(test_input)\n",
    "test_loss_sofa_variables_p = test_loss_sofa_variables*batch_size/len(test_input)\n",
    "sofa_loss = sum(sofa_losses)/len(sofa_losses)\n",
    "accuracy_sepsis = sum(accuracy_list)/len(accuracy_list)\n",
    "se_accuracy_sepsis = np.sqrt(accuracy_sepsis * (1-accuracy_sepsis)/len(test_input))\n",
    "se_loss_part1_p = torch.sqrt(test_loss_part1_sum_square_total / len(test_input) - test_loss_part1_p**2)/len(test_input)\n",
    "se_loss_part2_p = torch.sqrt(test_loss_part2_sum_square_total / len(test_input) - test_loss_part2_p**2)/len(test_input)\n",
    "se_loss_sofa_p  = torch.sqrt(test_loss_sofa_sum_square_total / len(test_input)  - test_loss_sofa_variables_p**2)/len(test_input)\n",
    "\n",
    "print(test_loss_p, test_loss_part1_p, test_loss_part2_p, sofa_loss, test_loss_sofa_variables_p)\n",
    "\n",
    "model_name = 'Dense&Dense&DMS&-&-&'\n",
    "MSE_string = \"{:.3f}\".format(test_loss_p.item()) + \"$_{[\" + \"{:.3f}\".format(test_loss_p.item()-1.96*se_loss_p.item()) +','+ \"{:.3f}\".format(test_loss_p.item()+1.96*se_loss_p.item()) + ']}$&'\n",
    "SOFA_string = \"{:.3f}\".format(sofa_loss)+ '$_{['+\"{:.3f}\".format(sofa_loss-1.96*se_loss_sofa_p.item())+','+\"{:.3f}\".format(sofa_loss+1.96*se_loss_sofa_p.item())+ ']}$&'\n",
    "MSE8_string = \"{:.3f}\".format(test_loss_part1_p.item()) + \"$_{[\" + \"{:.3f}\".format(test_loss_part1_p.item()-1.96*se_loss_part1_p.item()) +','+ \"{:.3f}\".format(test_loss_part1_p.item()+1.96*se_loss_part1_p.item()) + ']}$&'\n",
    "MSE16_string = \"{:.3f}\".format(test_loss_part2_p.item()) + \"$_{[\" + \"{:.3f}\".format(test_loss_part2_p.item()-1.96*se_loss_part2_p.item()) +','+ \"{:.3f}\".format(test_loss_part2_p.item()+1.96*se_loss_part2_p.item()) + ']}$&'\n",
    "SOFA_acc_string = \"{:.2f}\".format(accuracy_sepsis*100)+ '$_{['+\"{:.2f}\".format(100*(accuracy_sepsis-1.96*se_accuracy_sepsis))+','+\"{:.2f}\".format(100*(accuracy_sepsis+1.96*se_accuracy_sepsis))+ ']}$&'\n",
    "print(model_name,MSE_string, SOFA_string, sep='',file=open('eicu_table_sample_sepsis1.txt','a'))\n",
    "print(model_name,MSE8_string, MSE16_string, sep='',file=open('eicu_table_sample_sepsis2.txt','a'))\n",
    "print(model_name,TP,'&',TN,'&',FP,'&',FN,'&', TP+TN+FP+FN,'&', len(test_input), sep='',file=open('eicu_table_sample_sepsis3.txt','a')) \n",
    "print(model_name,len(test_input),'&',SOFA_acc_string, sep='',file=open('eicu_table_sample_sepsis4.txt','a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0d7b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import models.InformerAutoregressiveFull as autoformer\n",
    "importlib.reload(autoformer)\n",
    "batch_size, lr, patience = 32, 0.0005, 6  # batch_size increased, patience 10 --> 6\n",
    "d, N, he, dropout = 50, 2, 4, 0.2\n",
    "model = autoformer.Model(args).cuda()\n",
    "# Load pretrained weights here.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'max', patience=3)\n",
    "for model_path in os.listdir():\n",
    "    if \"eicu_mod10_Dense\" not in model_path: continue\n",
    "\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    print(test_input)\n",
    "    pbar = tqdm(range(0, len(test_input), batch_size))\n",
    "    loss_list = []\n",
    "    test_loss = 0\n",
    "    test_loss_part1 = 0\n",
    "    test_loss_part2 = 0\n",
    "    sofa_losses = []\n",
    "    test_loss_sofa_variables = 0\n",
    "    accuracy_list = []\n",
    "    TP, TN, FP, FN = 0, 0, 0, 0\n",
    "    test_loss_sum_square_total = 0\n",
    "    test_loss_part1_sum_square_total = 0\n",
    "    test_loss_part2_sum_square_total = 0\n",
    "    test_loss_sofa_sum_square_total = 0\n",
    "    for start in pbar:\n",
    "        print(start)\n",
    "        matrix = torch.tensor(test_input[start:start+batch_size], dtype=torch.float32).cuda()\n",
    "        input_matrix = matrix[:, :24]\n",
    "\n",
    "        #torch.Size([32, 24, 129])\n",
    "        input_mark = torch.arange(0, input_matrix.size(1)).unsqueeze(0).repeat(input_matrix.size(0), 1).cuda()\n",
    "        #torch.Size([32, 24])\n",
    "        input_mask = input_matrix[:, :24, V:]\n",
    "        output_matrix = matrix[:, 24:, :V]\n",
    "        #torch.Size([32, 24, 129])\n",
    "        output_mark = torch.arange(0, output_matrix.size(1)).unsqueeze(0).repeat(input_matrix.size(0), 1).cuda()\n",
    "        #torch.Size([32, 24])\n",
    "        output_mask = matrix[:, 24:, V:]\n",
    "        #torch.Size([32, 24, 129])\n",
    "        output_mark = torch.arange(0, output_matrix.size(1)).unsqueeze(0).repeat(input_matrix.size(0), 1).cuda()\n",
    "        dec_inp = torch.zeros_like(output_matrix[:, -args.pred_len:, :]).float()\n",
    "        with torch.no_grad():\n",
    "            output = model(input_matrix, input_mark, dec_inp, output_mark, trainn=False)\n",
    "            loss = output_mask[:, -args.pred_len:, :]*(\n",
    "            output-output_matrix[:, -args.pred_len:, :])**2\n",
    "        loss_list.extend(loss.sum(axis=-1).mean(axis=-1).detach().cpu().tolist())\n",
    "        for outpu, real, mask, input_mat in zip(output, output_matrix, output_mask, input_matrix):\n",
    "            a = (get_sofa(outpu*mask, var_to_ind))\n",
    "            b = (get_sofa(real, var_to_ind))\n",
    "            c = (get_sofa(input_mat, var_to_ind)) #SOFA Source (First Day)\n",
    "            prediction_sofa_difference = (sum(a)-sum(c)) >= 2\n",
    "            gold_sofa_difference = (sum(b)-sum(c)) >= 2\n",
    "            accuracy_list.append(prediction_sofa_difference == gold_sofa_difference)\n",
    "            if gold_sofa_difference:\n",
    "                if prediction_sofa_difference: TP +=1\n",
    "                else: FP += 1\n",
    "            else:\n",
    "                if prediction_sofa_difference: TN +=1\n",
    "                else: FN += 1\n",
    "            c, d = torch.tensor(a, dtype=torch.float32), torch.tensor(b, dtype=torch.float32)\n",
    "            sofa_losses.append(((c-d)**2).sum(axis=-1).mean().item())\n",
    "        loss_old = loss\n",
    "        loss = loss.sum(axis=-1).mean(axis=-1).sum()\n",
    "        loss_part1 = loss_old[:, :8].sum(axis=-1).mean()\n",
    "        loss_part2 = loss_old[:, 8:].sum(axis=-1).mean()\n",
    "        test_loss+=loss\n",
    "        test_loss_part1+=loss_part1\n",
    "        test_loss_part2+=loss_part2\n",
    "        test_loss_sofa_variables+=loss_old[:, :, factor_indices].sum(axis=-1).mean()\n",
    "        test_loss_sum_square_total += torch.sum(loss_old.sum(axis=-1).mean(axis=-1)**2)\n",
    "        test_loss_part1_sum_square_total += torch.sum(loss_old[:, :8].sum(axis=-1).mean(axis=-1)**2)\n",
    "        test_loss_part2_sum_square_total += torch.sum(loss_old[:, 8:].sum(axis=-1).mean(axis=-1)**2)\n",
    "        test_loss_sofa_sum_square_total += torch.sum(loss_old[:, :, factor_indices].sum(axis=-1).mean(axis=-1)**2)\n",
    "    test_loss_p = test_loss/len(test_input)\n",
    "    se_loss_p = torch.sqrt(test_loss_sum_square_total / len(test_input) - test_loss_p**2)/len(test_input)\n",
    "    test_loss_part1_p = test_loss_part1*batch_size/len(test_input)\n",
    "    test_loss_part2_p = test_loss_part2*batch_size/len(test_input)\n",
    "    test_loss_sofa_variables_p = test_loss_sofa_variables*batch_size/len(test_input)\n",
    "    sofa_loss = sum(sofa_losses)/len(sofa_losses)\n",
    "    accuracy_sepsis = sum(accuracy_list)/len(accuracy_list)\n",
    "    se_accuracy_sepsis = np.sqrt(accuracy_sepsi/tes * (1-accuracy_sepsis)/len(test_input))\n",
    "    se_loss_part1_p = torch.sqrt(test_loss_part1_sum_square_total / len(test_input) - test_loss_part1_p**2)/len(test_input)\n",
    "    se_loss_part2_p = torch.sqrt(test_loss_part2_sum_square_total / len(test_input) - test_loss_part2_p**2)/len(test_input)\n",
    "    se_loss_sofa_p  = torch.sqrt(test_loss_sofa_sum_square_total / len(test_input)  - test_loss_sofa_variables_p**2)/len(test_input)\n",
    "    print(test_loss_p, test_loss_part1_p, test_loss_part2_p, sofa_loss, test_loss_sofa_variables_p, accuracy_sepsis)\n",
    "    print('Informer_IMS_SF_B',test_loss_p.item(), test_loss_part1_p.item(), test_loss_part2_p.item(), sofa_loss, test_loss_sofa_variables_p.item(), accuracy_sepsis, sep=',',file=open('results.txt','a'))\n",
    "\n",
    "    model_name = model_path\n",
    "    MSE_string = \"{:.3f}\".format(test_loss_p.item()) + \"$_{[\" + \"{:.3f}\".format(test_loss_p.item()-1.96*se_loss_p.item()) +','+ \"{:.3f}\".format(test_loss_p.item()+1.96*se_loss_p.item()) + ']}$&'\n",
    "    SOFA_string = \"{:.3f}\".format(sofa_loss)+ '$_{['+\"{:.3f}\".format(sofa_loss-1.96*se_loss_sofa_p.item())+','+\"{:.3f}\".format(sofa_loss+1.96*se_loss_sofa_p.item())+ ']}$&'\n",
    "    MSE8_string = \"{:.3f}\".format(test_loss_part1_p.item()) + \"$_{[\" + \"{:.3f}\".format(test_loss_part1_p.item()-1.96*se_loss_part1_p.item()) +','+ \"{:.3f}\".format(test_loss_part1_p.item()+1.96*se_loss_part1_p.item()) + ']}$&'\n",
    "    MSE16_string = \"{:.3f}\".format(test_loss_part2_p.item()) + \"$_{[\" + \"{:.3f}\".format(test_loss_part2_p.item()-1.96*se_loss_part2_p.item()) +','+ \"{:.3f}\".format(test_loss_part2_p.item()+1.96*se_loss_part2_p.item()) + ']}$&'\n",
    "    SOFA_acc_string = \"{:.2f}\".format(accuracy_sepsis*100)+ '$_{['+\"{:.2f}\".format(100*(accuracy_sepsis-1.96*se_accuracy_sepsis))+','+\"{:.2f}\".format(100*(accuracy_sepsis+1.96*se_accuracy_sepsis))+ ']}$&'\n",
    "    print(model_name,MSE_string, SOFA_string, sep='',file=open('eicu_table_sample_sepsis1.txt','a'))\n",
    "    print(model_name,MSE8_string, MSE16_string, sep='',file=open('eicu_table_sample_sepsis2.txt','a'))\n",
    "    print(model_name,TP,'&',TN,'&',FP,'&',FN,'&', TP+TN+FP+FN,'&', len(test_input), sep='',file=open('eicu_table_sample_sepsis3.txt','a')) \n",
    "    print(model_name,len(test_input),'&',SOFA_acc_string, sep='',file=open('eicu_table_sample_sepsis4.txt','a'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e016dbc",
   "metadata": {},
   "source": [
    "# NEW EXPERIMENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ca8262",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model, e_layers, d_layers, d_ff = 256, 2, 2, 2048\n",
    "number_of_epochs = 100\n",
    "V, D = 98, 17\n",
    "fore_max_len = 2640\n",
    "print('loading of joblib completed')\n",
    "\n",
    "\n",
    "\n",
    "import argparse\n",
    "parser = argparse.ArgumentParser(description='Autoformer & Transformer family for Time Series Forecasting')\n",
    "\n",
    "# basic config\n",
    "parser.add_argument('--is_training', type=int, required=False, default=1, help='status')\n",
    "parser.add_argument('--train_only', type=bool, required=False, default=False, help='perform training on full input dataset without validation and testing')\n",
    "parser.add_argument('--model_id', type=str, required=False, default='test', help='model id')\n",
    "parser.add_argument('--model', type=str, required=False, default='Autoformer',\n",
    "                    help='model name, options: [Autoformer, Informer, Transformer]')\n",
    "\n",
    "# data loader\n",
    "parser.add_argument('--data', type=str, required=False, default='ETTm1', help='dataset type')\n",
    "parser.add_argument('--root_path', type=str, default='./data/ETT/', help='root path of the data file')\n",
    "parser.add_argument('--data_path', type=str, default='ETTh1.csv', help='data file')\n",
    "parser.add_argument('--features', type=str, default='M',\n",
    "                    help='forecasting task, options:[M, S, MS]; M:multivariate predict multivariate, S:univariate predict univariate, MS:multivariate predict univariate')\n",
    "parser.add_argument('--target', type=str, default='OT', help='target feature in S or MS task')\n",
    "parser.add_argument('--freq', type=str, default='h',\n",
    "                    help='freq for time features encoding, options:[s:secondly, t:minutely, h:hourly, d:daily, b:business days, w:weekly, m:monthly], you can also use more detailed freq like 15min or 3h')\n",
    "parser.add_argument('--checkpoints', type=str, default='./checkpoints/', help='location of model checkpoints')\n",
    "\n",
    "# forecasting task\n",
    "parser.add_argument('--seq_len', type=int, default=24, help='input sequence length')\n",
    "parser.add_argument('--label_len', type=int, default=0, help='start token length')\n",
    "parser.add_argument('--pred_len', type=int, default=24, help='prediction sequence length')\n",
    "\n",
    "\n",
    "# DLinear\n",
    "parser.add_argument('--individual', action='store_true', default=False, help='DLinear: a linear layer for each variate(channel) individually')\n",
    "# Formers \n",
    "#Michi: bisher nur default=3 zum laufen gebracht\n",
    "parser.add_argument('--embed_type', type=int, default=3, help='0: default 1: value embedding + temporal embedding + positional embedding 2: value embedding + temporal embedding 3: value embedding + positional embedding 4: value embedding')\n",
    "parser.add_argument('--enc_in', type=int, default=V*2, help='encoder input size') # DLinear with --individual, use this hyperparameter as the number of channels\n",
    "parser.add_argument('--dec_in', type=int, default=V, help='decoder input size')\n",
    "parser.add_argument('--c_out', type=int, default=V, help='output size')\n",
    "parser.add_argument('--d_model', type=int, default=d_model, help='dimension of model')\n",
    "parser.add_argument('--n_heads', type=int, default=8, help='num of heads')\n",
    "parser.add_argument('--e_layers', type=int, default=e_layers, help='num of encoder layers')\n",
    "parser.add_argument('--d_layers', type=int, default=d_layers, help='num of decoder layers')\n",
    "parser.add_argument('--d_ff', type=int, default=d_ff, help='dimension of fcn')\n",
    "parser.add_argument('--moving_avg', type=int, default=25, help='window size of moving average')\n",
    "parser.add_argument('--factor', type=int, default=1, help='attn factor')\n",
    "parser.add_argument('--distil', action='store_false',\n",
    "                    help='whether to use distilling in encoder, using this argument means not using distilling',\n",
    "                    default=True)\n",
    "parser.add_argument('--dropout', type=float, default=0.05, help='dropout')\n",
    "parser.add_argument('--embed', type=str, default='timeF',\n",
    "                    help='time features encoding, options:[timeF, fixed, learned]')\n",
    "parser.add_argument('--activation', type=str, default='gelu', help='activation')\n",
    "parser.add_argument('--output_attention', action='store_true', help='whether to output attention in ecoder')\n",
    "parser.add_argument('--do_predict', action='store_true', help='whether to predict unseen future data')\n",
    "\n",
    "# optimization\n",
    "parser.add_argument('--num_workers', type=int, default=10, help='data loader num workers')\n",
    "parser.add_argument('--itr', type=int, default=2, help='experiments times')\n",
    "parser.add_argument('--train_epochs', type=int, default=10, help='train epochs')\n",
    "parser.add_argument('--batch_size', type=int, default=32, help='batch size of train input data')\n",
    "parser.add_argument('--patience', type=int, default=3, help='early stopping patience')\n",
    "parser.add_argument('--learning_rate', type=float, default=0.0001, help='optimizer learning rate')\n",
    "parser.add_argument('--des', type=str, default='test', help='exp description')\n",
    "parser.add_argument('--loss', type=str, default='mse', help='loss function')\n",
    "parser.add_argument('--lradj', type=str, default='type1', help='adjust learning rate')\n",
    "parser.add_argument('--use_amp', action='store_true', help='use automatic mixed precision training', default=False)\n",
    "\n",
    "# GPU\n",
    "parser.add_argument('--use_gpu', type=bool, default=True, help='use gpu')\n",
    "parser.add_argument('--gpu', type=int, default=0, help='gpu')\n",
    "parser.add_argument('--use_multi_gpu', action='store_true', help='use multiple gpus', default=False)\n",
    "parser.add_argument('--devices', type=str, default='0,1,2,3', help='device ids of multile gpus')\n",
    "parser.add_argument('--test_flop', action='store_true', default=False, help='See utils/tools for usage')\n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa7b866",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import models.InformerAutoregressiveFull as autoformer\n",
    "importlib.reload(autoformer)\n",
    "batch_size, lr, patience = 32, 0.0005, 6  # batch_size increased, patience 10 --> 6\n",
    "d, N, he, dropout = 50, 2, 4, 0.2\n",
    "\n",
    "for model_path in os.listdir(\"/home/mitarb/fracarolli/eicu/final/240408_models\"):\n",
    "    if not(\"mod10\" in model_path or \"mod09\" in model_path): continue\n",
    "    print(model_path)\n",
    "    model = autoformer.Model(args).cuda()\n",
    "    # Load pretrained weights here.\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    #fore_savepath = 'mimic_iii_24h_strats_no_interp_with_ss_fore_blubdiblub.pytorch'\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'max', patience=3)\n",
    "    model.load_state_dict(torch.load(\"/home/mitarb/fracarolli/eicu/final/240408_models/\" + model_path))\n",
    "    #model.load_state_dict(torch.load(fore_savepath))\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    print(test_input)\n",
    "    pbar = tqdm(range(0, len(test_input), batch_size))\n",
    "    loss_list = []\n",
    "    test_loss = 0\n",
    "    test_loss_part1 = 0\n",
    "    test_loss_part2 = 0\n",
    "    test_loss_sofa_variables = 0\n",
    "    sofa_losses = []\n",
    "    accuracy_list = []\n",
    "    TP, TN, FP, FN = 0, 0, 0, 0\n",
    "    test_loss_sum_square_total = 0\n",
    "    test_loss_part1_sum_square_total = 0\n",
    "    test_loss_part2_sum_square_total = 0\n",
    "    test_loss_sofa_sum_square_total = 0\n",
    "    for start in pbar:\n",
    "        print(start)\n",
    "        matrix = torch.tensor(test_input[start:start+batch_size], dtype=torch.float32).cuda()\n",
    "        input_matrix = matrix[:, :24]\n",
    "\n",
    "        #torch.Size([32, 24, 129])\n",
    "        input_mark = torch.arange(0, input_matrix.size(1)).unsqueeze(0).repeat(input_matrix.size(0), 1).cuda()\n",
    "        #torch.Size([32, 24])\n",
    "        input_mask = input_matrix[:, :24, V:]\n",
    "        output_matrix = matrix[:, 24:, :V]\n",
    "        #torch.Size([32, 24, 129])\n",
    "        output_mark = torch.arange(0, output_matrix.size(1)).unsqueeze(0).repeat(input_matrix.size(0), 1).cuda()\n",
    "        #torch.Size([32, 24])\n",
    "        output_mask = matrix[:, 24:, V:]\n",
    "\n",
    "        output_mark = torch.arange(0, output_matrix.size(1)).unsqueeze(0).repeat(input_matrix.size(0), 1).cuda()\n",
    "        dec_inp = torch.zeros_like(output_matrix[:, -args.pred_len:, :]).float()\n",
    "        #torch.Size([32, 24, 129])\n",
    "        with torch.no_grad():\n",
    "            output = model(input_matrix, input_mark, dec_inp, output_mark, trainn=False)\n",
    "            loss = output_mask[:, -args.pred_len:, :]*(\n",
    "            output-output_matrix[:, -args.pred_len:, :])**2\n",
    "        loss_list.extend(loss.sum(axis=-1).mean(axis=-1).detach().cpu().tolist())\n",
    "        for outpu, real, mask, input_mat in zip(output, output_matrix, output_mask, input_matrix):\n",
    "            a = (get_sofa(outpu*mask, var_to_ind))\n",
    "            b = (get_sofa(real, var_to_ind))\n",
    "            c = (get_sofa(input_mat, var_to_ind)) #SOFA Source (First Day)\n",
    "            prediction_sofa_difference = (sum(a)-sum(c)) >= 2\n",
    "            gold_sofa_difference = (sum(b)-sum(c)) >= 2\n",
    "            accuracy_list.append(prediction_sofa_difference == gold_sofa_difference)\n",
    "            if gold_sofa_difference:\n",
    "                if prediction_sofa_difference: TP +=1\n",
    "                else: FP += 1\n",
    "            else:\n",
    "                if prediction_sofa_difference: TN +=1\n",
    "                else: FN += 1\n",
    "            c, d = torch.tensor(a, dtype=torch.float32), torch.tensor(b, dtype=torch.float32)\n",
    "            sofa_losses.append(((c-d)**2).sum(axis=-1).mean().item())\n",
    "        loss_old = loss\n",
    "        loss = loss.sum(axis=-1).mean(axis=-1).sum()\n",
    "        loss_part1 = loss_old[:, :8].sum(axis=-1).mean()\n",
    "        loss_part2 = loss_old[:, 8:].sum(axis=-1).mean()\n",
    "        test_loss+=loss\n",
    "        test_loss_part1+=loss_part1\n",
    "        test_loss_part2+=loss_part2\n",
    "        test_loss_sofa_variables+=loss_old[:, :, factor_indices].sum(axis=-1).mean()\n",
    "        test_loss_sum_square_total += torch.sum(loss_old.sum(axis=-1).mean(axis=-1)**2)\n",
    "        test_loss_part1_sum_square_total += torch.sum(loss_old[:, :8].sum(axis=-1).mean(axis=-1)**2)\n",
    "        test_loss_part2_sum_square_total += torch.sum(loss_old[:, 8:].sum(axis=-1).mean(axis=-1)**2)\n",
    "        test_loss_sofa_sum_square_total += torch.sum(loss_old[:, :, factor_indices].sum(axis=-1).mean(axis=-1)**2)\n",
    "    test_loss_p = test_loss/len(test_input)\n",
    "    se_loss_p = torch.sqrt(test_loss_sum_square_total / len(test_input) - test_loss_p**2)/len(test_input)\n",
    "    test_loss_part1_p = test_loss_part1*batch_size/len(test_input)\n",
    "    test_loss_part2_p = test_loss_part2*batch_size/len(test_input)\n",
    "    test_loss_sofa_variables_p = test_loss_sofa_variables*batch_size/len(test_input)\n",
    "    sofa_loss = sum(sofa_losses)/len(sofa_losses)\n",
    "    accuracy_sepsis = sum(accuracy_list)/len(accuracy_list)\n",
    "    se_accuracy_sepsis = np.sqrt(accuracy_sepsis * (1-accuracy_sepsis)/len(test_input))\n",
    "    se_loss_part1_p = torch.sqrt(test_loss_part1_sum_square_total / len(test_input) - test_loss_part1_p**2)/len(test_input)\n",
    "    se_loss_part2_p = torch.sqrt(test_loss_part2_sum_square_total / len(test_input) - test_loss_part2_p**2)/len(test_input)\n",
    "    se_loss_sofa_p  = torch.sqrt(test_loss_sofa_sum_square_total / len(test_input)  - test_loss_sofa_variables_p**2)/len(test_input)\n",
    "    print(test_loss_p, test_loss_part1_p, test_loss_part2_p, sofa_loss, test_loss_sofa_variables_p, accuracy_sepsis)\n",
    "    print('Informer_IMS_SF_D',test_loss_p.item(), test_loss_part1_p.item(), test_loss_part2_p.item(), sofa_loss, test_loss_sofa_variables_p.item(), accuracy_sepsis, sep=',',file=open('results.txt','a'))\n",
    "\n",
    "\n",
    "\n",
    "    model_name = model_path+'&'\n",
    "    MSE_string = \"{:.3f}\".format(test_loss_p.item()) + \"$_{[\" + \"{:.3f}\".format(test_loss_p.item()-1.96*se_loss_p.item()) +','+ \"{:.3f}\".format(test_loss_p.item()+1.96*se_loss_p.item()) + ']}$&'\n",
    "    SOFA_string = \"{:.3f}\".format(sofa_loss)+ '$_{['+\"{:.3f}\".format(sofa_loss-1.96*se_loss_sofa_p.item())+','+\"{:.3f}\".format(sofa_loss+1.96*se_loss_sofa_p.item())+ ']}$&'\n",
    "    MSE8_string = \"{:.3f}\".format(test_loss_part1_p.item()) + \"$_{[\" + \"{:.3f}\".format(test_loss_part1_p.item()-1.96*se_loss_part1_p.item()) +','+ \"{:.3f}\".format(test_loss_part1_p.item()+1.96*se_loss_part1_p.item()) + ']}$&'\n",
    "    MSE16_string = \"{:.3f}\".format(test_loss_part2_p.item()) + \"$_{[\" + \"{:.3f}\".format(test_loss_part2_p.item()-1.96*se_loss_part2_p.item()) +','+ \"{:.3f}\".format(test_loss_part2_p.item()+1.96*se_loss_part2_p.item()) + ']}$&'\n",
    "    SOFA_acc_string = \"{:.2f}\".format(accuracy_sepsis*100)+ '$_{['+\"{:.2f}\".format(100*(accuracy_sepsis-1.96*se_accuracy_sepsis))+','+\"{:.2f}\".format(100*(accuracy_sepsis+1.96*se_accuracy_sepsis))+ ']}$&'\n",
    "    print(model_name,MSE_string, SOFA_string, sep='',file=open('eicu_table_sample_sepsis1.txt','a'))\n",
    "    print(model_name,MSE8_string, MSE16_string, sep='',file=open('eicu_table_sample_sepsis2.txt','a'))\n",
    "    print(model_name,TP,'&',TN,'&',FP,'&',FN,'&', TP+TN+FP+FN,'&', len(test_input), sep='',file=open('eicu_table_sample_sepsis3.txt','a')) \n",
    "    print(model_name,len(test_input),'&',SOFA_acc_string, sep='',file=open('eicu_table_sample_sepsis4.txt','a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8da0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model, e_layers, d_layers, d_ff = 256, 2, 2, 2048\n",
    "number_of_epochs = 100\n",
    "V, D = 98, 17\n",
    "fore_max_len = 2640\n",
    "print('loading of joblib completed')\n",
    "\n",
    "\n",
    "\n",
    "import argparse\n",
    "parser = argparse.ArgumentParser(description='Autoformer & Transformer family for Time Series Forecasting')\n",
    "\n",
    "\n",
    "# basic config\n",
    "parser.add_argument('--is_training', type=int, required=False, default=1, help='status')\n",
    "parser.add_argument('--train_only', type=bool, required=False, default=False, help='perform training on full input dataset without validation and testing')\n",
    "parser.add_argument('--model_id', type=str, required=False, default='test', help='model id')\n",
    "parser.add_argument('--model', type=str, required=False, default='Autoformer',\n",
    "                    help='model name, options: [Autoformer, Informer, Transformer]')\n",
    "\n",
    "# data loader\n",
    "parser.add_argument('--data', type=str, required=False, default='ETTm1', help='dataset type')\n",
    "parser.add_argument('--root_path', type=str, default='./data/ETT/', help='root path of the data file')\n",
    "parser.add_argument('--data_path', type=str, default='ETTh1.csv', help='data file')\n",
    "parser.add_argument('--features', type=str, default='M',\n",
    "                    help='forecasting task, options:[M, S, MS]; M:multivariate predict multivariate, S:univariate predict univariate, MS:multivariate predict univariate')\n",
    "parser.add_argument('--target', type=str, default='OT', help='target feature in S or MS task')\n",
    "parser.add_argument('--freq', type=str, default='h',\n",
    "                    help='freq for time features encoding, options:[s:secondly, t:minutely, h:hourly, d:daily, b:business days, w:weekly, m:monthly], you can also use more detailed freq like 15min or 3h')\n",
    "parser.add_argument('--checkpoints', type=str, default='./checkpoints/', help='location of model checkpoints')\n",
    "\n",
    "# forecasting task\n",
    "parser.add_argument('--seq_len', type=int, default=24, help='input sequence length')\n",
    "parser.add_argument('--label_len', type=int, default=0, help='start token length')\n",
    "parser.add_argument('--pred_len', type=int, default=24, help='prediction sequence length')\n",
    "\n",
    "\n",
    "# DLinear\n",
    "parser.add_argument('--individual', action='store_true', default=False, help='DLinear: a linear layer for each variate(channel) individually')\n",
    "# Formers \n",
    "\n",
    "parser.add_argument('--embed_type', type=int, default=3, help='0: default 1: value embedding + temporal embedding + positional embedding 2: value embedding + temporal embedding 3: value embedding + positional embedding 4: value embedding')\n",
    "parser.add_argument('--enc_in', type=int, default=V*2, help='encoder input size') # DLinear with --individual, use this hyperparameter as the number of channels\n",
    "parser.add_argument('--dec_in', type=int, default=V, help='decoder input size')\n",
    "parser.add_argument('--c_out', type=int, default=V, help='output size')\n",
    "parser.add_argument('--d_model', type=int, default=512, help='dimension of model')\n",
    "parser.add_argument('--n_heads', type=int, default=8, help='num of heads')\n",
    "parser.add_argument('--e_layers', type=int, default=2, help='num of encoder layers')\n",
    "parser.add_argument('--d_layers', type=int, default=1, help='num of decoder layers')\n",
    "parser.add_argument('--d_ff', type=int, default=2048, help='dimension of fcn')\n",
    "parser.add_argument('--moving_avg', type=int, default=25, help='window size of moving average')\n",
    "parser.add_argument('--factor', type=int, default=1, help='attn factor')\n",
    "parser.add_argument('--distil', action='store_false',\n",
    "                    help='whether to use distilling in encoder, using this argument means not using distilling',\n",
    "                    default=True)\n",
    "parser.add_argument('--dropout', type=float, default=0.05, help='dropout')\n",
    "parser.add_argument('--embed', type=str, default='timeF',\n",
    "                    help='time features encoding, options:[timeF, fixed, learned]')\n",
    "parser.add_argument('--activation', type=str, default='gelu', help='activation')\n",
    "parser.add_argument('--output_attention', action='store_true', help='whether to output attention in ecoder')\n",
    "parser.add_argument('--do_predict', action='store_true', help='whether to predict unseen future data')\n",
    "\n",
    "# optimization\n",
    "parser.add_argument('--num_workers', type=int, default=10, help='data loader num workers')\n",
    "parser.add_argument('--itr', type=int, default=2, help='experiments times')\n",
    "parser.add_argument('--train_epochs', type=int, default=10, help='train epochs')\n",
    "parser.add_argument('--batch_size', type=int, default=32, help='batch size of train input data')\n",
    "parser.add_argument('--patience', type=int, default=3, help='early stopping patience')\n",
    "parser.add_argument('--learning_rate', type=float, default=0.0001, help='optimizer learning rate')\n",
    "parser.add_argument('--des', type=str, default='test', help='exp description')\n",
    "parser.add_argument('--loss', type=str, default='mse', help='loss function')\n",
    "parser.add_argument('--lradj', type=str, default='type1', help='adjust learning rate')\n",
    "parser.add_argument('--use_amp', action='store_true', help='use automatic mixed precision training', default=False)\n",
    "\n",
    "# GPU\n",
    "parser.add_argument('--use_gpu', type=bool, default=True, help='use gpu')\n",
    "parser.add_argument('--gpu', type=int, default=0, help='gpu')\n",
    "parser.add_argument('--use_multi_gpu', action='store_true', help='use multiple gpus', default=False)\n",
    "parser.add_argument('--devices', type=str, default='0,1,2,3', help='device ids of multile gpus')\n",
    "parser.add_argument('--test_flop', action='store_true', default=False, help='See utils/tools for usage')\n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd20f12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import models.InformerDMSFull as autoformer\n",
    "importlib.reload(autoformer)\n",
    "batch_size, lr, patience = 32, 0.0005, 6  # batch_size increased, patience 10 --> 6\n",
    "d, N, he, dropout = 50, 2, 4, 0.2\n",
    "model = autoformer.Model(args).cuda()\n",
    "# Load pretrained weights here.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "#fore_savepath = 'mimic_iii_24h_strats_no_interp_with_ss_fore_blubdiblub.pytorch'\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'max', patience=3)\n",
    "model.load_state_dict(torch.load(\"/home/mitarb/fracarolli/eicu/final/240408_models/eicu_informer.pytorch\"))\n",
    "#model.load_state_dict(torch.load(fore_savepath))\n",
    "model.eval()\n",
    "y_true, y_pred = [], []\n",
    "print(test_input)\n",
    "pbar = tqdm(range(0, len(test_input), batch_size))\n",
    "loss_list = []\n",
    "test_loss = 0\n",
    "test_loss = 0\n",
    "test_loss_part1 = 0\n",
    "test_loss_part2 = 0\n",
    "sofa_losses = []\n",
    "test_loss_sofa_variables = 0\n",
    "accuracy_list = []\n",
    "TP, TN, FP, FN = 0, 0, 0, 0\n",
    "test_loss_sum_square_total = 0\n",
    "test_loss_part1_sum_square_total = 0\n",
    "test_loss_part2_sum_square_total = 0\n",
    "test_loss_sofa_sum_square_total = 0\n",
    "for start in pbar:\n",
    "    print(start)\n",
    "    matrix = torch.tensor(test_input[start:start+batch_size], dtype=torch.float32).cuda()\n",
    "    input_matrix = matrix[:, :24]\n",
    "\n",
    "    #torch.Size([32, 24, 129])\n",
    "    input_mark = torch.arange(0, input_matrix.size(1)).unsqueeze(0).repeat(input_matrix.size(0), 1).cuda()\n",
    "    #torch.Size([32, 24])\n",
    "    input_mask = input_matrix[:, :24, V:]\n",
    "    output_matrix = matrix[:, 24:, :V]\n",
    "    #torch.Size([32, 24, 129])\n",
    "    output_mark = torch.arange(0, output_matrix.size(1)).unsqueeze(0).repeat(input_matrix.size(0), 1).cuda()\n",
    "    #torch.Size([32, 24])\n",
    "    output_mask = matrix[:, 24:, V:]\n",
    "    #torch.Size([32, 24, 129])\n",
    "    \n",
    "    output_mark = torch.arange(0, output_matrix.size(1)).unsqueeze(0).repeat(input_matrix.size(0), 1).cuda()\n",
    "\n",
    "    dec_inp = torch.zeros_like(output_matrix[:, -args.pred_len:, :]).float()\n",
    "    with torch.no_grad():\n",
    "        output = model(input_matrix, input_mark, dec_inp, output_mark)\n",
    "        loss = output_mask[:, -args.pred_len:, :]*(\n",
    "        output-output_matrix[:, -args.pred_len:, :])**2\n",
    "    loss_list.extend(loss.sum(axis=-1).mean(axis=-1).detach().cpu().tolist())\n",
    "    for outpu, real, mask, input_mat in zip(output, output_matrix, output_mask, input_matrix):\n",
    "        a=(get_sofa(outpu*mask, var_to_ind))\n",
    "        b=(get_sofa(real, var_to_ind))\n",
    "        c = (get_sofa(input_mat, var_to_ind)) #SOFA Source (First Day)\n",
    "        prediction_sofa_difference = (sum(a)-sum(c)) >= 2\n",
    "        gold_sofa_difference = (sum(b)-sum(c)) >= 2\n",
    "        accuracy_list.append(prediction_sofa_difference == gold_sofa_difference)\n",
    "        if gold_sofa_difference:\n",
    "            if prediction_sofa_difference: TP +=1\n",
    "            else: FP += 1\n",
    "        else:\n",
    "            if prediction_sofa_difference: TN +=1\n",
    "            else: FN += 1\n",
    "        c, d = torch.tensor(a, dtype=torch.float32), torch.tensor(b, dtype=torch.float32)\n",
    "        sofa_losses.append(((c-d)**2).sum(axis=-1).mean().item())\n",
    "    loss_old = loss\n",
    "    loss = loss.sum(axis=-1).mean(axis=-1).sum()\n",
    "    loss_part1 = loss_old[:, :8].sum(axis=-1).mean()\n",
    "    loss_part2 = loss_old[:, 8:].sum(axis=-1).mean()\n",
    "    test_loss+=loss\n",
    "    test_loss_part1+=loss_part1\n",
    "    test_loss_part2+=loss_part2\n",
    "    test_loss_sofa_variables+=loss_old[:, :, factor_indices].sum(axis=-1).mean()\n",
    "    test_loss_sum_square_total += torch.sum(loss_old.sum(axis=-1).mean(axis=-1)**2)\n",
    "    test_loss_part1_sum_square_total += torch.sum(loss_old[:, :8].sum(axis=-1).mean(axis=-1)**2)\n",
    "    test_loss_part2_sum_square_total += torch.sum(loss_old[:, 8:].sum(axis=-1).mean(axis=-1)**2)\n",
    "    test_loss_sofa_sum_square_total += torch.sum(loss_old[:, :, factor_indices].sum(axis=-1).mean(axis=-1)**2)\n",
    "test_loss_p = test_loss/len(test_input)\n",
    "se_loss_p = torch.sqrt(test_loss_sum_square_total / len(test_input) - test_loss_p**2)/len(test_input)\n",
    "test_loss_part1_p = test_loss_part1*batch_size/len(test_input)\n",
    "test_loss_part2_p = test_loss_part2*batch_size/len(test_input)\n",
    "test_loss_sofa_variables_p = test_loss_sofa_variables*batch_size/len(test_input)\n",
    "sofa_loss = sum(sofa_losses)/len(sofa_losses)\n",
    "accuracy_sepsis = sum(accuracy_list)/len(accuracy_list)\n",
    "se_accuracy_sepsis = np.sqrt(accuracy_sepsis * (1-accuracy_sepsis)/len(test_input))\n",
    "se_loss_part1_p = torch.sqrt(test_loss_part1_sum_square_total / len(test_input) - test_loss_part1_p**2)/len(test_input)\n",
    "se_loss_part2_p = torch.sqrt(test_loss_part2_sum_square_total / len(test_input) - test_loss_part2_p**2)/len(test_input)\n",
    "se_loss_sofa_p  = torch.sqrt(test_loss_sofa_sum_square_total / len(test_input)  - test_loss_sofa_variables_p**2)/len(test_input)\n",
    "\n",
    "print(test_loss_p, test_loss_part1_p, test_loss_part2_p, sofa_loss, test_loss_sofa_variables_p)\n",
    "\n",
    "model_name = 'Informer&Dense&DMS&-&-&'\n",
    "MSE_string = \"{:.3f}\".format(test_loss_p.item()) + \"$_{[\" + \"{:.3f}\".format(test_loss_p.item()-1.96*se_loss_p.item()) +','+ \"{:.3f}\".format(test_loss_p.item()+1.96*se_loss_p.item()) + ']}$&'\n",
    "SOFA_string = \"{:.3f}\".format(sofa_loss)+ '$_{['+\"{:.3f}\".format(sofa_loss-1.96*se_loss_sofa_p.item())+','+\"{:.3f}\".format(sofa_loss+1.96*se_loss_sofa_p.item())+ ']}$&'\n",
    "MSE8_string = \"{:.3f}\".format(test_loss_part1_p.item()) + \"$_{[\" + \"{:.3f}\".format(test_loss_part1_p.item()-1.96*se_loss_part1_p.item()) +','+ \"{:.3f}\".format(test_loss_part1_p.item()+1.96*se_loss_part1_p.item()) + ']}$&'\n",
    "MSE16_string = \"{:.3f}\".format(test_loss_part2_p.item()) + \"$_{[\" + \"{:.3f}\".format(test_loss_part2_p.item()-1.96*se_loss_part2_p.item()) +','+ \"{:.3f}\".format(test_loss_part2_p.item()+1.96*se_loss_part2_p.item()) + ']}$&'\n",
    "SOFA_acc_string = \"{:.2f}\".format(accuracy_sepsis*100)+ '$_{['+\"{:.2f}\".format(100*(accuracy_sepsis-1.96*se_accuracy_sepsis))+','+\"{:.2f}\".format(100*(accuracy_sepsis+1.96*se_accuracy_sepsis))+ ']}$&'\n",
    "print(model_name,MSE_string, SOFA_string, sep='',file=open('eicu_table_sample_sepsis1.txt','a'))\n",
    "print(model_name,MSE8_string, MSE16_string, sep='',file=open('eicu_table_sample_sepsis2.txt','a'))\n",
    "print(model_name,TP,'&',TN,'&',FP,'&',FN,'&', TP+TN+FP+FN,'&', len(test_input), sep='',file=open('eicu_table_sample_sepsis3.txt','a')) \n",
    "print(model_name,len(test_input),'&',SOFA_acc_string, sep='',file=open('eicu_table_sample_sepsis4.txt','a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4354a52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model, e_layers, d_layers, d_ff = 512, 2, 1, 2048\n",
    "number_of_epochs = 100\n",
    "V, D = 98, 17\n",
    "fore_max_len = 2640\n",
    "print('loading of joblib completed')\n",
    "\n",
    "\n",
    "\n",
    "import argparse\n",
    "parser = argparse.ArgumentParser(description='Autoformer & Transformer family for Time Series Forecasting')\n",
    "\n",
    "# basic config\n",
    "parser.add_argument('--is_training', type=int, required=False, default=1, help='status')\n",
    "parser.add_argument('--train_only', type=bool, required=False, default=False, help='perform training on full input dataset without validation and testing')\n",
    "parser.add_argument('--model_id', type=str, required=False, default='test', help='model id')\n",
    "parser.add_argument('--model', type=str, required=False, default='Autoformer',\n",
    "                    help='model name, options: [Autoformer, Informer, Transformer]')\n",
    "\n",
    "# data loader\n",
    "parser.add_argument('--data', type=str, required=False, default='ETTm1', help='dataset type')\n",
    "parser.add_argument('--root_path', type=str, default='./data/ETT/', help='root path of the data file')\n",
    "parser.add_argument('--data_path', type=str, default='ETTh1.csv', help='data file')\n",
    "parser.add_argument('--features', type=str, default='M',\n",
    "                    help='forecasting task, options:[M, S, MS]; M:multivariate predict multivariate, S:univariate predict univariate, MS:multivariate predict univariate')\n",
    "parser.add_argument('--target', type=str, default='OT', help='target feature in S or MS task')\n",
    "parser.add_argument('--freq', type=str, default='h',\n",
    "                    help='freq for time features encoding, options:[s:secondly, t:minutely, h:hourly, d:daily, b:business days, w:weekly, m:monthly], you can also use more detailed freq like 15min or 3h')\n",
    "parser.add_argument('--checkpoints', type=str, default='./checkpoints/', help='location of model checkpoints')\n",
    "\n",
    "# forecasting task\n",
    "parser.add_argument('--seq_len', type=int, default=24, help='input sequence length')\n",
    "parser.add_argument('--label_len', type=int, default=0, help='start token length')\n",
    "parser.add_argument('--pred_len', type=int, default=24, help='prediction sequence length')\n",
    "\n",
    "\n",
    "# DLinear\n",
    "parser.add_argument('--individual', action='store_true', default=False, help='DLinear: a linear layer for each variate(channel) individually')\n",
    "# Formers \n",
    "#Michi: bisher nur default=3 zum laufen gebracht\n",
    "parser.add_argument('--embed_type', type=int, default=3, help='0: default 1: value embedding + temporal embedding + positional embedding 2: value embedding + temporal embedding 3: value embedding + positional embedding 4: value embedding')\n",
    "parser.add_argument('--enc_in', type=int, default=V*2, help='encoder input size') # DLinear with --individual, use this hyperparameter as the number of channels\n",
    "parser.add_argument('--dec_in', type=int, default=V, help='decoder input size')\n",
    "parser.add_argument('--c_out', type=int, default=V, help='output size')\n",
    "parser.add_argument('--d_model', type=int, default=d_model, help='dimension of model')\n",
    "parser.add_argument('--n_heads', type=int, default=8, help='num of heads')\n",
    "parser.add_argument('--e_layers', type=int, default=e_layers, help='num of encoder layers')\n",
    "parser.add_argument('--d_layers', type=int, default=d_layers, help='num of decoder layers')\n",
    "parser.add_argument('--d_ff', type=int, default=d_ff, help='dimension of fcn')\n",
    "parser.add_argument('--moving_avg', type=int, default=25, help='window size of moving average')\n",
    "parser.add_argument('--factor', type=int, default=1, help='attn factor')\n",
    "parser.add_argument('--distil', action='store_false',\n",
    "                    help='whether to use distilling in encoder, using this argument means not using distilling',\n",
    "                    default=True)\n",
    "parser.add_argument('--dropout', type=float, default=0.05, help='dropout')\n",
    "parser.add_argument('--embed', type=str, default='timeF',\n",
    "                    help='time features encoding, options:[timeF, fixed, learned]')\n",
    "parser.add_argument('--activation', type=str, default='gelu', help='activation')\n",
    "parser.add_argument('--output_attention', action='store_true', help='whether to output attention in ecoder')\n",
    "parser.add_argument('--do_predict', action='store_true', help='whether to predict unseen future data')\n",
    "\n",
    "# optimization\n",
    "parser.add_argument('--num_workers', type=int, default=10, help='data loader num workers')\n",
    "parser.add_argument('--itr', type=int, default=2, help='experiments times')\n",
    "parser.add_argument('--train_epochs', type=int, default=10, help='train epochs')\n",
    "parser.add_argument('--batch_size', type=int, default=32, help='batch size of train input data')\n",
    "parser.add_argument('--patience', type=int, default=3, help='early stopping patience')\n",
    "parser.add_argument('--learning_rate', type=float, default=0.0001, help='optimizer learning rate')\n",
    "parser.add_argument('--des', type=str, default='test', help='exp description')\n",
    "parser.add_argument('--loss', type=str, default='mse', help='loss function')\n",
    "parser.add_argument('--lradj', type=str, default='type1', help='adjust learning rate')\n",
    "parser.add_argument('--use_amp', action='store_true', help='use automatic mixed precision training', default=False)\n",
    "\n",
    "# GPU\n",
    "parser.add_argument('--use_gpu', type=bool, default=True, help='use gpu')\n",
    "parser.add_argument('--gpu', type=int, default=0, help='gpu')\n",
    "parser.add_argument('--use_multi_gpu', action='store_true', help='use multiple gpus', default=False)\n",
    "parser.add_argument('--devices', type=str, default='0,1,2,3', help='device ids of multile gpus')\n",
    "parser.add_argument('--test_flop', action='store_true', default=False, help='See utils/tools for usage')\n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7999e5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import models.InformerAutoregressiveFull as autoformer\n",
    "importlib.reload(autoformer)\n",
    "batch_size, lr, patience = 32, 0.0005, 6  # batch_size increased, patience 10 --> 6\n",
    "d, N, he, dropout = 50, 2, 4, 0.2\n",
    "model = autoformer.Model(args).cuda()\n",
    "# Load pretrained weights here.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "#fore_savepath = 'mimic_iii_24h_strats_no_interp_with_ss_fore_blubdiblub.pytorch'\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'max', patience=3)\n",
    "model.load_state_dict(torch.load(\"/home/mitarb/fracarolli/eicu/final/240408_models/eicu_mod_dense_SS_RI_D.pytorch\"))\n",
    "#model.load_state_dict(torch.load(fore_savepath))\n",
    "model.eval()\n",
    "y_true, y_pred = [], []\n",
    "print(test_input)\n",
    "pbar = tqdm(range(0, len(test_input), batch_size))\n",
    "loss_list = []\n",
    "test_loss = 0\n",
    "test_loss_part1 = 0\n",
    "test_loss_part2 = 0\n",
    "test_loss_sofa_variables = 0\n",
    "sofa_losses = []\n",
    "accuracy_list = []\n",
    "TP, TN, FP, FN = 0, 0, 0, 0\n",
    "test_loss_sum_square_total = 0\n",
    "test_loss_part1_sum_square_total = 0\n",
    "test_loss_part2_sum_square_total = 0\n",
    "test_loss_sofa_sum_square_total = 0\n",
    "for start in pbar:\n",
    "    print(start)\n",
    "    matrix = torch.tensor(test_input[start:start+batch_size], dtype=torch.float32).cuda()\n",
    "    input_matrix = matrix[:, :24]\n",
    "\n",
    "    #torch.Size([32, 24, 129])\n",
    "    input_mark = torch.arange(0, input_matrix.size(1)).unsqueeze(0).repeat(input_matrix.size(0), 1).cuda()\n",
    "    #torch.Size([32, 24])\n",
    "    input_mask = input_matrix[:, :24, V:]\n",
    "    output_matrix = matrix[:, 24:, :V]\n",
    "    #torch.Size([32, 24, 129])\n",
    "    output_mark = torch.arange(0, output_matrix.size(1)).unsqueeze(0).repeat(input_matrix.size(0), 1).cuda()\n",
    "    #torch.Size([32, 24])\n",
    "    output_mask = matrix[:, 24:, V:]\n",
    "    \n",
    "    output_mark = torch.arange(0, output_matrix.size(1)).unsqueeze(0).repeat(input_matrix.size(0), 1).cuda()\n",
    "    dec_inp = torch.zeros_like(output_matrix[:, -args.pred_len:, :]).float()\n",
    "    #torch.Size([32, 24, 129])\n",
    "    with torch.no_grad():\n",
    "        output = model(input_matrix, input_mark, dec_inp, output_mark, trainn=False)\n",
    "        loss = output_mask[:, -args.pred_len:, :]*(\n",
    "        output-output_matrix[:, -args.pred_len:, :])**2\n",
    "    loss_list.extend(loss.sum(axis=-1).mean(axis=-1).detach().cpu().tolist())\n",
    "    for outpu, real, mask, input_mat in zip(output, output_matrix, output_mask, input_matrix):\n",
    "        a = (get_sofa(outpu*mask, var_to_ind))\n",
    "        b = (get_sofa(real, var_to_ind))\n",
    "        c = (get_sofa(input_mat, var_to_ind)) #SOFA Source (First Day)\n",
    "        prediction_sofa_difference = (sum(a)-sum(c)) >= 2\n",
    "        gold_sofa_difference = (sum(b)-sum(c)) >= 2\n",
    "        accuracy_list.append(prediction_sofa_difference == gold_sofa_difference)\n",
    "        if gold_sofa_difference:\n",
    "            if prediction_sofa_difference: TP +=1\n",
    "            else: FP += 1\n",
    "        else:\n",
    "            if prediction_sofa_difference: TN +=1\n",
    "            else: FN += 1\n",
    "        c, d = torch.tensor(a, dtype=torch.float32), torch.tensor(b, dtype=torch.float32)\n",
    "        sofa_losses.append(((c-d)**2).sum(axis=-1).mean().item())\n",
    "    loss_old = loss\n",
    "    loss = loss.sum(axis=-1).mean(axis=-1).sum()\n",
    "    loss_part1 = loss_old[:, :8].sum(axis=-1).mean()\n",
    "    loss_part2 = loss_old[:, 8:].sum(axis=-1).mean()\n",
    "    test_loss+=loss\n",
    "    test_loss_part1+=loss_part1\n",
    "    test_loss_part2+=loss_part2\n",
    "    test_loss_sofa_variables+=loss_old[:, :, factor_indices].sum(axis=-1).mean()\n",
    "    test_loss_sum_square_total += torch.sum(loss_old.sum(axis=-1).mean(axis=-1)**2)\n",
    "    test_loss_part1_sum_square_total += torch.sum(loss_old[:, :8].sum(axis=-1).mean(axis=-1)**2)\n",
    "    test_loss_part2_sum_square_total += torch.sum(loss_old[:, 8:].sum(axis=-1).mean(axis=-1)**2)\n",
    "    test_loss_sofa_sum_square_total += torch.sum(loss_old[:, :, factor_indices].sum(axis=-1).mean(axis=-1)**2)\n",
    "test_loss_p = test_loss/len(test_input)\n",
    "se_loss_p = torch.sqrt(test_loss_sum_square_total / len(test_input) - test_loss_p**2)/len(test_input)\n",
    "test_loss_part1_p = test_loss_part1*batch_size/len(test_input)\n",
    "test_loss_part2_p = test_loss_part2*batch_size/len(test_input)\n",
    "test_loss_sofa_variables_p = test_loss_sofa_variables*batch_size/len(test_input)\n",
    "sofa_loss = sum(sofa_losses)/len(sofa_losses)\n",
    "accuracy_sepsis = sum(accuracy_list)/len(accuracy_list)\n",
    "se_accuracy_sepsis = np.sqrt(accuracy_sepsis * (1-accuracy_sepsis)/len(test_input))\n",
    "se_loss_part1_p = torch.sqrt(test_loss_part1_sum_square_total / len(test_input) - test_loss_part1_p**2)/len(test_input)\n",
    "se_loss_part2_p = torch.sqrt(test_loss_part2_sum_square_total / len(test_input) - test_loss_part2_p**2)/len(test_input)\n",
    "se_loss_sofa_p  = torch.sqrt(test_loss_sofa_sum_square_total / len(test_input)  - test_loss_sofa_variables_p**2)/len(test_input)\n",
    "print(test_loss_p, test_loss_part1_p, test_loss_part2_p, sofa_loss, test_loss_sofa_variables_p, accuracy_sepsis)\n",
    "print('Informer_IMS_SF_D',test_loss_p.item(), test_loss_part1_p.item(), test_loss_part2_p.item(), sofa_loss, test_loss_sofa_variables_p.item(), accuracy_sepsis, sep=',',file=open('results.txt','a'))\n",
    "\n",
    "\n",
    "\n",
    "model_name = '9&Dense&IMS&SS-RI&No&'\n",
    "MSE_string = \"{:.3f}\".format(test_loss_p.item()) + \"$_{[\" + \"{:.3f}\".format(test_loss_p.item()-1.96*se_loss_p.item()) +','+ \"{:.3f}\".format(test_loss_p.item()+1.96*se_loss_p.item()) + ']}$&'\n",
    "SOFA_string = \"{:.3f}\".format(sofa_loss)+ '$_{['+\"{:.3f}\".format(sofa_loss-1.96*se_loss_sofa_p.item())+','+\"{:.3f}\".format(sofa_loss+1.96*se_loss_sofa_p.item())+ ']}$&'\n",
    "MSE8_string = \"{:.3f}\".format(test_loss_part1_p.item()) + \"$_{[\" + \"{:.3f}\".format(test_loss_part1_p.item()-1.96*se_loss_part1_p.item()) +','+ \"{:.3f}\".format(test_loss_part1_p.item()+1.96*se_loss_part1_p.item()) + ']}$&'\n",
    "MSE16_string = \"{:.3f}\".format(test_loss_part2_p.item()) + \"$_{[\" + \"{:.3f}\".format(test_loss_part2_p.item()-1.96*se_loss_part2_p.item()) +','+ \"{:.3f}\".format(test_loss_part2_p.item()+1.96*se_loss_part2_p.item()) + ']}$&'\n",
    "SOFA_acc_string = \"{:.2f}\".format(accuracy_sepsis*100)+ '$_{['+\"{:.2f}\".format(100*(accuracy_sepsis-1.96*se_accuracy_sepsis))+','+\"{:.2f}\".format(100*(accuracy_sepsis+1.96*se_accuracy_sepsis))+ ']}$&'\n",
    "print(model_name,MSE_string, SOFA_string, sep='',file=open('eicu_table_sample_sepsis1.txt','a'))\n",
    "print(model_name,MSE8_string, MSE16_string, sep='',file=open('eicu_table_sample_sepsis2.txt','a'))\n",
    "print(model_name,TP,'&',TN,'&',FP,'&',FN,'&', TP+TN+FP+FN,'&', len(test_input), sep='',file=open('eicu_table_sample_sepsis3.txt','a')) \n",
    "print(model_name,len(test_input),'&',SOFA_acc_string, sep='',file=open('eicu_table_sample_sepsis4.txt','a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64fae56",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import models.InformerAutoregressiveFull as autoformer\n",
    "importlib.reload(autoformer)\n",
    "batch_size, lr, patience = 32, 0.0005, 6  # batch_size increased, patience 10 --> 6\n",
    "d, N, he, dropout = 50, 2, 4, 0.2\n",
    "model = autoformer.Model(args).cuda()\n",
    "# Load pretrained weights here.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "#fore_savepath = 'mimic_iii_24h_strats_no_interp_with_ss_fore_blubdiblub.pytorch'\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'max', patience=3)\n",
    "model.load_state_dict(torch.load(\"/home/mitarb/fracarolli/eicu/final/240408_models/eicu_mod_dense_SS_RI_BP.pytorch\"))\n",
    "#model.load_state_dict(torch.load(fore_savepath))\n",
    "model.eval()\n",
    "y_true, y_pred = [], []\n",
    "print(test_input)\n",
    "pbar = tqdm(range(0, len(test_input), batch_size))\n",
    "loss_list = []\n",
    "test_loss = 0\n",
    "test_loss_part1 = 0\n",
    "test_loss_part2 = 0\n",
    "test_loss_sofa_variables = 0\n",
    "sofa_losses = []\n",
    "accuracy_list = []\n",
    "TP, TN, FP, FN = 0, 0, 0, 0\n",
    "test_loss_sum_square_total = 0\n",
    "test_loss_part1_sum_square_total = 0\n",
    "test_loss_part2_sum_square_total = 0\n",
    "test_loss_sofa_sum_square_total = 0\n",
    "for start in pbar:\n",
    "    print(start)\n",
    "    matrix = torch.tensor(test_input[start:start+batch_size], dtype=torch.float32).cuda()\n",
    "    input_matrix = matrix[:, :24]\n",
    "\n",
    "    #torch.Size([32, 24, 129])\n",
    "    input_mark = torch.arange(0, input_matrix.size(1)).unsqueeze(0).repeat(input_matrix.size(0), 1).cuda()\n",
    "    #torch.Size([32, 24])\n",
    "    input_mask = input_matrix[:, :24, V:]\n",
    "    output_matrix = matrix[:, 24:, :V]\n",
    "    #torch.Size([32, 24, 129])\n",
    "    output_mark = torch.arange(0, output_matrix.size(1)).unsqueeze(0).repeat(input_matrix.size(0), 1).cuda()\n",
    "    #torch.Size([32, 24])\n",
    "    output_mask = matrix[:, 24:, V:]\n",
    "    \n",
    "    output_mark = torch.arange(0, output_matrix.size(1)).unsqueeze(0).repeat(input_matrix.size(0), 1).cuda()\n",
    "    dec_inp = torch.zeros_like(output_matrix[:, -args.pred_len:, :]).float()\n",
    "    #torch.Size([32, 24, 129])\n",
    "    with torch.no_grad():\n",
    "        output = model(input_matrix, input_mark, dec_inp, output_mark, trainn=False)\n",
    "        loss = output_mask[:, -args.pred_len:, :]*(\n",
    "        output-output_matrix[:, -args.pred_len:, :])**2\n",
    "    loss_list.extend(loss.sum(axis=-1).mean(axis=-1).detach().cpu().tolist())\n",
    "    for outpu, real, mask, input_mat in zip(output, output_matrix, output_mask, input_matrix):\n",
    "        a = (get_sofa(outpu*mask, var_to_ind))\n",
    "        b = (get_sofa(real, var_to_ind))\n",
    "        c = (get_sofa(input_mat, var_to_ind)) #SOFA Source (First Day)\n",
    "        prediction_sofa_difference = (sum(a)-sum(c)) >= 2\n",
    "        gold_sofa_difference = (sum(b)-sum(c)) >= 2\n",
    "        accuracy_list.append(prediction_sofa_difference == gold_sofa_difference)\n",
    "        if gold_sofa_difference:\n",
    "            if prediction_sofa_difference: TP +=1\n",
    "            else: FP += 1\n",
    "        else:\n",
    "            if prediction_sofa_difference: TN +=1\n",
    "            else: FN += 1\n",
    "        c, d = torch.tensor(a, dtype=torch.float32), torch.tensor(b, dtype=torch.float32)\n",
    "        sofa_losses.append(((c-d)**2).sum(axis=-1).mean().item())\n",
    "    loss_old = loss\n",
    "    loss = loss.sum(axis=-1).mean(axis=-1).sum()\n",
    "    loss_part1 = loss_old[:, :8].sum(axis=-1).mean()\n",
    "    loss_part2 = loss_old[:, 8:].sum(axis=-1).mean()\n",
    "    test_loss+=loss\n",
    "    test_loss_part1+=loss_part1\n",
    "    test_loss_part2+=loss_part2\n",
    "    test_loss_sofa_variables+=loss_old[:, :, factor_indices].sum(axis=-1).mean()\n",
    "    test_loss_sum_square_total += torch.sum(loss_old.sum(axis=-1).mean(axis=-1)**2)\n",
    "    test_loss_part1_sum_square_total += torch.sum(loss_old[:, :8].sum(axis=-1).mean(axis=-1)**2)\n",
    "    test_loss_part2_sum_square_total += torch.sum(loss_old[:, 8:].sum(axis=-1).mean(axis=-1)**2)\n",
    "    test_loss_sofa_sum_square_total += torch.sum(loss_old[:, :, factor_indices].sum(axis=-1).mean(axis=-1)**2)\n",
    "test_loss_p = test_loss/len(test_input)\n",
    "se_loss_p = torch.sqrt(test_loss_sum_square_total / len(test_input) - test_loss_p**2)/len(test_input)\n",
    "test_loss_part1_p = test_loss_part1*batch_size/len(test_input)\n",
    "test_loss_part2_p = test_loss_part2*batch_size/len(test_input)\n",
    "test_loss_sofa_variables_p = test_loss_sofa_variables*batch_size/len(test_input)\n",
    "sofa_loss = sum(sofa_losses)/len(sofa_losses)\n",
    "accuracy_sepsis = sum(accuracy_list)/len(accuracy_list)\n",
    "se_accuracy_sepsis = np.sqrt(accuracy_sepsis * (1-accuracy_sepsis)/len(test_input))\n",
    "se_loss_part1_p = torch.sqrt(test_loss_part1_sum_square_total / len(test_input) - test_loss_part1_p**2)/len(test_input)\n",
    "se_loss_part2_p = torch.sqrt(test_loss_part2_sum_square_total / len(test_input) - test_loss_part2_p**2)/len(test_input)\n",
    "se_loss_sofa_p  = torch.sqrt(test_loss_sofa_sum_square_total / len(test_input)  - test_loss_sofa_variables_p**2)/len(test_input)\n",
    "print(test_loss_p, test_loss_part1_p, test_loss_part2_p, sofa_loss, test_loss_sofa_variables_p, accuracy_sepsis)\n",
    "print('Informer_IMS_SF_D',test_loss_p.item(), test_loss_part1_p.item(), test_loss_part2_p.item(), sofa_loss, test_loss_sofa_variables_p.item(), accuracy_sepsis, sep=',',file=open('results.txt','a'))\n",
    "\n",
    "\n",
    "\n",
    "model_name = '9&Dense&IMS&SS-RI&Yes&'\n",
    "MSE_string = \"{:.3f}\".format(test_loss_p.item()) + \"$_{[\" + \"{:.3f}\".format(test_loss_p.item()-1.96*se_loss_p.item()) +','+ \"{:.3f}\".format(test_loss_p.item()+1.96*se_loss_p.item()) + ']}$&'\n",
    "SOFA_string = \"{:.3f}\".format(sofa_loss)+ '$_{['+\"{:.3f}\".format(sofa_loss-1.96*se_loss_sofa_p.item())+','+\"{:.3f}\".format(sofa_loss+1.96*se_loss_sofa_p.item())+ ']}$&'\n",
    "MSE8_string = \"{:.3f}\".format(test_loss_part1_p.item()) + \"$_{[\" + \"{:.3f}\".format(test_loss_part1_p.item()-1.96*se_loss_part1_p.item()) +','+ \"{:.3f}\".format(test_loss_part1_p.item()+1.96*se_loss_part1_p.item()) + ']}$&'\n",
    "MSE16_string = \"{:.3f}\".format(test_loss_part2_p.item()) + \"$_{[\" + \"{:.3f}\".format(test_loss_part2_p.item()-1.96*se_loss_part2_p.item()) +','+ \"{:.3f}\".format(test_loss_part2_p.item()+1.96*se_loss_part2_p.item()) + ']}$&'\n",
    "SOFA_acc_string = \"{:.2f}\".format(accuracy_sepsis*100)+ '$_{['+\"{:.2f}\".format(100*(accuracy_sepsis-1.96*se_accuracy_sepsis))+','+\"{:.2f}\".format(100*(accuracy_sepsis+1.96*se_accuracy_sepsis))+ ']}$&'\n",
    "print(model_name,MSE_string, SOFA_string, sep='',file=open('eicu_table_sample_sepsis1.txt','a'))\n",
    "print(model_name,MSE8_string, MSE16_string, sep='',file=open('eicu_table_sample_sepsis2.txt','a'))\n",
    "print(model_name,TP,'&',TN,'&',FP,'&',FN,'&', TP+TN+FP+FN,'&', len(test_input), sep='',file=open('eicu_table_sample_sepsis3.txt','a')) \n",
    "print(model_name,len(test_input),'&',SOFA_acc_string, sep='',file=open('eicu_table_sample_sepsis4.txt','a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5130e78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_divisor=1\n",
    "class CVE(nn.Module):\n",
    "    def __init__(self, hid_units, output_dim):\n",
    "        super(CVE, self).__init__()\n",
    "        self.hid_units = hid_units\n",
    "        self.output_dim = output_dim\n",
    "        self.W1 = nn.Parameter(torch.Tensor(1, self.hid_units))\n",
    "        self.b1 = nn.Parameter(torch.Tensor(self.hid_units))\n",
    "        self.W2 = nn.Parameter(torch.Tensor(self.hid_units, self.output_dim))\n",
    "        nn.init.xavier_uniform_(self.W1)\n",
    "        nn.init.zeros_(self.b1)\n",
    "        nn.init.xavier_uniform_(self.W2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(-1)\n",
    "        x = torch.matmul(torch.tanh(torch.add(torch.matmul(x, self.W1), self.b1)), self.W2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hid_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.hid_dim = hid_dim\n",
    "        self.W = nn.Parameter(torch.Tensor(50, self.hid_dim))\n",
    "        self.b = nn.Parameter(torch.Tensor(self.hid_dim))\n",
    "        self.u = nn.Parameter(torch.Tensor(self.hid_dim, 1))\n",
    "        nn.init.xavier_uniform_(self.W)\n",
    "        nn.init.zeros_(self.b)\n",
    "        nn.init.xavier_uniform_(self.u)\n",
    "\n",
    "    def forward(self, x, mask, mask_value=-1e30):\n",
    "        attn_weights = torch.matmul(torch.tanh(torch.add(torch.matmul(x, self.W), self.b)), self.u)\n",
    "        mask = mask.unsqueeze(-1)\n",
    "        attn_weights = mask * attn_weights + (1 - mask) * mask_value\n",
    "        attn_weights = F.softmax(attn_weights, dim=-2)\n",
    "        return attn_weights\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, N=2, h=8, dk=None, dv=None, dff=None, dropout=0, d=8):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.N, self.h, self.dk, self.dv, self.dff, self.dropout = N, h, dk, dv, dff, dropout\n",
    "        self.epsilon = torch.finfo(torch.float32).eps * torch.finfo(torch.float32).eps\n",
    "        self.Wq = nn.Parameter(torch.Tensor(N, h, d, dk))\n",
    "        self.Wk = nn.Parameter(torch.Tensor(N, h, d, dk))\n",
    "        self.Wv = nn.Parameter(torch.Tensor(N, h, d, dv))\n",
    "        self.Wo = nn.Parameter(torch.Tensor(N, dv * h, d))\n",
    "        self.W1 = nn.Parameter(torch.Tensor(N, d, dff))\n",
    "        self.b1 = nn.Parameter(torch.Tensor(N, dff))\n",
    "        self.W2 = nn.Parameter(torch.Tensor(N, dff, d))\n",
    "        self.b2 = nn.Parameter(torch.Tensor(N, d))\n",
    "        self.gamma = nn.Parameter(torch.Tensor(2 * N))\n",
    "        self.beta = nn.Parameter(torch.Tensor(2 * N))\n",
    "        nn.init.xavier_uniform_(self.Wq)\n",
    "        nn.init.xavier_uniform_(self.Wk)\n",
    "        nn.init.xavier_uniform_(self.Wv)\n",
    "        nn.init.xavier_uniform_(self.Wo)\n",
    "        nn.init.xavier_uniform_(self.W1)\n",
    "        nn.init.zeros_(self.b1)\n",
    "        nn.init.xavier_uniform_(self.W2)\n",
    "        nn.init.zeros_(self.b2)\n",
    "        nn.init.ones_(self.gamma)\n",
    "        nn.init.zeros_(self.beta)\n",
    "        self.training = False\n",
    "        \n",
    "    def forward(self, x, mask, mask_value=-1e-30):\n",
    "        mask = mask.unsqueeze(-2)\n",
    "        for i in range(self.N):\n",
    "            # MHA\n",
    "            mha_ops = []\n",
    "            for j in range(self.h):\n",
    "                q = torch.matmul(x, self.Wq[i, j, :, :])          \n",
    "                k = torch.matmul(x, self.Wk[i, j, :, :]).permute(0, 2, 1)\n",
    "                v = torch.matmul(x, self.Wv[i, j, :, :])\n",
    "                A = torch.matmul(q, k)\n",
    "                # Mask unobserved steps.\n",
    "                A = mask * A + (1 - mask) * mask_value\n",
    "                # Mask for attention dropout.\n",
    "                if self.training:\n",
    "                    dp_mask = (torch.rand(A.shape, device=A.device) >= self.dropout).float()\n",
    "                    A = A * dp_mask + (1 - dp_mask) * mask_value\n",
    "                A = F.softmax(A, dim=-1)\n",
    "                mha_ops.append(torch.matmul(A, v))\n",
    "            conc = torch.cat(mha_ops, dim=-1)\n",
    "            proj = torch.matmul(conc, self.Wo[i, :, :])\n",
    "            # Dropout.\n",
    "            if self.training:\n",
    "                proj = F.dropout(proj, p=self.dropout)\n",
    "            # Add & LN\n",
    "            x = x + proj\n",
    "            mean = x.mean(dim=-1, keepdim=True)\n",
    "            variance = torch.mean(torch.square(x - mean), dim=-1, keepdim=True)\n",
    "            std = torch.sqrt(variance + self.epsilon)\n",
    "            x = (x - mean) / std\n",
    "            x = x * self.gamma[2 * i] + self.beta[2 * i]\n",
    "            # FFN\n",
    "            ffn_op = torch.add(torch.matmul(F.relu(torch.add(\n",
    "                torch.matmul(x, self.W1[i, :, :]), self.b1[i, :])),\n",
    "                                            self.W2[i, :, :]), self.b2[i, :])\n",
    "            # Dropout.\n",
    "            if self.training:\n",
    "                ffn_op = F.dropout(ffn_op, p=self.dropout)\n",
    "            # Add & LN\n",
    "            x = x + ffn_op\n",
    "            mean = x.mean(dim=-1, keepdim=True)\n",
    "            variance = torch.mean(torch.square(x - mean), dim=-1, keepdim=True)\n",
    "            std = torch.sqrt(variance + self.epsilon)\n",
    "            x = (x - mean) / std\n",
    "            x = x * self.gamma[2 * i + 1] + self.beta[2 * i + 1]\n",
    "        return x\n",
    "\n",
    "\n",
    "class StratsModel(nn.Module):\n",
    "    def __init__(self, D, max_len, V, d, N, he, dropout, forecast=False, autoregressive=True):\n",
    "        super(StratsModel, self).__init__()\n",
    "        self.forecast = forecast\n",
    "        self.autoregressive=autoregressive\n",
    "        self.demo_enc = nn.Sequential(nn.Linear(D, D * d),\n",
    "                                      nn.Tanh(),\n",
    "                                      nn.Linear(D * d, d),\n",
    "                                      nn.Tanh())\n",
    "        self.varis_emb = nn.Embedding(V + 1, d)\n",
    "        nn.init.uniform_(self.varis_emb.weight, a=-0.05, b=0.05)\n",
    "        self.values_emb = CVE(int(np.sqrt(d)), d)\n",
    "        self.times_emb = CVE(int(np.sqrt(d)), d)\n",
    "        # self.comb_emb = nn.LayerNorm(d)\n",
    "        self.transformer = Transformer(N, he, dk=d//he, dv=d//he, dff=2*d, dropout=dropout, d=d)\n",
    "\n",
    "        if autoregressive:\n",
    "            self.linearize = nn.Linear(50, V)\n",
    "            self.dec = nn.TransformerDecoderLayer(d_model=V, dim_feedforward=V, nhead=1, batch_first=True)\n",
    "        else:\n",
    "            self.attention = Attention(2 * d)\n",
    "            self.attentions = nn.ModuleList([Attention(2*d) for i in range(24)])\n",
    "            # self.concat = nn.Linear(2 * d, 1)\n",
    "            self.fore_op_calc = nn.Linear(2 * d, V)\n",
    "            self.op_calc = nn.Linear(V, 1)\n",
    "            self.sig = nn.Sigmoid()        \n",
    "        \n",
    "\n",
    "    def forward(self, demo, times, values, varis, trainn=False, tgt=torch.zeros(32, 1, V).cuda()):\n",
    "        demo_enc = self.demo_enc(demo)\n",
    "        varis_emb = self.varis_emb(varis)\n",
    "        values_emb = self.values_emb(values)\n",
    "        times_emb = self.times_emb(times)\n",
    "\n",
    "        comb_emb = (torch.sum(torch.stack([varis_emb, values_emb, times_emb]), dim=0))\n",
    "        self.trainn = trainn\n",
    "        mask = torch.clip(varis, 0, 1)\n",
    "        cont_emb = self.transformer(comb_emb, mask)\n",
    "        \n",
    "        \n",
    "        \n",
    "        if self.autoregressive:\n",
    "            if self.trainn:\n",
    "                #print(cont_emb.size())\n",
    "                \n",
    "                mask_new = torch.nn.Transformer.generate_square_subsequent_mask(25).cuda()\n",
    "                linear_memory = self.linearize(cont_emb)\n",
    "                start_input = torch.zeros(cont_emb.size(0), 1, V).cuda()\n",
    "                tgt = torch.concat([start_input, tgt], dim=1)\n",
    "                result=self.dec(tgt, linear_memory, mask_new)\n",
    "                return result[:, 1:, :]\n",
    "            if not self.trainn:\n",
    "                output = torch.zeros(cont_emb.size(0), 1, V).cuda()\n",
    "                linear_memory  = self.linearize(cont_emb)\n",
    "                for i in range(24):\n",
    "                    #print(i)\n",
    "                    #print(linear_memory.size(), output.size())\n",
    "                    res = self.dec(output, linear_memory)\n",
    "                    #print(res.size(), output.size())\n",
    "                    output = torch.concat([output, res[:,-1:,:]], dim=1)\n",
    "                return output[:, 1:, :]\n",
    "                \n",
    "        if not self.autoregressive:\n",
    "            fused_embs = []\n",
    "            for attention in self.attentions:\n",
    "                attn_weights = attention(cont_emb, mask)\n",
    "                fused_emb = torch.matmul(attn_weights.transpose(-1, -2), cont_emb).squeeze(-2)\n",
    "                # fused_emb torch.Size([32, 50])\n",
    "                fused_embs.append(fused_emb)\n",
    "            # fused_embs Liste mit 6 torch.Size([32, 50])\n",
    "            fused_emb = torch.stack(fused_embs, axis=1)\n",
    "            # altes fused_emb: torch.Size([32, 50])\n",
    "            # neues fused_emb: torch.Size([32, 6, 50])\n",
    "            # demo_enc torch.Size([32, 50])\n",
    "\n",
    "            # demo_enc.unsqueeze(axis=-2).repeat(1, 6, 1):  torch.Size([32, 6, 50])\n",
    "            #print(fused_emb.size())\n",
    "            concat = torch.cat([fused_emb, demo_enc.unsqueeze(axis=-2).repeat(1, 24, 1)], axis=-1)\n",
    "            #concat torch.Size([32, 6, 100])\n",
    "\n",
    "            x = self.fore_op_calc(concat)\n",
    "            # neues x torch.Size([32, 6, 258])\n",
    "            # altes x: torch.Size([32, 258])\n",
    "            if self.forecast:\n",
    "                return x\n",
    "            else:\n",
    "                x = self.sig(self.op_calc(x)).mean(axis=-2)\n",
    "                return x\n",
    "\n",
    "lr, batch_size, samples_per_epoch, patience = 0.0005, 32, int(102400/sample_divisor), 6\n",
    "d, N, he, dropout = 50, 2, 4, 0.2\n",
    "print('number of parameters: ', V)\n",
    "\n",
    "model = StratsModel(17, fore_max_len, V, d, N, he, dropout, forecast=True).cuda()\n",
    "\n",
    "# raise Exception\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=3)\n",
    "# Pretrain fore_model.\n",
    "best_val_loss = np.inf\n",
    "\n",
    "fore_savepath = '/home/mitarb/fracarolli/eicu/final/240408_models/eicu_mod_triplet_SS_RI_BP.pytorch'\n",
    "loss_func = torch.nn.MSELoss(reduction=\"none\")\n",
    "model.load_state_dict(torch.load(fore_savepath))\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "test_loss_part1 = 0\n",
    "test_loss_part2 = 0\n",
    "sofa_losses = []\n",
    "test_loss_sofa_variables = 0\n",
    "accuracy_list = []\n",
    "TP, TN, FP, FN = 0, 0, 0, 0\n",
    "test_loss_sum_square_total = 0\n",
    "test_loss_part1_sum_square_total = 0\n",
    "test_loss_part2_sum_square_total = 0\n",
    "test_loss_sofa_sum_square_total = 0\n",
    "for start in tqdm(range(0, len(test_ip[0]), 32)):\n",
    "    with torch.no_grad():\n",
    "        output = model(*[torch.tensor(ip[start:start+batch_size], dtype=torch.float32).cuda() \n",
    "                         if i < 3 else torch.tensor(ip[start:start+batch_size], dtype=torch.int32).cuda() \n",
    "                         for i, ip in enumerate(test_ip)], trainn=False)#, tgt=torch.tensor(test_ip[start:start+batch_size], dtype=torch.float32).cuda())\n",
    "    \n",
    "    matrix = torch.tensor(test_input[start:start+batch_size], dtype=torch.float32).cuda()\n",
    "    input_matrix = matrix[:, :24]\n",
    "    output_matrix = matrix[:, 24:, :V]\n",
    "    #torch.Size([32, 24, 129])\n",
    "    output_mark = torch.arange(0, output_matrix.size(1)).unsqueeze(0).repeat(batch_size, 1).cuda()\n",
    "    #torch.Size([32, 24])\n",
    "    output_mask = matrix[:, 24:, V:]\n",
    "    #torch.Size([32, 24, 129])\n",
    "    loss = output_mask[:, -args.pred_len:, :]*(\n",
    "        output-output_matrix[:, -args.pred_len:, :])**2\n",
    "    for outpu, real, mask, input_mat in zip(output, output_matrix, output_mask, input_matrix):\n",
    "        a = (get_sofa(outpu*mask, var_to_ind))\n",
    "        b = (get_sofa(real, var_to_ind))\n",
    "        c = (get_sofa(input_mat, var_to_ind)) #SOFA Source (First Day)\n",
    "        prediction_sofa_difference = (sum(a)-sum(c)) >= 2\n",
    "        gold_sofa_difference = (sum(b)-sum(c)) >= 2\n",
    "        accuracy_list.append(prediction_sofa_difference == gold_sofa_difference)\n",
    "        if gold_sofa_difference:\n",
    "            if prediction_sofa_difference: TP +=1\n",
    "            else: FP += 1\n",
    "        else:\n",
    "            if prediction_sofa_difference: TN +=1\n",
    "            else: FN += 1\n",
    "        c, d = torch.tensor(a, dtype=torch.float32), torch.tensor(b, dtype=torch.float32)\n",
    "        sofa_losses.append(((c-d)**2).sum(axis=-1).mean().item())\n",
    "    loss_old = loss\n",
    "    loss = loss.sum(axis=-1).mean(axis=-1).sum()\n",
    "    loss_part1 = loss_old[:, :8].sum(axis=-1).mean()\n",
    "    loss_part2 = loss_old[:, 8:].sum(axis=-1).mean()\n",
    "    test_loss+=loss\n",
    "    test_loss_part1+=loss_part1\n",
    "    test_loss_part2+=loss_part2\n",
    "    test_loss_sofa_variables+=loss_old[:, :, factor_indices].sum(axis=-1).mean()\n",
    "    test_loss_sum_square_total += torch.sum(loss_old.sum(axis=-1).mean(axis=-1)**2)\n",
    "    test_loss_part1_sum_square_total += torch.sum(loss_old[:, :8].sum(axis=-1).mean(axis=-1)**2)\n",
    "    test_loss_part2_sum_square_total += torch.sum(loss_old[:, 8:].sum(axis=-1).mean(axis=-1)**2)\n",
    "    test_loss_sofa_sum_square_total += torch.sum(loss_old[:, :, factor_indices].sum(axis=-1).mean(axis=-1)**2)\n",
    "test_loss_p = test_loss/len(test_input)\n",
    "se_loss_p = torch.sqrt(test_loss_sum_square_total / len(test_input) - test_loss_p**2)/len(test_input)\n",
    "test_loss_part1_p = test_loss_part1*batch_size/len(test_input)\n",
    "test_loss_part2_p = test_loss_part2*batch_size/len(test_input)\n",
    "test_loss_sofa_variables_p = test_loss_sofa_variables*batch_size/len(test_input)\n",
    "sofa_loss = sum(sofa_losses)/len(sofa_losses)\n",
    "accuracy_sepsis = sum(accuracy_list)/len(accuracy_list)\n",
    "se_accuracy_sepsis = np.sqrt(accuracy_sepsis * (1-accuracy_sepsis)/len(test_input))\n",
    "se_loss_part1_p = torch.sqrt(test_loss_part1_sum_square_total / len(test_input) - test_loss_part1_p**2)/len(test_input)\n",
    "se_loss_part2_p = torch.sqrt(test_loss_part2_sum_square_total / len(test_input) - test_loss_part2_p**2)/len(test_input)\n",
    "se_loss_sofa_p  = torch.sqrt(test_loss_sofa_sum_square_total / len(test_input)  - test_loss_sofa_variables_p**2)/len(test_input)\n",
    "print(test_loss_p, test_loss_part1_p, test_loss_part2_p, sofa_loss, test_loss_sofa_variables_p, accuracy_sepsis)\n",
    "print('Triplet_IMS_SF_B',test_loss_p.item(), test_loss_part1_p.item(), test_loss_part2_p.item(), sofa_loss, test_loss_sofa_variables_p.item(), accuracy_sepsis, sep=',',file=open('results.txt','a'))\n",
    "\n",
    "\n",
    "model_name = '4&Triplet&IMS&SS-RI&Yes&'\n",
    "MSE_string = \"{:.3f}\".format(test_loss_p.item()) + \"$_{[\" + \"{:.3f}\".format(test_loss_p.item()-1.96*se_loss_p.item()) +','+ \"{:.3f}\".format(test_loss_p.item()+1.96*se_loss_p.item()) + ']}$&'\n",
    "SOFA_string = \"{:.3f}\".format(sofa_loss)+ '$_{['+\"{:.3f}\".format(sofa_loss-1.96*se_loss_sofa_p.item())+','+\"{:.3f}\".format(sofa_loss+1.96*se_loss_sofa_p.item())+ ']}$&'\n",
    "MSE8_string = \"{:.3f}\".format(test_loss_part1_p.item()) + \"$_{[\" + \"{:.3f}\".format(test_loss_part1_p.item()-1.96*se_loss_part1_p.item()) +','+ \"{:.3f}\".format(test_loss_part1_p.item()+1.96*se_loss_part1_p.item()) + ']}$&'\n",
    "MSE16_string = \"{:.3f}\".format(test_loss_part2_p.item()) + \"$_{[\" + \"{:.3f}\".format(test_loss_part2_p.item()-1.96*se_loss_part2_p.item()) +','+ \"{:.3f}\".format(test_loss_part2_p.item()+1.96*se_loss_part2_p.item()) + ']}$&'\n",
    "SOFA_acc_string = \"{:.2f}\".format(accuracy_sepsis*100)+ '$_{['+\"{:.2f}\".format(100*(accuracy_sepsis-1.96*se_accuracy_sepsis))+','+\"{:.2f}\".format(100*(accuracy_sepsis+1.96*se_accuracy_sepsis))+ ']}$&'\n",
    "print(model_name,MSE_string, SOFA_string, sep='',file=open('eicu_table_sample_sepsis1.txt','a'))\n",
    "print(model_name,MSE8_string, MSE16_string, sep='',file=open('eicu_table_sample_sepsis2.txt','a'))\n",
    "print(model_name,TP,'&',TN,'&',FP,'&',FN,'&', TP+TN+FP+FN,'&', len(test_input), sep='',file=open('eicu_table_sample_sepsis3.txt','a')) \n",
    "print(model_name,len(test_input),'&',SOFA_acc_string, sep='',file=open('eicu_table_sample_sepsis4.txt','a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54841d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CVE(nn.Module):\n",
    "    def __init__(self, hid_units, output_dim):\n",
    "        super(CVE, self).__init__()\n",
    "        self.hid_units = hid_units\n",
    "        self.output_dim = output_dim\n",
    "        self.W1 = nn.Parameter(torch.Tensor(1, self.hid_units))\n",
    "        self.b1 = nn.Parameter(torch.Tensor(self.hid_units))\n",
    "        self.W2 = nn.Parameter(torch.Tensor(self.hid_units, self.output_dim))\n",
    "        nn.init.xavier_uniform_(self.W1)\n",
    "        nn.init.zeros_(self.b1)\n",
    "        nn.init.xavier_uniform_(self.W2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(-1)\n",
    "        x = torch.matmul(torch.tanh(torch.add(torch.matmul(x, self.W1), self.b1)), self.W2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hid_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.hid_dim = hid_dim\n",
    "        self.W = nn.Parameter(torch.Tensor(50, self.hid_dim))\n",
    "        self.b = nn.Parameter(torch.Tensor(self.hid_dim))\n",
    "        self.u = nn.Parameter(torch.Tensor(self.hid_dim, 1))\n",
    "        nn.init.xavier_uniform_(self.W)\n",
    "        nn.init.zeros_(self.b)\n",
    "        nn.init.xavier_uniform_(self.u)\n",
    "\n",
    "    def forward(self, x, mask, mask_value=-1e30):\n",
    "        attn_weights = torch.matmul(torch.tanh(torch.add(torch.matmul(x, self.W), self.b)), self.u)\n",
    "        mask = mask.unsqueeze(-1)\n",
    "        attn_weights = mask * attn_weights + (1 - mask) * mask_value\n",
    "        attn_weights = F.softmax(attn_weights, dim=-2)\n",
    "        return attn_weights\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, N=2, h=8, dk=None, dv=None, dff=None, dropout=0, d=8):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.N, self.h, self.dk, self.dv, self.dff, self.dropout = N, h, dk, dv, dff, dropout\n",
    "        self.epsilon = torch.finfo(torch.float32).eps * torch.finfo(torch.float32).eps\n",
    "        self.Wq = nn.Parameter(torch.Tensor(N, h, d, dk))\n",
    "        self.Wk = nn.Parameter(torch.Tensor(N, h, d, dk))\n",
    "        self.Wv = nn.Parameter(torch.Tensor(N, h, d, dv))\n",
    "        self.Wo = nn.Parameter(torch.Tensor(N, dv * h, d))\n",
    "        self.W1 = nn.Parameter(torch.Tensor(N, d, dff))\n",
    "        self.b1 = nn.Parameter(torch.Tensor(N, dff))\n",
    "        self.W2 = nn.Parameter(torch.Tensor(N, dff, d))\n",
    "        self.b2 = nn.Parameter(torch.Tensor(N, d))\n",
    "        self.gamma = nn.Parameter(torch.Tensor(2 * N))\n",
    "        self.beta = nn.Parameter(torch.Tensor(2 * N))\n",
    "        nn.init.xavier_uniform_(self.Wq)\n",
    "        nn.init.xavier_uniform_(self.Wk)\n",
    "        nn.init.xavier_uniform_(self.Wv)\n",
    "        nn.init.xavier_uniform_(self.Wo)\n",
    "        nn.init.xavier_uniform_(self.W1)\n",
    "        nn.init.zeros_(self.b1)\n",
    "        nn.init.xavier_uniform_(self.W2)\n",
    "        nn.init.zeros_(self.b2)\n",
    "        nn.init.ones_(self.gamma)\n",
    "        nn.init.zeros_(self.beta)\n",
    "        self.training = False\n",
    "        \n",
    "    def forward(self, x, mask, mask_value=-1e-30):\n",
    "        mask = mask.unsqueeze(-2)\n",
    "        for i in range(self.N):\n",
    "            # MHA\n",
    "            mha_ops = []\n",
    "            for j in range(self.h):\n",
    "                q = torch.matmul(x, self.Wq[i, j, :, :])          \n",
    "                k = torch.matmul(x, self.Wk[i, j, :, :]).permute(0, 2, 1)\n",
    "                v = torch.matmul(x, self.Wv[i, j, :, :])\n",
    "                A = torch.matmul(q, k)\n",
    "                # Mask unobserved steps.\n",
    "                A = mask * A + (1 - mask) * mask_value\n",
    "                # Mask for attention dropout.\n",
    "                if self.training:\n",
    "                    dp_mask = (torch.rand(A.shape, device=A.device) >= self.dropout).float()\n",
    "                    A = A * dp_mask + (1 - dp_mask) * mask_value\n",
    "                A = F.softmax(A, dim=-1)\n",
    "                mha_ops.append(torch.matmul(A, v))\n",
    "            conc = torch.cat(mha_ops, dim=-1)\n",
    "            proj = torch.matmul(conc, self.Wo[i, :, :])\n",
    "            # Dropout.\n",
    "            if self.training:\n",
    "                proj = F.dropout(proj, p=self.dropout)\n",
    "            # Add & LN\n",
    "            x = x + proj\n",
    "            mean = x.mean(dim=-1, keepdim=True)\n",
    "            variance = torch.mean(torch.square(x - mean), dim=-1, keepdim=True)\n",
    "            std = torch.sqrt(variance + self.epsilon)\n",
    "            x = (x - mean) / std\n",
    "            x = x * self.gamma[2 * i] + self.beta[2 * i]\n",
    "            # FFN\n",
    "            ffn_op = torch.add(torch.matmul(F.relu(torch.add(\n",
    "                torch.matmul(x, self.W1[i, :, :]), self.b1[i, :])),\n",
    "                                            self.W2[i, :, :]), self.b2[i, :])\n",
    "            # Dropout.\n",
    "            if self.training:\n",
    "                ffn_op = F.dropout(ffn_op, p=self.dropout)\n",
    "            # Add & LN\n",
    "            x = x + ffn_op\n",
    "            mean = x.mean(dim=-1, keepdim=True)\n",
    "            variance = torch.mean(torch.square(x - mean), dim=-1, keepdim=True)\n",
    "            std = torch.sqrt(variance + self.epsilon)\n",
    "            x = (x - mean) / std\n",
    "            x = x * self.gamma[2 * i + 1] + self.beta[2 * i + 1]\n",
    "        return x\n",
    "\n",
    "\n",
    "class StratsModel(nn.Module):\n",
    "    def __init__(self, D, max_len, V, d, N, he, dropout, forecast=False, autoregressive=True):\n",
    "        super(StratsModel, self).__init__()\n",
    "        self.forecast = forecast\n",
    "        self.autoregressive=autoregressive\n",
    "        self.demo_enc = nn.Sequential(nn.Linear(D, D * d),\n",
    "                                      nn.Tanh(),\n",
    "                                      nn.Linear(D * d, d),\n",
    "                                      nn.Tanh())\n",
    "        self.varis_emb = nn.Embedding(V + 1, d)\n",
    "        nn.init.uniform_(self.varis_emb.weight, a=-0.05, b=0.05)\n",
    "        self.values_emb = CVE(int(np.sqrt(d)), d)\n",
    "        self.times_emb = CVE(int(np.sqrt(d)), d)\n",
    "        # self.comb_emb = nn.LayerNorm(d)\n",
    "        self.transformer = Transformer(N, he, dk=d//he, dv=d//he, dff=2*d, dropout=dropout, d=d)\n",
    "\n",
    "        if autoregressive:\n",
    "            self.linearize = nn.Linear(50, V)\n",
    "            self.dec = nn.TransformerDecoderLayer(d_model=V, dim_feedforward=V, nhead=1, batch_first=True)\n",
    "        else:\n",
    "            self.attention = Attention(2 * d)\n",
    "            self.attentions = nn.ModuleList([Attention(2*d) for i in range(24)])\n",
    "            # self.concat = nn.Linear(2 * d, 1)\n",
    "            self.fore_op_calc = nn.Linear(2 * d, V)\n",
    "            self.op_calc = nn.Linear(V, 1)\n",
    "            self.sig = nn.Sigmoid()        \n",
    "        \n",
    "\n",
    "    def forward(self, demo, times, values, varis, trainn=False, tgt=torch.zeros(32, 1, V).cuda()):\n",
    "        demo_enc = self.demo_enc(demo)\n",
    "        varis_emb = self.varis_emb(varis)\n",
    "        values_emb = self.values_emb(values)\n",
    "        times_emb = self.times_emb(times)\n",
    "\n",
    "        comb_emb = (torch.sum(torch.stack([varis_emb, values_emb, times_emb]), dim=0))\n",
    "        self.trainn = trainn\n",
    "        mask = torch.clip(varis, 0, 1)\n",
    "        cont_emb = self.transformer(comb_emb, mask)\n",
    "        \n",
    "        \n",
    "        \n",
    "        if self.autoregressive:\n",
    "            if self.trainn:\n",
    "                #print(cont_emb.size())\n",
    "                \n",
    "                mask_new = torch.nn.Transformer.generate_square_subsequent_mask(25).cuda()\n",
    "                linear_memory = self.linearize(cont_emb)\n",
    "                start_input = torch.zeros(cont_emb.size(0), 1, V).cuda()\n",
    "                tgt = torch.concat([start_input, tgt], dim=1)\n",
    "                result=self.dec(tgt, linear_memory, mask_new)\n",
    "                return result[:, 1:, :]\n",
    "            if not self.trainn:\n",
    "                output = torch.zeros(cont_emb.size(0), 1, V).cuda()\n",
    "                linear_memory  = self.linearize(cont_emb)\n",
    "                for i in range(24):\n",
    "                    #print(i)\n",
    "                    #print(linear_memory.size(), output.size())\n",
    "                    res = self.dec(output, linear_memory)\n",
    "                    #print(res.size(), output.size())\n",
    "                    output = torch.concat([output, res[:,-1:,:]], dim=1)\n",
    "                return output[:, 1:, :]\n",
    "                \n",
    "        if not self.autoregressive:\n",
    "            fused_embs = []\n",
    "            for attention in self.attentions:\n",
    "                attn_weights = attention(cont_emb, mask)\n",
    "                fused_emb = torch.matmul(attn_weights.transpose(-1, -2), cont_emb).squeeze(-2)\n",
    "                # fused_emb torch.Size([32, 50])\n",
    "                fused_embs.append(fused_emb)\n",
    "            # fused_embs Liste mit 6 torch.Size([32, 50])\n",
    "            fused_emb = torch.stack(fused_embs, axis=1)\n",
    "            # altes fused_emb: torch.Size([32, 50])\n",
    "            # neues fused_emb: torch.Size([32, 6, 50])\n",
    "            # demo_enc torch.Size([32, 50])\n",
    "\n",
    "            # demo_enc.unsqueeze(axis=-2).repeat(1, 6, 1):  torch.Size([32, 6, 50])\n",
    "            #print(fused_emb.size())\n",
    "            concat = torch.cat([fused_emb, demo_enc.unsqueeze(axis=-2).repeat(1, 24, 1)], axis=-1)\n",
    "            #concat torch.Size([32, 6, 100])\n",
    "\n",
    "            x = self.fore_op_calc(concat)\n",
    "            # neues x torch.Size([32, 6, 258])\n",
    "            # altes x: torch.Size([32, 258])\n",
    "            if self.forecast:\n",
    "                return x\n",
    "            else:\n",
    "                x = self.sig(self.op_calc(x)).mean(axis=-2)\n",
    "                return x\n",
    "\n",
    "lr, batch_size, samples_per_epoch, patience = 0.0005, 32, int(102400/sample_divisor), 6\n",
    "d, N, he, dropout = 50, 2, 4, 0.2\n",
    "print('number of parameters: ', V)\n",
    "\n",
    "model = StratsModel(17, fore_max_len, V, d, N, he, dropout, forecast=True).cuda()\n",
    "\n",
    "# raise Exception\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=3)\n",
    "# Pretrain fore_model.\n",
    "best_val_loss = np.inf\n",
    "\n",
    "fore_savepath = '/home/mitarb/fracarolli/eicu/final/240408_models/eicu_mod_triplet_SS_RI_D.pytorch'\n",
    "loss_func = torch.nn.MSELoss(reduction=\"none\")\n",
    "model.load_state_dict(torch.load(fore_savepath))\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "test_loss_part1 = 0\n",
    "test_loss_part2 = 0\n",
    "sofa_losses = []\n",
    "test_loss_sofa_variables = 0\n",
    "accuracy_list = []\n",
    "TP, TN, FP, FN = 0, 0, 0, 0\n",
    "test_loss_sum_square_total = 0\n",
    "test_loss_part1_sum_square_total = 0\n",
    "test_loss_part2_sum_square_total = 0\n",
    "test_loss_sofa_sum_square_total = 0\n",
    "for start in tqdm(range(0, len(test_ip[0]), 32)):\n",
    "    with torch.no_grad():\n",
    "        output = model(*[torch.tensor(ip[start:start+batch_size], dtype=torch.float32).cuda() \n",
    "                         if i < 3 else torch.tensor(ip[start:start+batch_size], dtype=torch.int32).cuda() \n",
    "                         for i, ip in enumerate(test_ip)], trainn=False)#, tgt=torch.tensor(test_ip[start:start+batch_size], dtype=torch.float32).cuda())\n",
    "    \n",
    "    matrix = torch.tensor(test_input[start:start+batch_size], dtype=torch.float32).cuda()\n",
    "    input_matrix = matrix[:, :24]\n",
    "    output_matrix = matrix[:, 24:, :V]\n",
    "    #torch.Size([32, 24, 129])\n",
    "    output_mark = torch.arange(0, output_matrix.size(1)).unsqueeze(0).repeat(batch_size, 1).cuda()\n",
    "    #torch.Size([32, 24])\n",
    "    output_mask = matrix[:, 24:, V:]\n",
    "    #torch.Size([32, 24, 129])\n",
    "    loss = output_mask[:, -args.pred_len:, :]*(\n",
    "        output-output_matrix[:, -args.pred_len:, :])**2\n",
    "    for outpu, real, mask, input_mat in zip(output, output_matrix, output_mask, input_matrix):\n",
    "        a = (get_sofa(outpu*mask, var_to_ind))\n",
    "        b = (get_sofa(real, var_to_ind))\n",
    "        c = (get_sofa(input_mat, var_to_ind)) #SOFA Source (First Day)\n",
    "        prediction_sofa_difference = (sum(a)-sum(c)) >= 2\n",
    "        gold_sofa_difference = (sum(b)-sum(c)) >= 2\n",
    "        accuracy_list.append(prediction_sofa_difference == gold_sofa_difference)\n",
    "        if gold_sofa_difference:\n",
    "            if prediction_sofa_difference: TP +=1\n",
    "            else: FP += 1\n",
    "        else:\n",
    "            if prediction_sofa_difference: TN +=1\n",
    "            else: FN += 1\n",
    "        c, d = torch.tensor(a, dtype=torch.float32), torch.tensor(b, dtype=torch.float32)\n",
    "        sofa_losses.append(((c-d)**2).sum(axis=-1).mean().item())\n",
    "    loss_old = loss\n",
    "    loss = loss.sum(axis=-1).mean(axis=-1).sum()\n",
    "    loss_part1 = loss_old[:, :8].sum(axis=-1).mean()\n",
    "    loss_part2 = loss_old[:, 8:].sum(axis=-1).mean()\n",
    "    test_loss+=loss\n",
    "    test_loss_part1+=loss_part1\n",
    "    test_loss_part2+=loss_part2\n",
    "    test_loss_sofa_variables+=loss_old[:, :, factor_indices].sum(axis=-1).mean()\n",
    "    test_loss_sum_square_total += torch.sum(loss_old.sum(axis=-1).mean(axis=-1)**2)\n",
    "    test_loss_part1_sum_square_total += torch.sum(loss_old[:, :8].sum(axis=-1).mean(axis=-1)**2)\n",
    "    test_loss_part2_sum_square_total += torch.sum(loss_old[:, 8:].sum(axis=-1).mean(axis=-1)**2)\n",
    "    test_loss_sofa_sum_square_total += torch.sum(loss_old[:, :, factor_indices].sum(axis=-1).mean(axis=-1)**2)\n",
    "test_loss_p = test_loss/len(test_input)\n",
    "se_loss_p = torch.sqrt(test_loss_sum_square_total / len(test_input) - test_loss_p**2)/len(test_input)\n",
    "test_loss_part1_p = test_loss_part1*batch_size/len(test_input)\n",
    "test_loss_part2_p = test_loss_part2*batch_size/len(test_input)\n",
    "test_loss_sofa_variables_p = test_loss_sofa_variables*batch_size/len(test_input)\n",
    "sofa_loss = sum(sofa_losses)/len(sofa_losses)\n",
    "accuracy_sepsis = sum(accuracy_list)/len(accuracy_list)\n",
    "se_accuracy_sepsis = np.sqrt(accuracy_sepsis * (1-accuracy_sepsis)/len(test_input))\n",
    "se_loss_part1_p = torch.sqrt(test_loss_part1_sum_square_total / len(test_input) - test_loss_part1_p**2)/len(test_input)\n",
    "se_loss_part2_p = torch.sqrt(test_loss_part2_sum_square_total / len(test_input) - test_loss_part2_p**2)/len(test_input)\n",
    "se_loss_sofa_p  = torch.sqrt(test_loss_sofa_sum_square_total / len(test_input)  - test_loss_sofa_variables_p**2)/len(test_input)\n",
    "print(test_loss_p, test_loss_part1_p, test_loss_part2_p, sofa_loss, test_loss_sofa_variables_p, accuracy_sepsis)\n",
    "print('Triplet_IMS_SF_B',test_loss_p.item(), test_loss_part1_p.item(), test_loss_part2_p.item(), sofa_loss, test_loss_sofa_variables_p.item(), accuracy_sepsis, sep=',',file=open('results.txt','a'))\n",
    "\n",
    "\n",
    "model_name = '4&Triplet&IMS&SS-RI&No&'\n",
    "MSE_string = \"{:.3f}\".format(test_loss_p.item()) + \"$_{[\" + \"{:.3f}\".format(test_loss_p.item()-1.96*se_loss_p.item()) +','+ \"{:.3f}\".format(test_loss_p.item()+1.96*se_loss_p.item()) + ']}$&'\n",
    "SOFA_string = \"{:.3f}\".format(sofa_loss)+ '$_{['+\"{:.3f}\".format(sofa_loss-1.96*se_loss_sofa_p.item())+','+\"{:.3f}\".format(sofa_loss+1.96*se_loss_sofa_p.item())+ ']}$&'\n",
    "MSE8_string = \"{:.3f}\".format(test_loss_part1_p.item()) + \"$_{[\" + \"{:.3f}\".format(test_loss_part1_p.item()-1.96*se_loss_part1_p.item()) +','+ \"{:.3f}\".format(test_loss_part1_p.item()+1.96*se_loss_part1_p.item()) + ']}$&'\n",
    "MSE16_string = \"{:.3f}\".format(test_loss_part2_p.item()) + \"$_{[\" + \"{:.3f}\".format(test_loss_part2_p.item()-1.96*se_loss_part2_p.item()) +','+ \"{:.3f}\".format(test_loss_part2_p.item()+1.96*se_loss_part2_p.item()) + ']}$&'\n",
    "SOFA_acc_string = \"{:.2f}\".format(accuracy_sepsis*100)+ '$_{['+\"{:.2f}\".format(100*(accuracy_sepsis-1.96*se_accuracy_sepsis))+','+\"{:.2f}\".format(100*(accuracy_sepsis+1.96*se_accuracy_sepsis))+ ']}$&'\n",
    "print(model_name,MSE_string, SOFA_string, sep='',file=open('eicu_table_sample_sepsis1.txt','a'))\n",
    "print(model_name,MSE8_string, MSE16_string, sep='',file=open('eicu_table_sample_sepsis2.txt','a'))\n",
    "print(model_name,TP,'&',TN,'&',FP,'&',FN,'&', TP+TN+FP+FN,'&', len(test_input), sep='',file=open('eicu_table_sample_sepsis3.txt','a')) \n",
    "print(model_name,len(test_input),'&',SOFA_acc_string, sep='',file=open('eicu_table_sample_sepsis4.txt','a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4f0ea1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae67462d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
