{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99d86537",
   "metadata": {},
   "source": [
    "*ToDo*\n",
    "\n",
    "02) pred_window und obs_window ausprobieren.\n",
    "03) output nochmal genau anschauen.\n",
    "04) was ist maskiert anschauen - und wie genau?\n",
    "05) VRAM wenig ausgelastet. Batch size mal mit 320 ausprobieren, aber auch durchziehen.\n",
    "06) Task pred: scheduler einfügen.\n",
    "07) Analyse each var (sparsity)\n",
    "08) loss per var/quality of varis loss (is this already ablation to forecast only one var)\n",
    "09) Include sepsis definition\n",
    "10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "driven-termination",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, auc\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchinfo import summary\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c94544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data settings\n",
    "test_cond = 0\n",
    "\n",
    "if test_cond == 1:\n",
    "    data_path = '/home/mitarb/fracarolli/files/231113_STraTS_new/mimic_iii_preprocessed_new_z_err20092.pkl'\n",
    "    sample_divisor = 100\n",
    "    number_of_epochs = 20\n",
    "else:\n",
    "    data_path = '/home/mitarb/fracarolli/files/231113_STraTS_new/mimic_iii_preprocessed_new_z_err20092.pkl'\n",
    "    sample_divisor = 1\n",
    "    number_of_epochs = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recreational-record",
   "metadata": {},
   "source": [
    "## Load forecast dataset into matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fdc7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inv_list(l, start=0):  # Create vind\n",
    "    d = {}\n",
    "    for i in range(len(l)):\n",
    "        d[l[i]] = i+start\n",
    "    return d\n",
    "\n",
    "\n",
    "def f(x):\n",
    "    mask   = [0 for i in range(V)]\n",
    "    values = [0 for i in range(V)]\n",
    "    #print('hi', mask , V)\n",
    "    #mask, values = np.zeros(V), np.zeros(V)\n",
    "    #print('hi', mask)\n",
    "    for vv in x:  # tuple of ['vind','value']\n",
    "        v = int(vv[0])-1  # shift index of vind\n",
    "        mask[v] = 1\n",
    "        values[v] = vv[1]  # get value\n",
    "    return values+mask  # concat\n",
    "\n",
    "\n",
    "def pad(x):\n",
    "    if len(x) > 880:\n",
    "        print(len(x))\n",
    "    return x+[0]*(fore_max_len-len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "executive-annual",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_window_old = 12  # hours\n",
    "obs_windows = range(0, 1, 1)\n",
    "\n",
    "# Read data.\n",
    "data, oc, train_ind, valid_ind, test_ind = pickle.load(open(data_path, 'rb'))\n",
    "# Remove test patients.\n",
    "data = data.merge(oc[['ts_ind', 'SUBJECT_ID']], on='ts_ind', how='left')\n",
    "test_sub = oc.loc[oc.ts_ind.isin(test_ind)].SUBJECT_ID.unique()\n",
    "data = data.loc[~data.SUBJECT_ID.isin(test_sub)]\n",
    "oc = oc.loc[~oc.SUBJECT_ID.isin(test_sub)]\n",
    "data.drop(columns=['SUBJECT_ID', 'TABLE'], inplace=True)\n",
    "# Fix age.\n",
    "data.loc[(data.variable == 'Age') & (data.value > 200), 'value'] = 91.4\n",
    "# Get static data with mean fill and missingness indicator.\n",
    "static_varis = ['Age', 'Gender']\n",
    "ii = data.variable.isin(static_varis)\n",
    "static_data = data.loc[ii]  # static data are 'Age' and 'Gender'\n",
    "data = data.loc[~ii]  # ~ binary flip\n",
    "# print('data\\n',data)\n",
    "\n",
    "static_var_to_ind = inv_list(static_varis)  # {'Age': 0, 'Gender': 1}\n",
    "D = len(static_varis)  # 2 by definition\n",
    "N = data.ts_ind.max()+1  # /= 12: 52861\n",
    "demo = np.zeros((int(N), int(D)))\n",
    "for row in tqdm(static_data.itertuples()):\n",
    "    demo[int(row.ts_ind), static_var_to_ind[row.variable]] = row.value\n",
    "# print('Demo after tqdm command \\n',demo[:10])\n",
    "# Normalize static data.\n",
    "means = demo.mean(axis=0, keepdims=True)  # quite sparse\n",
    "stds = demo.std(axis=0, keepdims=True)\n",
    "stds = (stds == 0)*1 + (stds != 0)*stds\n",
    "demo = (demo-means)/stds\n",
    "# print('Demo after normalisation \\n',demo[:10])\n",
    "# Get variable indices.\n",
    "varis = sorted(list(set(data.variable)))\n",
    "V = len(varis)  # 129 for \\=12 with varis all variables except for static ones\n",
    "# print('varis', varis, V)\n",
    "var_to_ind = inv_list(varis, start=1)\n",
    "data['vind'] = data.variable.map(var_to_ind)\n",
    "data = data[['ts_ind', 'vind', 'hour', 'value']].sort_values(by=['ts_ind', 'vind', 'hour'])\n",
    "# print('data vind\\n', data['vind'], '\\n data\\n',data)\n",
    "# Find max_len.\n",
    "fore_max_len = 880  # hard coded max_len of vars\n",
    "# Get forecast inputs and outputs.\n",
    "fore_times_ip = []\n",
    "fore_values_ip = []\n",
    "fore_varis_ip = []\n",
    "fore_op = []\n",
    "fore_op_awesome = []\n",
    "fore_inds = []\n",
    "for w in tqdm(range(25, 124, 4)):  # range(20, 124, 4), pred_window=2\n",
    "    pred_data = data.loc[(data.hour>=w)&(data.hour<=w+24)]\n",
    "    pred_data = pred_data.groupby(['ts_ind', 'vind']).agg({'value':'first'}).reset_index()\n",
    "    pred_data['vind_value'] = pred_data[['vind', 'value']].values.tolist()\n",
    "    pred_data = pred_data.groupby('ts_ind').agg({'vind_value':list}).reset_index()\n",
    "    pred_data['vind_value'] = pred_data['vind_value'].apply(f)   \n",
    "\n",
    "    obs_data = data.loc[(data.hour < w) & (data.hour >= w-24)]\n",
    "    obs_data = obs_data.loc[obs_data.ts_ind.isin(pred_data.ts_ind)]\n",
    "    obs_data = obs_data.groupby('ts_ind').head(fore_max_len)\n",
    "    obs_data = obs_data.groupby('ts_ind').agg({'vind': list, 'hour': list, 'value': list}).reset_index()\n",
    "    #Michi: Hier hole ich mir die 24 Stunden vor, und 24 Stunden nach einem bestimmten Zeitpunkt raus\n",
    "    for pred_window  in range(-24, 24, 1):\n",
    "        pred_data = data.loc[(data.hour >= w+pred_window) & (data.hour <= w+1 +pred_window)]\n",
    "        pred_data = pred_data.groupby(['ts_ind', 'vind']).agg({'value': 'first'}).reset_index()\n",
    "        pred_data['vind_value'+str(pred_window)] = pred_data[['vind', 'value']].values.tolist()\n",
    "        pred_data = pred_data.groupby('ts_ind').agg({'vind_value'+str(pred_window): list}).reset_index()\n",
    "        pred_data['vind_value'+str(pred_window)] = pred_data['vind_value'+str(pred_window)].apply(f)  # 721 entries with 2*129 vind_values\n",
    "        obs_data = obs_data.merge(pred_data, on='ts_ind')\n",
    "\n",
    "    for col in ['vind', 'hour', 'value']:\n",
    "        obs_data[col] = obs_data[col].apply(pad)\n",
    "    fore_op_awesome.append(np.array(list([list(obs_data['vind_value'+str(pred_window)]) for pred_window in range(-24, 24, 1)])))\n",
    "    #fore_op.append(np.array(list(obs_data.vind_value)))\n",
    "    fore_inds.append(np.array([int(x) for x in list(obs_data.ts_ind)]))\n",
    "    fore_times_ip.append(np.array(list(obs_data.hour)))\n",
    "    fore_values_ip.append(np.array(list(obs_data.value)))\n",
    "    fore_varis_ip.append(np.array(list(obs_data.vind)))\n",
    "    \n",
    "del data\n",
    "fore_times_ip = np.concatenate(fore_times_ip, axis=0)\n",
    "fore_values_ip = np.concatenate(fore_values_ip, axis=0)\n",
    "fore_varis_ip = np.concatenate(fore_varis_ip, axis=0)\n",
    "\n",
    "fore_op_awesome = np.concatenate(fore_op_awesome, axis=1)\n",
    "fore_op = np.swapaxes(fore_op_awesome, 0, 1)\n",
    "print(fore_op.shape)\n",
    "\n",
    "#raise Exception\n",
    "fore_inds = np.concatenate(fore_inds, axis=0)\n",
    "fore_demo = demo[fore_inds]\n",
    "# Get train and valid ts_ind for forecast task.\n",
    "train_sub = oc.loc[oc.ts_ind.isin(train_ind)].SUBJECT_ID.unique()\n",
    "valid_sub = oc.loc[oc.ts_ind.isin(valid_ind)].SUBJECT_ID.unique()\n",
    "rem_sub = oc.loc[~oc.SUBJECT_ID.isin(np.concatenate((train_ind, valid_ind)))].SUBJECT_ID.unique()\n",
    "bp = int(0.8*len(rem_sub))\n",
    "train_sub = np.concatenate((train_sub, rem_sub[:bp]))\n",
    "valid_sub = np.concatenate((valid_sub, rem_sub[bp:]))\n",
    "train_ind = oc.loc[oc.SUBJECT_ID.isin(train_sub)].ts_ind.unique()  # Add remaining ts_ind s of train subjects.\n",
    "valid_ind = oc.loc[oc.SUBJECT_ID.isin(valid_sub)].ts_ind.unique()  # Add remaining ts_ind s of train subjects.\n",
    "# Generate 3 sets of inputs and outputs.\n",
    "train_ind = np.argwhere(np.in1d(fore_inds, train_ind)).flatten()\n",
    "valid_ind = np.argwhere(np.in1d(fore_inds, valid_ind)).flatten()\n",
    "fore_train_ip = [ip[train_ind] for ip in [fore_demo, fore_times_ip, fore_values_ip, fore_varis_ip]]\n",
    "fore_valid_ip = [ip[valid_ind] for ip in [fore_demo, fore_times_ip, fore_values_ip, fore_varis_ip]]\n",
    "del fore_times_ip, fore_values_ip, fore_varis_ip, demo, fore_demo\n",
    "fore_train_op = fore_op[train_ind]\n",
    "fore_valid_op = fore_op[valid_ind]\n",
    "del fore_op\n",
    "\n",
    "print('lengths of rem_sub, fore_train_ip[1], fore_valid_ip[0]')\n",
    "print(len(rem_sub), fore_train_ip[1].shape, fore_valid_ip[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8fd59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sofa(matrix, var_to_ind): #24x131 matrix\n",
    "    # GCS: min_eye, min_motor, min_verbal = 5, 5, 5\n",
    "    #print(matrix.size())\n",
    "    #raise Exception\n",
    "    key =\"GCS_eye\"\n",
    "    var_to_ind = {x:i-1 for x,i in var_to_ind.items()}\n",
    "    a=matrix[:, var_to_ind[key]][torch.nonzero(matrix[:, var_to_ind[key]], as_tuple=True)]\n",
    "    #print(a)\n",
    "    min_eye = min(matrix[:, var_to_ind[key]][torch.nonzero(matrix[:, var_to_ind[key]], as_tuple=True)]*mean_std_dict[key][1]+mean_std_dict[key][0], default=4)\n",
    "    key = \"GCS_motor\"\n",
    "    min_motor = min(matrix[:, var_to_ind[key]][torch.nonzero(matrix[:, var_to_ind[key]], as_tuple=True)]*mean_std_dict[key][1]+mean_std_dict[key][0], default=6)\n",
    "    key = \"GCS_verbal\"\n",
    "    min_verbal = min(matrix[:, var_to_ind[key]][torch.nonzero(matrix[:, var_to_ind[key]], as_tuple=True)]*mean_std_dict[key][1]+mean_std_dict[key][0], default=5)\n",
    "    \n",
    "\n",
    "    GCS = min_eye + min_motor + min_verbal\n",
    "    if GCS > 14: GCS_sofa = 0\n",
    "    elif GCS > 12: GCS_sofa = 1\n",
    "    elif GCS > 9:  GCS_sofa = 2\n",
    "    elif GCS > 5:  GCS_sofa = 3\n",
    "    else: GCS_sofa = 4\n",
    "    #print('GCS_sofa is', GCS_sofa, ';     GCS is', GCS,'; GCS eye', min_eye, '; GCS motor', min_motor, '; GCS verbal', min_verbal)\n",
    "\n",
    "    key = \"Bilirubin (Total)\"\n",
    "    bilir = max(matrix[:, var_to_ind[key]][torch.nonzero(matrix[:, var_to_ind[key]], as_tuple=True)]*mean_std_dict[key][1]+mean_std_dict[key][0], default=1)\n",
    "    if bilir > 12: bilir_sofa = 4\n",
    "    elif bilir > 6: bilir_sofa = 3\n",
    "    elif bilir > 2: bilir_sofa = 2\n",
    "    elif bilir > 1.2: bilir_sofa = 1\n",
    "    else: bilir_sofa = 0\n",
    "    #print('bilir_Sofa is', bilir_sofa, ';   bilirubin is', bilir)\n",
    "    \n",
    "    # Coagulation (Platelets)\n",
    "    key = \"Platelet Count\"\n",
    "    plate = max(matrix[:, var_to_ind[key]][torch.nonzero(matrix[:, var_to_ind[key]], as_tuple=True)]*mean_std_dict[key][1]+mean_std_dict[key][0], default=160)\n",
    "    if plate > 150: plate_sofa = 0\n",
    "    elif plate > 100: plate_sofa = 1\n",
    "    elif plate > 50: plate_sofa = 2\n",
    "    elif plate > 20: plate_sofa = 3\n",
    "    else: plate_sofa = 4\n",
    "    #print('plate_sofa is', plate_sofa, ';   platelet count is', plate)\n",
    "    \n",
    "    # print('Urinmenge 24h', sum(data_var[data_var['variable']=='Urine']['value2']))\n",
    "\n",
    "    key = \"Urine\"\n",
    "    urine = sum(matrix[:, var_to_ind[key]][torch.nonzero(matrix[:, var_to_ind[key]], as_tuple=True)]*mean_std_dict[key][1]+mean_std_dict[key][0])\n",
    "    key = \"Creatinine Blood\"\n",
    "    creat = max(matrix[:, var_to_ind[key]][torch.nonzero(matrix[:, var_to_ind[key]], as_tuple=True)]*mean_std_dict[key][1]+mean_std_dict[key][0], default=1)\n",
    "    \n",
    "    if (urine < 200) or (creat > 5): renal_sofa = 4\n",
    "    elif  (urine < 500) or (creat > 3.5): renal_sofa = 3\n",
    "    elif creat > 2.0: renal_sofa = 2\n",
    "    elif creat > 1.2: renal_sofa = 1\n",
    "    else: renal_sofa = 0\n",
    "    #print('renal_sofa:',renal_sofa,';       urine 24:',urine,'; creat:', creat)\n",
    "    \n",
    "    CS_data = get_CS(matrix, var_to_ind)\n",
    "    cs_sofa = CS_SOFA(CS_data)\n",
    "    \n",
    "    #cs_sofa = 0\n",
    "    key=\"FiO2\"\n",
    "    fio2 = (matrix[:, var_to_ind[key]]*mean_std_dict[key][1]+mean_std_dict[key][0])\n",
    "    key=\"PO2\"\n",
    "    po2 = (matrix[:, var_to_ind[key]]*mean_std_dict[key][1]+mean_std_dict[key][0])\n",
    "    PaO2FiO2 = 100*po2/fio2\n",
    "    #print(\"size\", PaO2FiO2.size())\n",
    "    PaO2FiO2 = PaO2FiO2[torch.nonzero(PaO2FiO2, as_tuple=True)]\n",
    "    pao2fio2 = min(PaO2FiO2)\n",
    "    if pao2fio2<100: resp=4\n",
    "    elif pao2fio2<200: resp=3\n",
    "    elif pao2fio2<300:resp=2\n",
    "    elif pao2fio2<400:resp=1\n",
    "    else: resp=0\n",
    "    return GCS_sofa, cs_sofa, resp, plate_sofa, bilir_sofa, renal_sofa\n",
    "\n",
    "def get_CS(matrix, var_to_ind):\n",
    "    #data_var = data_pat[data_pat['variable'].isin(['Dobutamine','Dopamine','Epinephrine','Norepinephrine','Weight'])]\n",
    "    #data_var['value2'] = data_var['value']*data_var['std']+data_var['mean']\n",
    "    \n",
    "    #weight = min(data_var[data_var['variable']=='Weight']['value2'], default=80)  # set default weight to 80kg.\n",
    "    key = \"Weight\"\n",
    "    weight = min(matrix[:, var_to_ind[key]][torch.nonzero(matrix[:, var_to_ind[key]], as_tuple=True)], default=80)#*mean_std_dict[key][1]+mean_std_dict[key][0], default=1)\n",
    "    key = \"Dopamine\"\n",
    "    try:\n",
    "        data_dop = max(matrix[:, var_to_ind[key]][torch.nonzero(matrix[:, var_to_ind[key]], as_tuple=True)]*mean_std_dict[key][1]+mean_std_dict[key][0], default=1)\n",
    "        data_dop = data_dop /60/weight*1000\n",
    "    except:\n",
    "        data_dop = 0\n",
    "\n",
    "    key = \"Dobutamine\"\n",
    "    \n",
    "    try:\n",
    "        data_dobu = max(matrix[:, var_to_ind[key]][torch.nonzero(matrix[:, var_to_ind[key]], as_tuple=True)]*mean_std_dict[key][1]+mean_std_dict[key][0], default=1)\n",
    "        data_dobu = data_dobu  /60/weight*1000\n",
    "    except:\n",
    "        data_dobu = 0\n",
    "    key = \"Epinephrine\"\n",
    "    \n",
    "    try:\n",
    "        data_epi = max(matrix[:, var_to_ind[key]][torch.nonzero(matrix[:, var_to_ind[key]], as_tuple=True)]*mean_std_dict[key][1]+mean_std_dict[key][0], default=1)\n",
    "        data_epi = data_epi  /60/weight*1000\n",
    "    except:\n",
    "        data_epi = 0\n",
    "    key = \"Norepinephrine\"\n",
    "    \n",
    "    try:\n",
    "        data_nore = max(matrix[:, var_to_ind[key]][torch.nonzero(matrix[:, var_to_ind[key]], as_tuple=True)]*mean_std_dict[key][1]+mean_std_dict[key][0], default=1)\n",
    "        data_nore = data_nore /60/weight*1000\n",
    "    except:\n",
    "        data_nore = 0\n",
    "        \n",
    "\n",
    "\n",
    "    key = \"SBP\"\n",
    "    SBP = (matrix[:, var_to_ind[key]]*mean_std_dict[key][1]+mean_std_dict[key][0])\n",
    "\n",
    "    key = \"DBP\"\n",
    "    DBP = (matrix[:, var_to_ind[key]]*mean_std_dict[key][1]+mean_std_dict[key][0])\n",
    "\n",
    "    MAP = 2/3 * DBP + 1/3 * SBP\n",
    "    MAP = min(MAP[torch.nonzero(MAP, as_tuple=True)], default=100)\n",
    "                 \n",
    "        \n",
    "    return MAP, data_dop, data_dobu, data_epi, data_nore \n",
    "    \n",
    "def CS_SOFA(data):\n",
    "    map = data[0]\n",
    "    dop, dobu, epi, nore = data[1:5]\n",
    "    # print('CS data: mdden', data)\n",
    "    if (dop > 15) or (epi > 0.1) or (nore > 0.01): CS = 4\n",
    "    elif (dop > 5) or (epi > 0) or (nore > 0): CS = 3\n",
    "    elif (dop > 0) or (dobu > 0): CS = 2\n",
    "    elif map < 70: CS = 1\n",
    "    else: CS = 0\n",
    "    # print('CS Sofa is:', CS)\n",
    "    return CS \n",
    "    \n",
    "factor_keys =[\"GCS_eye\", \"GCS_motor\", \"GCS_verbal\", \"Bilirubin (Total)\", \"Platelet Count\", \"Urine\", \"Creatinine Blood\", \"FiO2\", \"PO2\", \"Dopamine\", \"Dobutamine\",\n",
    "\"Epinephrine\", \"Norepinephrine\", \"SBP\", \"DBP\"]\n",
    "factor_indices = sorted([var_to_ind[x]-1 for x in factor_keys])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b18c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fore_max_len = 880\n",
    "# Read data.\n",
    "# data_path = '/home/mitarb/fracarolli/files/230613_STraTS_preprocessed/mimic_iii_preprocessed.pkl'\n",
    "data, oc, train_ind, valid_ind, test_ind = pickle.load(open(data_path, 'rb'))\n",
    "means_stds = data.groupby(\"variable\").agg({\"mean\":\"first\", \"std\":\"first\"})\n",
    "#print(means_stds)\n",
    "mean_std_dict = dict()\n",
    "print(means_stds.keys)\n",
    "for pos, row in means_stds.iterrows():\n",
    "    print(pos)\n",
    "    print(row)\n",
    "    mean_std_dict[pos] = (float(row[\"mean\"]), float(row[\"std\"]))\n",
    "print(mean_std_dict)\n",
    "#raise Exception\n",
    "# Filter labeled data in first 24h.\n",
    "data = data.loc[data.ts_ind.isin(np.concatenate((train_ind, valid_ind, test_ind), axis=-1))]\n",
    "data = data.loc[(data.hour >= 0) & (data.hour <= 48)]\n",
    "old_oc = oc\n",
    "oc = oc.loc[oc.ts_ind.isin(np.concatenate((train_ind, valid_ind, test_ind), axis=-1))]\n",
    "# Fix age.\n",
    "data.loc[(data.variable == 'Age') & (data.value > 200), 'value'] = 91.4\n",
    "# Get y and N.\n",
    "y = np.array(oc.sort_values(by='ts_ind')['in_hospital_mortality']).astype('float32')\n",
    "print(y.shape)\n",
    "#raise Exception\n",
    "data['ts_ind'] = data['ts_ind'].astype(int) \n",
    "N = int(data.ts_ind.max() + 1)\n",
    "\n",
    "# Create demographic/static data\n",
    "# Get static data with mean fill and missingness indicator.\n",
    "static_varis = ['Age', 'Gender']\n",
    "ii = data.variable.isin(static_varis)\n",
    "static_data = data.loc[ii]\n",
    "data = data.loc[~ii]  # reduce data to non-demo/non-static\n",
    "static_var_to_ind = inv_list(static_varis)\n",
    "D = len(static_varis)\n",
    "demo = np.zeros((int(N), D))\n",
    "for row in tqdm(static_data.itertuples()):\n",
    "    demo[int(row.ts_ind), static_var_to_ind[row.variable]] = row.value\n",
    "# Normalize static data.\n",
    "means = demo.mean(axis=0, keepdims=True)\n",
    "stds = demo.std(axis=0, keepdims=True)\n",
    "stds = (stds == 0)*1 + (stds != 0)*stds\n",
    "demo = (demo-means)/stds\n",
    "\n",
    "# Trim to max len.\n",
    "data = data.sample(frac=1)\n",
    "data = data.groupby('ts_ind').head(880)\n",
    "\n",
    "# Get N, V, var_to_ind.\n",
    "#N = data.ts_ind.max() + 1\n",
    "varis = sorted(list(set(data.variable)))\n",
    "# V = len(varis)\n",
    "var_to_ind = inv_list(varis, start=1)\n",
    "data['vind'] = data.variable.map(var_to_ind)\n",
    "data = data[['ts_ind', 'vind', 'hour', 'value']].sort_values(by=['ts_ind', 'vind', 'hour'])\n",
    "# Add obs index.\n",
    "data = data.sort_values(by=['ts_ind']).reset_index(drop=True)\n",
    "data = data.reset_index().rename(columns={'index': 'obs_ind'})\n",
    "data = data.merge(data.groupby('ts_ind').agg({'obs_ind': 'min'}).reset_index().rename(columns={\n",
    "                                                            'obs_ind': 'first_obs_ind'}), on='ts_ind')\n",
    "data['obs_ind'] = data['obs_ind'] - data['first_obs_ind']\n",
    "# Find max_len.\n",
    "max_len = data.obs_ind.max()+1\n",
    "print('max_len', max_len)\n",
    "# Generate times_ip and values_ip matrices.\n",
    "times_inp = np.zeros((N, max_len), dtype='float32')\n",
    "values_inp = np.zeros((N, max_len), dtype='float32')\n",
    "varis_inp = np.zeros((N, max_len), dtype='int32')\n",
    "for row in tqdm(data.itertuples()):\n",
    "    ts_ind = row.ts_ind\n",
    "    l = row.obs_ind\n",
    "    times_inp[ts_ind, l] = row.hour\n",
    "    values_inp[ts_ind, l] = row.value\n",
    "    varis_inp[ts_ind, l] = row.vind\n",
    "\n",
    "w=0\n",
    "\n",
    "fore_in = []\n",
    "\n",
    "pred_data = data.loc[(data.hour>=0)&(data.hour<=48)]\n",
    "pred_data = pred_data.groupby(['ts_ind', 'vind']).agg({'value':'first'}).reset_index()\n",
    "pred_data['vind_value'] = pred_data[['vind', 'value']].values.tolist()\n",
    "pred_data = pred_data.groupby('ts_ind').agg({'vind_value':list}).reset_index()\n",
    "pred_data['vind_value'] = pred_data['vind_value'].apply(f)   \n",
    "\n",
    "obs_data = data.loc[(data.hour < 48) & (data.hour >= 0)]\n",
    "resultdict = dict()\n",
    "for ts_ind in obs_data.ts_ind:\n",
    "    resultdict[ts_ind] = [[]]\n",
    "obs_data = obs_data.loc[obs_data.ts_ind.isin(pred_data.ts_ind)]\n",
    "obs_data = obs_data.groupby('ts_ind').head(fore_max_len)\n",
    "obs_data = obs_data.groupby('ts_ind').agg({'vind': list, 'hour': list, 'value': list}).reset_index()\n",
    "\n",
    "for pred_window  in range(0, 48, 1):\n",
    "    print(pred_window)\n",
    "    #print(w+1 +pred_window)\n",
    "    #print(w+pred_window)\n",
    "    pred_data = data.loc[(data.hour >= w+pred_window) & (data.hour <= w+1 +pred_window)]\n",
    "    pred_data = pred_data.groupby(['ts_ind', 'vind']).agg({'value': 'first'}).reset_index()\n",
    "    pred_data['vind_value'+str(pred_window)] = pred_data[['vind', 'value']].values.tolist()\n",
    "    pred_data = pred_data.groupby('ts_ind').agg({'vind_value'+str(pred_window): list}).reset_index()\n",
    "    pred_data['vind_value'+str(pred_window)] = pred_data['vind_value'+str(pred_window)].apply(f)  # 721 entries with 2*129 vind_values\n",
    "    obs_data = obs_data.merge(pred_data, on='ts_ind')\n",
    "    #print(list(data['vind_value'+str(pred_window)]))\n",
    "    #print(len(list(data['vind_value'+str(pred_window)])))\n",
    "    #print(len(list(data['vind_value'+str(pred_window)])[0]))\n",
    "\n",
    "blub = (np.array(list([list(obs_data['vind_value'+str(pred_window)]) for pred_window in range(0, 48, 1)])))\n",
    "print(\"bleb\", blub.shape)\n",
    "op = np.swapaxes(blub, 0, 1)\n",
    "\n",
    "weird_oc = oc.loc[oc.ts_ind.isin(obs_data.ts_ind)]\n",
    "y = np.array(weird_oc.sort_values(by='ts_ind')['in_hospital_mortality']).astype('float32')\n",
    "print(y.shape)\n",
    "data.drop(columns=['obs_ind', 'first_obs_ind'], inplace=True)\n",
    "\n",
    "train_ind = [x for x in train_ind if x < op.shape[0]]\n",
    "valid_ind = [x for x in valid_ind if x < op.shape[0]]\n",
    "test_ind = [x for x in test_ind if x < op.shape[0]]\n",
    "\n",
    "train_input = op[train_ind]\n",
    "valid_input = op[valid_ind]\n",
    "test_input = op[test_ind]\n",
    "\n",
    "# Generate 3 sets of inputs and outputs.\n",
    "train_ip = [ip[train_ind] for ip in [demo, times_inp, values_inp, varis_inp]]\n",
    "valid_ip = [ip[valid_ind] for ip in [demo, times_inp, values_inp, varis_inp]]\n",
    "test_ip = [ip[test_ind] for ip in [demo, times_inp, values_inp, varis_inp]]\n",
    "del demo, times_inp, values_inp, varis_inp  # warum wird demo nicht gelöscht?\n",
    "\n",
    "if test_cond == 1:\n",
    "    tr_ind = [divmod(tr, 12)[0] for tr in train_ind]\n",
    "    va_ind = [divmod(tr, 12)[0] for tr in valid_ind]\n",
    "    te_ind = [divmod(tr, 12)[0] for tr in test_ind]\n",
    "    \n",
    "    train_op = y[tr_ind]  # is a problem for the test case...\n",
    "    valid_op = y[va_ind]\n",
    "    test_op = y[te_ind]\n",
    "else:\n",
    "    train_op = y[train_ind]  # is a problem for the test case...\n",
    "    valid_op = y[valid_ind]\n",
    "    test_op = y[test_ind]\n",
    "print('y is:', y)\n",
    "del y\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4241bbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Michi: die ganzen Argparses könnte man noch anders implementieren, wollte es nur schnell zum laufen bringen.\n",
    "import argparse\n",
    "parser = argparse.ArgumentParser(description='Autoformer & Transformer family for Time Series Forecasting')\n",
    "\n",
    "# basic config\n",
    "parser.add_argument('--is_training', type=int, required=False, default=1, help='status')\n",
    "parser.add_argument('--train_only', type=bool, required=False, default=False, help='perform training on full input dataset without validation and testing')\n",
    "parser.add_argument('--model_id', type=str, required=False, default='test', help='model id')\n",
    "parser.add_argument('--model', type=str, required=False, default='Autoformer',\n",
    "                    help='model name, options: [Autoformer, Informer, Transformer]')\n",
    "\n",
    "# data loader\n",
    "parser.add_argument('--data', type=str, required=False, default='ETTm1', help='dataset type')\n",
    "parser.add_argument('--root_path', type=str, default='./data/ETT/', help='root path of the data file')\n",
    "parser.add_argument('--data_path', type=str, default='ETTh1.csv', help='data file')\n",
    "parser.add_argument('--features', type=str, default='M',\n",
    "                    help='forecasting task, options:[M, S, MS]; M:multivariate predict multivariate, S:univariate predict univariate, MS:multivariate predict univariate')\n",
    "parser.add_argument('--target', type=str, default='OT', help='target feature in S or MS task')\n",
    "parser.add_argument('--freq', type=str, default='h',\n",
    "                    help='freq for time features encoding, options:[s:secondly, t:minutely, h:hourly, d:daily, b:business days, w:weekly, m:monthly], you can also use more detailed freq like 15min or 3h')\n",
    "parser.add_argument('--checkpoints', type=str, default='./checkpoints/', help='location of model checkpoints')\n",
    "\n",
    "# forecasting task\n",
    "parser.add_argument('--seq_len', type=int, default=24, help='input sequence length')\n",
    "parser.add_argument('--label_len', type=int, default=0, help='start token length')\n",
    "parser.add_argument('--pred_len', type=int, default=24, help='prediction sequence length')\n",
    "\n",
    "\n",
    "# DLinear\n",
    "parser.add_argument('--individual', action='store_true', default=False, help='DLinear: a linear layer for each variate(channel) individually')\n",
    "# Formers \n",
    "#Michi: bisher nur default=3 zum laufen gebracht\n",
    "parser.add_argument('--embed_type', type=int, default=3, help='0: default 1: value embedding + temporal embedding + positional embedding 2: value embedding + temporal embedding 3: value embedding + positional embedding 4: value embedding')\n",
    "parser.add_argument('--enc_in', type=int, default=262, help='encoder input size') # DLinear with --individual, use this hyperparameter as the number of channels\n",
    "parser.add_argument('--dec_in', type=int, default=131, help='decoder input size')\n",
    "parser.add_argument('--c_out', type=int, default=131, help='output size')\n",
    "parser.add_argument('--d_model', type=int, default=512, help='dimension of model')\n",
    "parser.add_argument('--n_heads', type=int, default=8, help='num of heads')\n",
    "parser.add_argument('--e_layers', type=int, default=2, help='num of encoder layers')\n",
    "parser.add_argument('--d_layers', type=int, default=1, help='num of decoder layers')\n",
    "parser.add_argument('--d_ff', type=int, default=2048, help='dimension of fcn')\n",
    "parser.add_argument('--moving_avg', type=int, default=25, help='window size of moving average')\n",
    "parser.add_argument('--factor', type=int, default=1, help='attn factor')\n",
    "parser.add_argument('--distil', action='store_false',\n",
    "                    help='whether to use distilling in encoder, using this argument means not using distilling',\n",
    "                    default=True)\n",
    "parser.add_argument('--dropout', type=float, default=0.05, help='dropout')\n",
    "parser.add_argument('--embed', type=str, default='timeF',\n",
    "                    help='time features encoding, options:[timeF, fixed, learned]')\n",
    "parser.add_argument('--activation', type=str, default='gelu', help='activation')\n",
    "parser.add_argument('--output_attention', action='store_true', help='whether to output attention in ecoder')\n",
    "parser.add_argument('--do_predict', action='store_true', help='whether to predict unseen future data')\n",
    "\n",
    "# optimization\n",
    "parser.add_argument('--num_workers', type=int, default=10, help='data loader num workers')\n",
    "parser.add_argument('--itr', type=int, default=2, help='experiments times')\n",
    "parser.add_argument('--train_epochs', type=int, default=10, help='train epochs')\n",
    "parser.add_argument('--batch_size', type=int, default=32, help='batch size of train input data')\n",
    "parser.add_argument('--patience', type=int, default=3, help='early stopping patience')\n",
    "parser.add_argument('--learning_rate', type=float, default=0.0001, help='optimizer learning rate')\n",
    "parser.add_argument('--des', type=str, default='test', help='exp description')\n",
    "parser.add_argument('--loss', type=str, default='mse', help='loss function')\n",
    "parser.add_argument('--lradj', type=str, default='type1', help='adjust learning rate')\n",
    "parser.add_argument('--use_amp', action='store_true', help='use automatic mixed precision training', default=False)\n",
    "\n",
    "# GPU\n",
    "parser.add_argument('--use_gpu', type=bool, default=True, help='use gpu')\n",
    "parser.add_argument('--gpu', type=int, default=0, help='gpu')\n",
    "parser.add_argument('--use_multi_gpu', action='store_true', help='use multiple gpus', default=False)\n",
    "parser.add_argument('--devices', type=str, default='0,1,2,3', help='device ids of multile gpus')\n",
    "parser.add_argument('--test_flop', action='store_true', default=False, help='See utils/tools for usage')\n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "import importlib\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b118361e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import models.InformerAutoregressiveFullRegression as autoformer\n",
    "importlib.reload(autoformer)\n",
    "model = autoformer.Model(args).cuda()\n",
    "\n",
    "lr, batch_size, samples_per_epoch, patience = 0.0005, 32, int(102400/sample_divisor), 6\n",
    "d, N, he, dropout = 50, 2, 4, 0.2\n",
    "V=131\n",
    "print('number of parameters: ', V)\n",
    "\n",
    "\n",
    "#a = summary(model, [(32, 2), (32, 880), (32, 880), (32, 880)],  # shape of fore_train_ip\n",
    "#            dtypes=[torch.float, torch.float, torch.float, torch.long])\n",
    "#print(a)  # Model summary\n",
    "# raise Exception\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=3)\n",
    "# Pretrain fore_model.\n",
    "best_val_loss = np.inf\n",
    "N_fore = len(fore_train_op)\n",
    "#print(N_fore)\n",
    "fore_savepath = 'informer_regression2.pytorch'\n",
    "loss_func = torch.nn.MSELoss(reduction=\"none\")\n",
    "\n",
    "# torch.compile(model)\n",
    "for e in range(number_of_epochs):\n",
    "    e_indices = np.random.choice(range(N_fore), size=samples_per_epoch, replace=False)\n",
    "    e_loss = 0\n",
    "    pbar = tqdm(range(0, len(e_indices), batch_size))\n",
    "    model.train()\n",
    "    for start in pbar:\n",
    "        ind = e_indices[start:start+batch_size]\n",
    "\n",
    "        matrix = torch.tensor(fore_train_op[ind], dtype=torch.float32).cuda()\n",
    "        #torch.Size([32, 48, 258])\n",
    "        input_matrix = matrix[:, :24, :262]\n",
    "        #torch.Size([32, 24, 129])\n",
    "        input_mark = torch.arange(0, input_matrix.size(1)).unsqueeze(0).repeat(input_matrix.size(0), 1).cuda()\n",
    "        #torch.Size([32, 24])\n",
    "        input_mask = matrix[:, :24, 131:]\n",
    "        #torch.Size([32, 24, 129])\n",
    "        output_matrix = matrix[:, 24:, :131]\n",
    "        #torch.Size([32, 24, 129])\n",
    "        \n",
    "        output_mark = torch.arange(0, output_matrix.size(1)).unsqueeze(0).repeat(input_matrix.size(0), 1).cuda()\n",
    "        #torch.Size([32, 24])\n",
    "        output_mask = matrix[:, 24:, 131:]\n",
    "        #torch.Size([32, 24, 129])\n",
    "        dec_inp = torch.zeros_like(output_matrix[:, -args.pred_len:, :]).float()\n",
    "        #print(dec_inp.size())\n",
    "        #dec_inp = torch.cat([output_matrix[:, :args.label_len, :], dec_inp], dim=1).float().cuda()\n",
    "        #raise Exception\n",
    "        #print(str(type(model)))\n",
    "        #print(model)\n",
    "\n",
    "        output = model(input_matrix, input_mark, dec_inp, output_mark, tgt=output_matrix, trainn=False, backprop=True)#, enc_self_mask=input_mask, dec_self_mask=output_mask)\n",
    "        #print(output.size())\n",
    "        loss = 0\n",
    "        for outpu, real, mask, input_mat in zip(output, output_matrix, output_mask, input_matrix):\n",
    "            b = sum(get_sofa(real, var_to_ind))\n",
    "            loss_term = (outpu-b)**2\n",
    "            loss+=loss_term\n",
    "            #print(loss_term)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        e_loss += loss.detach()\n",
    "        pbar.set_description('%f' % e_loss)\n",
    "    val_loss = 0\n",
    "    #raise Exception\n",
    "    loss_list = []\n",
    "    model.eval()\n",
    "    pbar = tqdm(range(0, len(fore_valid_op), 32))  # len(fore_valid_op)           ####################   maybe also batch_size instead of 32\n",
    "    for start in pbar:\n",
    "        matrix = torch.tensor(fore_valid_op[start:start+batch_size], dtype=torch.float32).cuda()\n",
    "        #torch.Size([32, 48, 258])\n",
    "        input_matrix = matrix[:, :24, :262]\n",
    "        #torch.Size([32, 24, 129])\n",
    "        input_mark = torch.arange(0, input_matrix.size(1)).unsqueeze(0).repeat(input_matrix.size(0), 1).cuda()\n",
    "        #torch.Size([32, 24])\n",
    "        input_mask = matrix[:, :24, 131:]\n",
    "        #torch.Size([32, 24, 129])\n",
    "        output_matrix = matrix[:, 24:, :131]\n",
    "        #torch.Size([32, 24, 129])\n",
    "        output_mark = torch.arange(0, output_matrix.size(1)).unsqueeze(0).repeat(input_matrix.size(0), 1).cuda()\n",
    "        #torch.Size([32, 24])\n",
    "        output_mask = matrix[:, 24:, 131:]\n",
    "        #torch.Size([32, 24, 129])\n",
    "        dec_inp = torch.zeros_like(output_matrix[:, -args.pred_len:, :]).float()\n",
    "        #print(dec_inp.size())\n",
    "        #dec_inp = torch.cat([output_matrix[:, :args.label_len, :], dec_inp], dim=1).float().cuda()\n",
    "        #raise Exception\n",
    "        #print(repr(model))\n",
    "\n",
    "        output = model(input_matrix, input_mark, dec_inp, output_mark, trainn=False)#, enc_self_mask=input_mask, dec_self_mask=output_mask)\n",
    "        loss = 0\n",
    "        for outpu, real, mask, input_mat in zip(output, output_matrix, output_mask, input_matrix):\n",
    "            b = sum(get_sofa(real, var_to_ind))\n",
    "            loss_term = (outpu-b)**2\n",
    "            loss+=loss_term\n",
    "        val_loss += loss.detach().cpu()\n",
    "        pbar.set_description('%f' % val_loss)\n",
    "    loss_p = e_loss*batch_size/samples_per_epoch\n",
    "    val_loss_p = val_loss*batch_size/len(fore_valid_op)\n",
    "    print('Epoch', e, 'loss', loss_p, 'val loss', val_loss_p, \"mean and std\", np.mean(loss_list), np.std(loss_list))\n",
    "    with open('loss_values_log_regression', 'a') as f:\n",
    "        f.write(str(e)+' ' + str(loss_p.item()) + ' ' + str(val_loss_p.item())+ '\\n')\n",
    "    scheduler.step(val_loss_p.item())\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), fore_savepath)\n",
    "        best_epoch = e\n",
    "    if (e-best_epoch) > patience:\n",
    "        break\n",
    "    \n",
    "print('Training has ended.')\n",
    "\n",
    "#Informer IMS, Sampling, Backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bb9339",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
