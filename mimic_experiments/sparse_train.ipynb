{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99d86537",
   "metadata": {},
   "source": [
    "*ToDo*\n",
    "\n",
    "02) pred_window und obs_window ausprobieren.\n",
    "03) output nochmal genau anschauen.\n",
    "04) was ist maskiert anschauen - und wie genau?\n",
    "05) VRAM wenig ausgelastet. Batch size mal mit 320 ausprobieren, aber auch durchziehen.\n",
    "06) Task pred: scheduler einfügen.\n",
    "07) Analyse each var (sparsity)\n",
    "08) loss per var/quality of varis loss (is this already ablation to forecast only one var)\n",
    "09) Include sepsis definition\n",
    "10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "driven-termination",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-27T18:44:23.730468Z",
     "iopub.status.busy": "2023-09-27T18:44:23.729922Z",
     "iopub.status.idle": "2023-09-27T18:44:28.087910Z",
     "shell.execute_reply": "2023-09-27T18:44:28.086841Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, auc\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchinfo import summary\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c94544",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-27T18:44:28.092841Z",
     "iopub.status.busy": "2023-09-27T18:44:28.092321Z",
     "iopub.status.idle": "2023-09-27T18:44:28.100641Z",
     "shell.execute_reply": "2023-09-27T18:44:28.099632Z"
    }
   },
   "outputs": [],
   "source": [
    "# data settings\n",
    "test_cond = 0\n",
    "\n",
    "if test_cond == 1:\n",
    "    data_path = '/home/mitarb/fracarolli/files/231113_STraTS_new/mimic_iii_preprocessed_new_z_err20092.pkl'\n",
    "    sample_divisor = 100\n",
    "    number_of_epochs = 20\n",
    "else:\n",
    "    data_path = '/home/mitarb/fracarolli/files/231113_STraTS_new/mimic_iii_preprocessed_new_z_err20092.pkl'\n",
    "    sample_divisor = 1\n",
    "    number_of_epochs = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recreational-record",
   "metadata": {},
   "source": [
    "## Load forecast dataset into matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fdc7de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-27T18:44:28.105291Z",
     "iopub.status.busy": "2023-09-27T18:44:28.104966Z",
     "iopub.status.idle": "2023-09-27T18:44:28.114534Z",
     "shell.execute_reply": "2023-09-27T18:44:28.113623Z"
    }
   },
   "outputs": [],
   "source": [
    "def inv_list(l, start=0):  # Create vind\n",
    "    d = {}\n",
    "    for i in range(len(l)):\n",
    "        d[l[i]] = i+start\n",
    "    return d\n",
    "\n",
    "\n",
    "def f(x):\n",
    "    mask   = [0 for i in range(V)]\n",
    "    values = [0 for i in range(V)]\n",
    "    #print('hi', mask , V)\n",
    "    #mask, values = np.zeros(V), np.zeros(V)\n",
    "    #print('hi', mask)\n",
    "    for vv in x:  # tuple of ['vind','value']\n",
    "        v = int(vv[0])-1  # shift index of vind\n",
    "        mask[v] = 1\n",
    "        values[v] = vv[1]  # get value\n",
    "    return values+mask  # concat\n",
    "\n",
    "\n",
    "def pad(x):\n",
    "    if len(x) > 880:\n",
    "        print(len(x))\n",
    "    return x+[0]*(fore_max_len-len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "executive-annual",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-27T18:44:28.118811Z",
     "iopub.status.busy": "2023-09-27T18:44:28.118511Z",
     "iopub.status.idle": "2023-09-27T19:42:38.600945Z",
     "shell.execute_reply": "2023-09-27T19:42:38.599754Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred_window_old = 12  # hours\n",
    "obs_windows = range(0, 1, 1)\n",
    "\n",
    "# Read data.\n",
    "data, oc, train_ind, valid_ind, test_ind = pickle.load(open(data_path, 'rb'))\n",
    "# Remove test patients.\n",
    "print(data.head())\n",
    "raise Exception\n",
    "data = data.merge(oc[['ts_ind', 'SUBJECT_ID']], on='ts_ind', how='left')\n",
    "test_sub = oc.loc[oc.ts_ind.isin(test_ind)].SUBJECT_ID.unique()\n",
    "data = data.loc[~data.SUBJECT_ID.isin(test_sub)]\n",
    "oc = oc.loc[~oc.SUBJECT_ID.isin(test_sub)]\n",
    "data.drop(columns=['SUBJECT_ID', 'TABLE'], inplace=True)\n",
    "# Fix age.\n",
    "data.loc[(data.variable == 'Age') & (data.value > 200), 'value'] = 91.4\n",
    "# Get static data with mean fill and missingness indicator.\n",
    "static_varis = ['Age', 'Gender']\n",
    "ii = data.variable.isin(static_varis)\n",
    "static_data = data.loc[ii]  # static data are 'Age' and 'Gender'\n",
    "data = data.loc[~ii]  # ~ binary flip\n",
    "# print('data\\n',data)\n",
    "\n",
    "static_var_to_ind = inv_list(static_varis)  # {'Age': 0, 'Gender': 1}\n",
    "D = len(static_varis)  # 2 by definition\n",
    "N = data.ts_ind.max()+1  # /= 12: 52861\n",
    "demo = np.zeros((int(N), int(D)))\n",
    "for row in tqdm(static_data.itertuples()):\n",
    "    demo[int(row.ts_ind), static_var_to_ind[row.variable]] = row.value\n",
    "# print('Demo after tqdm command \\n',demo[:10])\n",
    "# Normalize static data.\n",
    "means = demo.mean(axis=0, keepdims=True)  # quite sparse\n",
    "stds = demo.std(axis=0, keepdims=True)\n",
    "stds = (stds == 0)*1 + (stds != 0)*stds\n",
    "demo = (demo-means)/stds\n",
    "# print('Demo after normalisation \\n',demo[:10])\n",
    "# Get variable indices.\n",
    "varis = sorted(list(set(data.variable)))\n",
    "V = len(varis)  # 129 for \\=12 with varis all variables except for static ones\n",
    "# print('varis', varis, V)\n",
    "var_to_ind = inv_list(varis, start=1)\n",
    "data['vind'] = data.variable.map(var_to_ind)\n",
    "data = data[['ts_ind', 'vind', 'hour', 'value']].sort_values(by=['ts_ind', 'vind', 'hour'])\n",
    "# print('data vind\\n', data['vind'], '\\n data\\n',data)\n",
    "# Find max_len.\n",
    "fore_max_len = 880  # hard coded max_len of vars\n",
    "# Get forecast inputs and outputs.\n",
    "fore_times_ip = []\n",
    "fore_values_ip = []\n",
    "fore_varis_ip = []\n",
    "fore_op = []\n",
    "fore_op_awesome = []\n",
    "fore_inds = []\n",
    "for w in tqdm(range(25, 124, 4)):  # range(20, 124, 4), pred_window=2\n",
    "    pred_data = data.loc[(data.hour>=w)&(data.hour<=w+24)]\n",
    "    pred_data = pred_data.groupby(['ts_ind', 'vind']).agg({'value':'first'}).reset_index()\n",
    "    pred_data['vind_value'] = pred_data[['vind', 'value']].values.tolist()\n",
    "    pred_data = pred_data.groupby('ts_ind').agg({'vind_value':list}).reset_index()\n",
    "    pred_data['vind_value'] = pred_data['vind_value'].apply(f)   \n",
    "\n",
    "    obs_data = data.loc[(data.hour < w) & (data.hour >= w-24)]\n",
    "    obs_data = obs_data.loc[obs_data.ts_ind.isin(pred_data.ts_ind)]\n",
    "    obs_data = obs_data.groupby('ts_ind').head(fore_max_len)\n",
    "    obs_data = obs_data.groupby('ts_ind').agg({'vind': list, 'hour': list, 'value': list}).reset_index()\n",
    "    #Michi: Hier hole ich mir die 24 Stunden vor, und 24 Stunden nach einem bestimmten Zeitpunkt raus\n",
    "    for pred_window  in range(-24, 24, 1):\n",
    "        pred_data = data.loc[(data.hour >= w+pred_window) & (data.hour <= w+1 +pred_window)]\n",
    "        pred_data = pred_data.groupby(['ts_ind', 'vind']).agg({'value': 'first'}).reset_index()\n",
    "        pred_data['vind_value'+str(pred_window)] = pred_data[['vind', 'value']].values.tolist()\n",
    "        pred_data = pred_data.groupby('ts_ind').agg({'vind_value'+str(pred_window): list}).reset_index()\n",
    "        pred_data['vind_value'+str(pred_window)] = pred_data['vind_value'+str(pred_window)].apply(f)  # 721 entries with 2*129 vind_values\n",
    "        obs_data = obs_data.merge(pred_data, on='ts_ind')\n",
    "\n",
    "    for col in ['vind', 'hour', 'value']:\n",
    "        obs_data[col] = obs_data[col].apply(pad)\n",
    "    fore_op_awesome.append(np.array(list([list(obs_data['vind_value'+str(pred_window)]) for pred_window in range(0, 24, 1)])))\n",
    "    #fore_op.append(np.array(list(obs_data.vind_value)))\n",
    "    fore_inds.append(np.array([int(x) for x in list(obs_data.ts_ind)]))\n",
    "    fore_times_ip.append(np.array(list(obs_data.hour)))\n",
    "    fore_values_ip.append(np.array(list(obs_data.value)))\n",
    "    fore_varis_ip.append(np.array(list(obs_data.vind)))\n",
    "    \n",
    "del data\n",
    "fore_times_ip = np.concatenate(fore_times_ip, axis=0)\n",
    "fore_values_ip = np.concatenate(fore_values_ip, axis=0)\n",
    "fore_varis_ip = np.concatenate(fore_varis_ip, axis=0)\n",
    "\n",
    "fore_op_awesome = np.concatenate(fore_op_awesome, axis=1)\n",
    "fore_op = np.swapaxes(fore_op_awesome, 0, 1)\n",
    "print(fore_op.shape)\n",
    "\n",
    "#raise Exception\n",
    "fore_inds = np.concatenate(fore_inds, axis=0)\n",
    "fore_demo = demo[fore_inds]\n",
    "# Get train and valid ts_ind for forecast task.\n",
    "train_sub = oc.loc[oc.ts_ind.isin(train_ind)].SUBJECT_ID.unique()\n",
    "valid_sub = oc.loc[oc.ts_ind.isin(valid_ind)].SUBJECT_ID.unique()\n",
    "rem_sub = oc.loc[~oc.SUBJECT_ID.isin(np.concatenate((train_ind, valid_ind)))].SUBJECT_ID.unique()\n",
    "bp = int(0.8*len(rem_sub))\n",
    "train_sub = np.concatenate((train_sub, rem_sub[:bp]))\n",
    "valid_sub = np.concatenate((valid_sub, rem_sub[bp:]))\n",
    "train_ind = oc.loc[oc.SUBJECT_ID.isin(train_sub)].ts_ind.unique()  # Add remaining ts_ind s of train subjects.\n",
    "valid_ind = oc.loc[oc.SUBJECT_ID.isin(valid_sub)].ts_ind.unique()  # Add remaining ts_ind s of train subjects.\n",
    "# Generate 3 sets of inputs and outputs.\n",
    "train_ind = np.argwhere(np.in1d(fore_inds, train_ind)).flatten()\n",
    "valid_ind = np.argwhere(np.in1d(fore_inds, valid_ind)).flatten()\n",
    "fore_train_ip = [ip[train_ind] for ip in [fore_demo, fore_times_ip, fore_values_ip, fore_varis_ip]]\n",
    "fore_valid_ip = [ip[valid_ind] for ip in [fore_demo, fore_times_ip, fore_values_ip, fore_varis_ip]]\n",
    "del fore_times_ip, fore_values_ip, fore_varis_ip, demo, fore_demo\n",
    "fore_train_op = fore_op[train_ind]\n",
    "fore_valid_op = fore_op[valid_ind]\n",
    "del fore_op\n",
    "\n",
    "print('lengths of rem_sub, fore_train_ip[1], fore_valid_ip[0]')\n",
    "print(len(rem_sub), fore_train_ip[1].shape, fore_valid_ip[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imperial-concern",
   "metadata": {},
   "source": [
    "## Load target dataset into matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29484c03",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-27T19:42:38.605135Z",
     "iopub.status.busy": "2023-09-27T19:42:38.604810Z",
     "iopub.status.idle": "2023-09-27T19:42:38.616687Z",
     "shell.execute_reply": "2023-09-27T19:42:38.615822Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_sofa(matrix): #24x131 matrix\n",
    "    # GCS: min_eye, min_motor, min_verbal = 5, 5, 5\n",
    "    print(matrix.size())\n",
    "    #raise Exception\n",
    "    key =\"GCS_eye\"\n",
    "    a=matrix[:, var_to_ind[key]][torch.nonzero(matrix[:, var_to_ind[key]], as_tuple=True)]\n",
    "    print(a)\n",
    "    min_eye = min(matrix[:, var_to_ind[key]][torch.nonzero(matrix[:, var_to_ind[key]], as_tuple=True)]*mean_std_dict[key][1]+mean_std_dict[key][0], default=5)\n",
    "    #print(\"calculatedvalue\", min_eye)\n",
    "    #print(\"table_value\", matrix[:, var_to_ind[key]])\n",
    "    #print(\"std\", mean_std_dict[key][1])\n",
    "    #print(\"mean\", mean_std_dict[key][0])\n",
    "    key = \"GCS_motor\"\n",
    "    min_motor = min(matrix[:, var_to_ind[key]][torch.nonzero(matrix[:, var_to_ind[key]], as_tuple=True)]*mean_std_dict[key][1]+mean_std_dict[key][0], default=5)\n",
    "    key = \"GCS_verbal\"\n",
    "    min_verbal = min(matrix[:, var_to_ind[key]][torch.nonzero(matrix[:, var_to_ind[key]], as_tuple=True)]*mean_std_dict[key][1]+mean_std_dict[key][0], default=5)\n",
    "    \n",
    "\n",
    "    GCS = min_eye + min_motor + min_verbal\n",
    "    if GCS > 14: GCS_sofa = 0\n",
    "    elif GCS > 12: GCS_sofa = 1\n",
    "    elif GCS > 9:  GCS_sofa = 2\n",
    "    elif GCS > 5:  GCS_sofa = 3\n",
    "    else: GCS_sofa = 4\n",
    "    print('GCS_sofa is', GCS_sofa, ';     GCS is', GCS,'; GCS eye', min_eye, '; GCS motor', min_motor, '; GCS verbal', min_verbal)\n",
    "\n",
    "    # bilirubin Liver SOFA\n",
    "\n",
    "    key = \"Bilirubin (Total)\"\n",
    "    bilir = max(matrix[:, var_to_ind[key]][torch.nonzero(matrix[:, var_to_ind[key]], as_tuple=True)]*mean_std_dict[key][1]+mean_std_dict[key][0], default=1)\n",
    "    print(\"calculatedvalue\", bilir)\n",
    "    print(\"table_value\", matrix[:, var_to_ind[key]])\n",
    "    print(\"std\", mean_std_dict[key][1])\n",
    "    print(\"mean\", mean_std_dict[key][0])\n",
    "    if bilir > 12: bilir_sofa = 4\n",
    "    elif bilir > 6: bilir_sofa = 3\n",
    "    elif bilir > 2: bilir_sofa = 2\n",
    "    elif bilir > 1.2: bilir_sofa = 1\n",
    "    else: bilir_sofa = 0\n",
    "    print('bilir_Sofa is', bilir_sofa, ';   bilirubin is', bilir)\n",
    "    \n",
    "    # Coagulation (Platelets)\n",
    "    key = \"Platelet Count\"\n",
    "    plate = max(matrix[:, var_to_ind[key]][torch.nonzero(matrix[:, var_to_ind[key]], as_tuple=True)]*mean_std_dict[key][1]+mean_std_dict[key][0], default=160)\n",
    "    if plate > 150: plate_sofa = 0\n",
    "    elif plate > 100: plate_sofa = 1\n",
    "    elif plate > 50: plate_sofa = 2\n",
    "    elif plate > 20: plate_sofa = 4\n",
    "    else: plate_sofa = 4\n",
    "    print('plate_sofa is', plate_sofa, ';   platelet count is', plate)\n",
    "    \n",
    "    # print('Urinmenge 24h', sum(data_var[data_var['variable']=='Urine']['value2']))\n",
    "\n",
    "    key = \"Urine\"\n",
    "    urine = sum(matrix[:, var_to_ind[key]][torch.nonzero(matrix[:, var_to_ind[key]], as_tuple=True)]*mean_std_dict[key][1]+mean_std_dict[key][0])\n",
    "    key = \"Creatinine Blood\"\n",
    "    creat = max(matrix[:, var_to_ind[key]][torch.nonzero(matrix[:, var_to_ind[key]], as_tuple=True)]*mean_std_dict[key][1]+mean_std_dict[key][0], default=1)\n",
    "    \n",
    "    if (urine < 200) or (creat > 5): renal_sofa = 4\n",
    "    elif  (urine < 500) or (creat > 3.5): renal_sofa = 3\n",
    "    elif creat > 2.0: renal_sofa = 2\n",
    "    elif creat > 1.2: renal_sofa = 1\n",
    "    else: renal_sofa = 0\n",
    "    print('renal_sofa:',renal_sofa,';       urine 24:',urine,'; creat:', creat)\n",
    "    \n",
    "    CS_data = get_CS(matrix)\n",
    "    cs_sofa = CS_SOFA(CS_data)\n",
    "    \n",
    "    #cs_sofa = 0\n",
    "    key=\"FiO2\"\n",
    "    fio2 = (matrix[:, var_to_ind[key]]*mean_std_dict[key][1]+mean_std_dict[key][0])\n",
    "    key=\"PO2\"\n",
    "    po2 = (matrix[:, var_to_ind[key]]*mean_std_dict[key][1]+mean_std_dict[key][0])\n",
    "    PaO2FiO2 = 100*po2/fio2\n",
    "    print(\"size\", PaO2FiO2.size())\n",
    "    PaO2FiO2 = PaO2FiO2[torch.nonzero(PaO2FiO2, as_tuple=True)]\n",
    "    pao2fio2 = min(PaO2FiO2)\n",
    "    if pao2fio2<100: resp=4\n",
    "    elif pao2fio2<200: resp=3\n",
    "    elif pao2fio2<300:resp=2\n",
    "    elif pao2fio2<400:resp=1\n",
    "    else: resp=0\n",
    "    return GCS_sofa, cs_sofa, resp, plate_sofa, bilir_sofa, renal_sofa\n",
    "\n",
    "def get_CS(matrix):\n",
    "    #data_var = data_pat[data_pat['variable'].isin(['Dobutamine','Dopamine','Epinephrine','Norepinephrine','Weight'])]\n",
    "    #data_var['value2'] = data_var['value']*data_var['std']+data_var['mean']\n",
    "    \n",
    "    #weight = min(data_var[data_var['variable']=='Weight']['value2'], default=80)  # set default weight to 80kg.\n",
    "    key = \"Weight\"\n",
    "    weight = min(matrix[:, var_to_ind[key]][torch.nonzero(matrix[:, var_to_ind[key]], as_tuple=True)], default=80)#*mean_std_dict[key][1]+mean_std_dict[key][0], default=1)\n",
    "    key = \"Dopamine\"\n",
    "    try:\n",
    "        data_dop = max(matrix[:, var_to_ind[key]][torch.nonzero(matrix[:, var_to_ind[key]], as_tuple=True)]*mean_std_dict[key][1]+mean_std_dict[key][0], default=1)\n",
    "        data_dop = data_dop /60/weight*1000\n",
    "    except:\n",
    "        data_dop = 0\n",
    "\n",
    "    key = \"Dobutamine\"\n",
    "    \n",
    "    try:\n",
    "        data_dobu = max(matrix[:, var_to_ind[key]][torch.nonzero(matrix[:, var_to_ind[key]], as_tuple=True)]*mean_std_dict[key][1]+mean_std_dict[key][0], default=1)\n",
    "        data_dobu = data_dop /60/weight*1000\n",
    "    except:\n",
    "        data_dobu = 0\n",
    "    key = \"Epinephrine\"\n",
    "    \n",
    "    try:\n",
    "        data_epi = max(matrix[:, var_to_ind[key]][torch.nonzero(matrix[:, var_to_ind[key]], as_tuple=True)]*mean_std_dict[key][1]+mean_std_dict[key][0], default=1)\n",
    "        data_epi = data_dop /60/weight*1000\n",
    "    except:\n",
    "        data_epi = 0\n",
    "    key = \"Norepinephrine\"\n",
    "    \n",
    "    try:\n",
    "        data_nore = max(matrix[:, var_to_ind[key]][torch.nonzero(matrix[:, var_to_ind[key]], as_tuple=True)]*mean_std_dict[key][1]+mean_std_dict[key][0], default=1)\n",
    "        data_nore = data_dop /60/weight*1000\n",
    "    except:\n",
    "        data_nore = 0\n",
    "        \n",
    "\n",
    "\n",
    "    key = \"SBP\"\n",
    "    SBP = (matrix[:, var_to_ind[key]]*mean_std_dict[key][1]+mean_std_dict[key][0])\n",
    "\n",
    "    key = \"DBP\"\n",
    "    DBP = (matrix[:, var_to_ind[key]]*mean_std_dict[key][1]+mean_std_dict[key][0])\n",
    "\n",
    "    MAP = 2/3 * DBP + 1/3 * SBP\n",
    "    MAP = min(MAP[torch.nonzero(MAP, as_tuple=True)], default=100)\n",
    "                 \n",
    "        \n",
    "    return MAP, data_dop, data_dobu, data_epi, data_nore \n",
    "    \n",
    "def CS_SOFA(data):\n",
    "    map = data[0]\n",
    "    dop, dobu, epi, nore = data[1:5]\n",
    "    # print('CS data: mdden', data)\n",
    "    if (dop > 15) or (epi > 0.1) or (nore > 0.01): CS = 4\n",
    "    elif (dop > 5) or (epi > 0) or (nore > 0): CS = 3\n",
    "    elif (dop > 0) or (dobu > 0): CS = 2\n",
    "    elif map < 70: CS = 1\n",
    "    else: CS = 0\n",
    "    # print('CS Sofa is:', CS)\n",
    "    return CS \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3491ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fore_max_len = 880\n",
    "# Read data.\n",
    "# data_path = '/home/mitarb/fracarolli/files/230613_STraTS_preprocessed/mimic_iii_preprocessed.pkl'\n",
    "data, oc, train_ind, valid_ind, test_ind = pickle.load(open(data_path, 'rb'))\n",
    "means_stds = data.groupby(\"variable\").agg({\"mean\":\"first\", \"std\":\"first\"})\n",
    "#print(means_stds)\n",
    "mean_std_dict = dict()\n",
    "print(means_stds.keys)\n",
    "for pos, row in means_stds.iterrows():\n",
    "    print(pos)\n",
    "    print(row)\n",
    "    mean_std_dict[pos] = (float(row[\"mean\"]), float(row[\"std\"]))\n",
    "print(mean_std_dict)\n",
    "#raise Exception\n",
    "# Filter labeled data in first 24h.\n",
    "data = data.loc[data.ts_ind.isin(np.concatenate((train_ind, valid_ind, test_ind), axis=-1))]\n",
    "data = data.loc[(data.hour >= 0) & (data.hour <= 48)]\n",
    "old_oc = oc\n",
    "oc = oc.loc[oc.ts_ind.isin(np.concatenate((train_ind, valid_ind, test_ind), axis=-1))]\n",
    "# Fix age.\n",
    "data.loc[(data.variable == 'Age') & (data.value > 200), 'value'] = 91.4\n",
    "# Get y and N.\n",
    "y = np.array(oc.sort_values(by='ts_ind')['in_hospital_mortality']).astype('float32')\n",
    "print(y.shape)\n",
    "#raise Exception\n",
    "data['ts_ind'] = data['ts_ind'].astype(int) \n",
    "N = int(data.ts_ind.max() + 1)\n",
    "\n",
    "# Create demographic/static data\n",
    "# Get static data with mean fill and missingness indicator.\n",
    "static_varis = ['Age', 'Gender']\n",
    "ii = data.variable.isin(static_varis)\n",
    "static_data = data.loc[ii]\n",
    "data = data.loc[~ii]  # reduce data to non-demo/non-static\n",
    "static_var_to_ind = inv_list(static_varis)\n",
    "D = len(static_varis)\n",
    "demo = np.zeros((int(N), D))\n",
    "for row in tqdm(static_data.itertuples()):\n",
    "    demo[int(row.ts_ind), static_var_to_ind[row.variable]] = row.value\n",
    "# Normalize static data.\n",
    "means = demo.mean(axis=0, keepdims=True)\n",
    "stds = demo.std(axis=0, keepdims=True)\n",
    "stds = (stds == 0)*1 + (stds != 0)*stds\n",
    "demo = (demo-means)/stds\n",
    "\n",
    "# Trim to max len.\n",
    "data = data.sample(frac=1)\n",
    "data = data.groupby('ts_ind').head(880)\n",
    "\n",
    "# Get N, V, var_to_ind.\n",
    "#N = data.ts_ind.max() + 1\n",
    "varis = sorted(list(set(data.variable)))\n",
    "# V = len(varis)\n",
    "var_to_ind = inv_list(varis, start=1)\n",
    "data['vind'] = data.variable.map(var_to_ind)\n",
    "data = data[['ts_ind', 'vind', 'hour', 'value']].sort_values(by=['ts_ind', 'vind', 'hour'])\n",
    "# Add obs index.\n",
    "data = data.sort_values(by=['ts_ind']).reset_index(drop=True)\n",
    "data = data.reset_index().rename(columns={'index': 'obs_ind'})\n",
    "data = data.merge(data.groupby('ts_ind').agg({'obs_ind': 'min'}).reset_index().rename(columns={\n",
    "                                                            'obs_ind': 'first_obs_ind'}), on='ts_ind')\n",
    "data['obs_ind'] = data['obs_ind'] - data['first_obs_ind']\n",
    "# Find max_len.\n",
    "max_len = data.obs_ind.max()+1\n",
    "print('max_len', max_len)\n",
    "# Generate times_ip and values_ip matrices.\n",
    "times_inp = np.zeros((N, max_len), dtype='float32')\n",
    "values_inp = np.zeros((N, max_len), dtype='float32')\n",
    "varis_inp = np.zeros((N, max_len), dtype='int32')\n",
    "for row in tqdm(data.itertuples()):\n",
    "    ts_ind = row.ts_ind\n",
    "    l = row.obs_ind\n",
    "    times_inp[ts_ind, l] = row.hour\n",
    "    values_inp[ts_ind, l] = row.value\n",
    "    varis_inp[ts_ind, l] = row.vind\n",
    "\n",
    "w=0\n",
    "\n",
    "fore_in = []\n",
    "\n",
    "pred_data = data.loc[(data.hour>=0)&(data.hour<=48)]\n",
    "pred_data = pred_data.groupby(['ts_ind', 'vind']).agg({'value':'first'}).reset_index()\n",
    "pred_data['vind_value'] = pred_data[['vind', 'value']].values.tolist()\n",
    "pred_data = pred_data.groupby('ts_ind').agg({'vind_value':list}).reset_index()\n",
    "pred_data['vind_value'] = pred_data['vind_value'].apply(f)   \n",
    "\n",
    "obs_data = data.loc[(data.hour < 24) & (data.hour >= 0)]\n",
    "resultdict = dict()\n",
    "for ts_ind in obs_data.ts_ind:\n",
    "    resultdict[ts_ind] = [[]]\n",
    "obs_data = obs_data.loc[obs_data.ts_ind.isin(pred_data.ts_ind)]\n",
    "obs_data = obs_data.groupby('ts_ind').head(fore_max_len)\n",
    "obs_data = obs_data.groupby('ts_ind').agg({'vind': list, 'hour': list, 'value': list}).reset_index()\n",
    "\n",
    "for pred_window  in range(24, 48, 1):\n",
    "    print(pred_window)\n",
    "    #print(w+1 +pred_window)\n",
    "    #print(w+pred_window)\n",
    "    pred_data = data.loc[(data.hour >= w+pred_window) & (data.hour <= w+1 +pred_window)]\n",
    "    pred_data = pred_data.groupby(['ts_ind', 'vind']).agg({'value': 'first'}).reset_index()\n",
    "    pred_data['vind_value'+str(pred_window)] = pred_data[['vind', 'value']].values.tolist()\n",
    "    pred_data = pred_data.groupby('ts_ind').agg({'vind_value'+str(pred_window): list}).reset_index()\n",
    "    pred_data['vind_value'+str(pred_window)] = pred_data['vind_value'+str(pred_window)].apply(f)  # 721 entries with 2*129 vind_values\n",
    "    obs_data = obs_data.merge(pred_data, on='ts_ind')\n",
    "    #print(list(data['vind_value'+str(pred_window)]))\n",
    "    #print(len(list(data['vind_value'+str(pred_window)])))\n",
    "    #print(len(list(data['vind_value'+str(pred_window)])[0]))\n",
    "\n",
    "blub = (np.array(list([list(obs_data['vind_value'+str(pred_window)]) for pred_window in range(24, 48, 1)])))\n",
    "print(\"bleb\", blub.shape)\n",
    "op = np.swapaxes(blub, 0, 1)\n",
    "\n",
    "weird_oc = oc.loc[oc.ts_ind.isin(obs_data.ts_ind)]\n",
    "y = np.array(weird_oc.sort_values(by='ts_ind')['in_hospital_mortality']).astype('float32')\n",
    "print(y.shape)\n",
    "data.drop(columns=['obs_ind', 'first_obs_ind'], inplace=True)\n",
    "\n",
    "train_ind = [x for x in train_ind if x < op.shape[0]]\n",
    "valid_ind = [x for x in valid_ind if x < op.shape[0]]\n",
    "test_ind = [x for x in test_ind if x < op.shape[0]]\n",
    "\n",
    "train_input = op[train_ind]\n",
    "valid_input = op[valid_ind]\n",
    "test_input = op[test_ind]\n",
    "\n",
    "# Generate 3 sets of inputs and outputs.\n",
    "train_ip = [ip[train_ind] for ip in [demo, times_inp, values_inp, varis_inp]]\n",
    "valid_ip = [ip[valid_ind] for ip in [demo, times_inp, values_inp, varis_inp]]\n",
    "test_ip = [ip[test_ind] for ip in [demo, times_inp, values_inp, varis_inp]]\n",
    "del demo, times_inp, values_inp, varis_inp  # warum wird demo nicht gelöscht?\n",
    "\n",
    "if test_cond == 1:\n",
    "    tr_ind = [divmod(tr, 12)[0] for tr in train_ind]\n",
    "    va_ind = [divmod(tr, 12)[0] for tr in valid_ind]\n",
    "    te_ind = [divmod(tr, 12)[0] for tr in test_ind]\n",
    "    \n",
    "    train_op = y[tr_ind]  # is a problem for the test case...\n",
    "    valid_op = y[va_ind]\n",
    "    test_op = y[te_ind]\n",
    "else:\n",
    "    train_op = y[train_ind]  # is a problem for the test case...\n",
    "    valid_op = y[valid_ind]\n",
    "    test_op = y[test_ind]\n",
    "print('y is:', y)\n",
    "del y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subsequent-deputy",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-27T19:42:38.630290Z",
     "iopub.status.busy": "2023-09-27T19:42:38.630068Z",
     "iopub.status.idle": "2023-09-27T19:42:39.501805Z",
     "shell.execute_reply": "2023-09-27T19:42:39.500729Z"
    }
   },
   "outputs": [],
   "source": [
    "class CVE(nn.Module):\n",
    "    def __init__(self, hid_units, output_dim):\n",
    "        super(CVE, self).__init__()\n",
    "        self.hid_units = hid_units\n",
    "        self.output_dim = output_dim\n",
    "        self.W1 = nn.Parameter(torch.Tensor(1, self.hid_units))\n",
    "        self.b1 = nn.Parameter(torch.Tensor(self.hid_units))\n",
    "        self.W2 = nn.Parameter(torch.Tensor(self.hid_units, self.output_dim))\n",
    "        nn.init.xavier_uniform_(self.W1)\n",
    "        nn.init.zeros_(self.b1)\n",
    "        nn.init.xavier_uniform_(self.W2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(-1)\n",
    "        x = torch.matmul(torch.tanh(torch.add(torch.matmul(x, self.W1), self.b1)), self.W2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hid_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.hid_dim = hid_dim\n",
    "        self.W = nn.Parameter(torch.Tensor(50, self.hid_dim))\n",
    "        self.b = nn.Parameter(torch.Tensor(self.hid_dim))\n",
    "        self.u = nn.Parameter(torch.Tensor(self.hid_dim, 1))\n",
    "        nn.init.xavier_uniform_(self.W)\n",
    "        nn.init.zeros_(self.b)\n",
    "        nn.init.xavier_uniform_(self.u)\n",
    "\n",
    "    def forward(self, x, mask, mask_value=-1e30):\n",
    "        attn_weights = torch.matmul(torch.tanh(torch.add(torch.matmul(x, self.W), self.b)), self.u)\n",
    "        mask = mask.unsqueeze(-1)\n",
    "        attn_weights = mask * attn_weights + (1 - mask) * mask_value\n",
    "        attn_weights = F.softmax(attn_weights, dim=-2)\n",
    "        return attn_weights\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, N=2, h=8, dk=None, dv=None, dff=None, dropout=0, d=8):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.N, self.h, self.dk, self.dv, self.dff, self.dropout = N, h, dk, dv, dff, dropout\n",
    "        self.epsilon = torch.finfo(torch.float32).eps * torch.finfo(torch.float32).eps\n",
    "        self.Wq = nn.Parameter(torch.Tensor(N, h, d, dk))\n",
    "        self.Wk = nn.Parameter(torch.Tensor(N, h, d, dk))\n",
    "        self.Wv = nn.Parameter(torch.Tensor(N, h, d, dv))\n",
    "        self.Wo = nn.Parameter(torch.Tensor(N, dv * h, d))\n",
    "        self.W1 = nn.Parameter(torch.Tensor(N, d, dff))\n",
    "        self.b1 = nn.Parameter(torch.Tensor(N, dff))\n",
    "        self.W2 = nn.Parameter(torch.Tensor(N, dff, d))\n",
    "        self.b2 = nn.Parameter(torch.Tensor(N, d))\n",
    "        self.gamma = nn.Parameter(torch.Tensor(2 * N))\n",
    "        self.beta = nn.Parameter(torch.Tensor(2 * N))\n",
    "        nn.init.xavier_uniform_(self.Wq)\n",
    "        nn.init.xavier_uniform_(self.Wk)\n",
    "        nn.init.xavier_uniform_(self.Wv)\n",
    "        nn.init.xavier_uniform_(self.Wo)\n",
    "        nn.init.xavier_uniform_(self.W1)\n",
    "        nn.init.zeros_(self.b1)\n",
    "        nn.init.xavier_uniform_(self.W2)\n",
    "        nn.init.zeros_(self.b2)\n",
    "        nn.init.ones_(self.gamma)\n",
    "        nn.init.zeros_(self.beta)\n",
    "        self.training = False\n",
    "        \n",
    "    def forward(self, x, mask, mask_value=-1e-30):\n",
    "        mask = mask.unsqueeze(-2)\n",
    "        for i in range(self.N):\n",
    "            # MHA\n",
    "            mha_ops = []\n",
    "            for j in range(self.h):\n",
    "                q = torch.matmul(x, self.Wq[i, j, :, :])          \n",
    "                k = torch.matmul(x, self.Wk[i, j, :, :]).permute(0, 2, 1)\n",
    "                v = torch.matmul(x, self.Wv[i, j, :, :])\n",
    "                A = torch.matmul(q, k)\n",
    "                # Mask unobserved steps.\n",
    "                A = mask * A + (1 - mask) * mask_value\n",
    "                # Mask for attention dropout.\n",
    "                if self.training:\n",
    "                    dp_mask = (torch.rand(A.shape, device=A.device) >= self.dropout).float()\n",
    "                    A = A * dp_mask + (1 - dp_mask) * mask_value\n",
    "                A = F.softmax(A, dim=-1)\n",
    "                mha_ops.append(torch.matmul(A, v))\n",
    "            conc = torch.cat(mha_ops, dim=-1)\n",
    "            proj = torch.matmul(conc, self.Wo[i, :, :])\n",
    "            # Dropout.\n",
    "            if self.training:\n",
    "                proj = F.dropout(proj, p=self.dropout)\n",
    "            # Add & LN\n",
    "            x = x + proj\n",
    "            mean = x.mean(dim=-1, keepdim=True)\n",
    "            variance = torch.mean(torch.square(x - mean), dim=-1, keepdim=True)\n",
    "            std = torch.sqrt(variance + self.epsilon)\n",
    "            x = (x - mean) / std\n",
    "            x = x * self.gamma[2 * i] + self.beta[2 * i]\n",
    "            # FFN\n",
    "            ffn_op = torch.add(torch.matmul(F.relu(torch.add(\n",
    "                torch.matmul(x, self.W1[i, :, :]), self.b1[i, :])),\n",
    "                                            self.W2[i, :, :]), self.b2[i, :])\n",
    "            # Dropout.\n",
    "            if self.training:\n",
    "                ffn_op = F.dropout(ffn_op, p=self.dropout)\n",
    "            # Add & LN\n",
    "            x = x + ffn_op\n",
    "            mean = x.mean(dim=-1, keepdim=True)\n",
    "            variance = torch.mean(torch.square(x - mean), dim=-1, keepdim=True)\n",
    "            std = torch.sqrt(variance + self.epsilon)\n",
    "            x = (x - mean) / std\n",
    "            x = x * self.gamma[2 * i + 1] + self.beta[2 * i + 1]\n",
    "        return x\n",
    "\n",
    "\n",
    "class StratsModel(nn.Module):\n",
    "    def __init__(self, D, max_len, V, d, N, he, dropout, forecast=False, autoregressive=True):\n",
    "        super(StratsModel, self).__init__()\n",
    "        self.forecast = forecast\n",
    "        self.autoregressive=autoregressive\n",
    "        self.demo_enc = nn.Sequential(nn.Linear(D, 2 * d),\n",
    "                                      nn.Tanh(),\n",
    "                                      nn.Linear(2 * d, d),\n",
    "                                      nn.Tanh())\n",
    "        self.varis_emb = nn.Embedding(V + 1, d)\n",
    "        nn.init.uniform_(self.varis_emb.weight, a=-0.05, b=0.05)\n",
    "        self.values_emb = CVE(int(np.sqrt(d)), d)\n",
    "        self.times_emb = CVE(int(np.sqrt(d)), d)\n",
    "        # self.comb_emb = nn.LayerNorm(d)\n",
    "        self.transformer = Transformer(N, he, dk=d//he, dv=d//he, dff=2*d, dropout=dropout, d=d)\n",
    "\n",
    "        if autoregressive:\n",
    "            self.linearize = nn.Linear(50, 131)\n",
    "            self.dec = nn.TransformerDecoderLayer(d_model=131, dim_feedforward=131, nhead=1, batch_first=True)\n",
    "        else:\n",
    "            self.attention = Attention(2 * d)\n",
    "            self.attentions = nn.ModuleList([Attention(2*d) for i in range(24)])\n",
    "            # self.concat = nn.Linear(2 * d, 1)\n",
    "            self.fore_op_calc = nn.Linear(2 * d, V)\n",
    "            self.op_calc = nn.Linear(V, 1)\n",
    "            self.sig = nn.Sigmoid()        \n",
    "        \n",
    "\n",
    "    def forward(self, demo, times, values, varis, trainn=False, tgt=torch.zeros(32, 1, 131).cuda()):\n",
    "        demo_enc = self.demo_enc(demo)\n",
    "        varis_emb = self.varis_emb(varis)\n",
    "        values_emb = self.values_emb(values)\n",
    "        times_emb = self.times_emb(times)\n",
    "\n",
    "        comb_emb = (torch.sum(torch.stack([varis_emb, values_emb, times_emb]), dim=0))\n",
    "        self.trainn = trainn\n",
    "        mask = torch.clip(varis, 0, 1)\n",
    "        cont_emb = self.transformer(comb_emb, mask)\n",
    "        \n",
    "        \n",
    "        \n",
    "        if self.autoregressive:\n",
    "            if self.trainn:\n",
    "                #print(cont_emb.size())\n",
    "                \n",
    "                mask_new = torch.nn.Transformer.generate_square_subsequent_mask(25).cuda()\n",
    "                linear_memory = self.linearize(cont_emb)\n",
    "                start_input = torch.zeros(32, 1, 131).cuda()\n",
    "                tgt = torch.concat([start_input, tgt], dim=1)\n",
    "                result=self.dec(tgt, linear_memory, mask_new)\n",
    "                return result[:, 1:, :]\n",
    "            if not self.trainn:\n",
    "                output = torch.zeros(32, 1, 131).cuda()\n",
    "                linear_memory  = self.linearize(cont_emb)\n",
    "                for i in range(24):\n",
    "                    #print(i)\n",
    "                    #print(linear_memory.size(), output.size())\n",
    "                    res = self.dec(output.detach(), linear_memory)\n",
    "                    #print(res.size(), output.size())\n",
    "                    output = torch.concat([output, res[:,-1:,:]], dim=1)\n",
    "                return output[:, 1:, :]\n",
    "                \n",
    "        if not self.autoregressive:\n",
    "            fused_embs = []\n",
    "            for attention in self.attentions:\n",
    "                attn_weights = attention(cont_emb, mask)\n",
    "                fused_emb = torch.matmul(attn_weights.transpose(-1, -2), cont_emb).squeeze(-2)\n",
    "                # fused_emb torch.Size([32, 50])\n",
    "                fused_embs.append(fused_emb)\n",
    "            # fused_embs Liste mit 6 torch.Size([32, 50])\n",
    "            fused_emb = torch.stack(fused_embs, axis=1)\n",
    "            # altes fused_emb: torch.Size([32, 50])\n",
    "            # neues fused_emb: torch.Size([32, 6, 50])\n",
    "            # demo_enc torch.Size([32, 50])\n",
    "\n",
    "            # demo_enc.unsqueeze(axis=-2).repeat(1, 6, 1):  torch.Size([32, 6, 50])\n",
    "            #print(fused_emb.size())\n",
    "            concat = torch.cat([fused_emb, demo_enc.unsqueeze(axis=-2).repeat(1, 24, 1)], axis=-1)\n",
    "            #concat torch.Size([32, 6, 100])\n",
    "\n",
    "            x = self.fore_op_calc(concat)\n",
    "            # neues x torch.Size([32, 6, 258])\n",
    "            # altes x: torch.Size([32, 258])\n",
    "            if self.forecast:\n",
    "                return x\n",
    "            else:\n",
    "                x = self.sig(self.op_calc(x)).mean(axis=-2)\n",
    "                return x\n",
    "\n",
    "print(\"Model definitions loaded\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quick-fetish",
   "metadata": {},
   "source": [
    "## Pretrain on forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "temporal-grain",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-27T19:42:39.505399Z",
     "iopub.status.busy": "2023-09-27T19:42:39.505096Z",
     "iopub.status.idle": "2023-09-28T19:17:41.431025Z",
     "shell.execute_reply": "2023-09-28T19:17:41.428879Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lr, batch_size, samples_per_epoch, patience = 0.0005, 32, int(102400/sample_divisor), 6\n",
    "d, N, he, dropout = 50, 2, 4, 0.2\n",
    "print('number of parameters: ', V)\n",
    "\n",
    "model = StratsModel(2, fore_max_len, V, d, N, he, dropout, forecast=True).cuda()\n",
    "a = summary(model, [(32, 2), (32, 880), (32, 880), (32, 880)],  # shape of fore_train_ip\n",
    "            dtypes=[torch.float, torch.float, torch.float, torch.long])\n",
    "print(a)  # Model summary\n",
    "# raise Exception\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=3)\n",
    "# Pretrain fore_model.\n",
    "best_val_loss = np.inf\n",
    "N_fore = len(fore_train_op)\n",
    "#print(N_fore)\n",
    "fore_savepath = 'mimic_iii_sparse_ims_sampling_detach.pytorch'\n",
    "loss_func = torch.nn.MSELoss(reduction=\"none\")\n",
    "\n",
    "# torch.compile(model)\n",
    "for e in range(number_of_epochs):\n",
    "    e_indices = np.random.choice(range(N_fore), size=samples_per_epoch, replace=False)\n",
    "    e_loss = 0\n",
    "    pbar = tqdm(range(0, len(e_indices), batch_size))\n",
    "    model.train()\n",
    "    for start in pbar:\n",
    "        ind = e_indices[start:start+batch_size]\n",
    "        # raise Exception\n",
    "\n",
    "        output = model(*[torch.tensor(ip[ind], dtype=torch.float32).cuda() if i < 3 \n",
    "                         else torch.tensor(ip[ind], dtype=torch.int32).cuda() \n",
    "                         for i, ip in enumerate(fore_train_ip)], trainn=False, tgt=torch.tensor(fore_train_op[ind, :, :131], dtype=torch.float32).cuda())\n",
    "        #raise Exception\n",
    "        #print(fore_train_op[ind].shape)\n",
    "        #raise Exception\n",
    "        loss = torch.tensor(fore_train_op[ind, :, 131:], dtype=torch.float32).cuda()*(\n",
    "            output-torch.tensor(fore_train_op[ind, :, :131], dtype=torch.float32).cuda())**2\n",
    "        loss = loss.sum(axis=-1).mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        e_loss += loss.detach()\n",
    "        pbar.set_description('%f' % e_loss)\n",
    "    val_loss = 0\n",
    "    model.eval()\n",
    "    loss_list = []\n",
    "    pbar = tqdm(range(0, len(fore_valid_op)-32, 32))  # len(fore_valid_op)           ####################   maybe also batch_size instead of 32\n",
    "    for start in pbar:\n",
    "        #print(start, start+batch_size)\n",
    "        output = model(*[torch.tensor(ip[start:start+batch_size], dtype=torch.float32).cuda() \n",
    "                         if i < 3 else torch.tensor(ip[start:start+batch_size], dtype=torch.int32).cuda() \n",
    "                         for i, ip in enumerate(fore_valid_ip)], trainn=False, tgt=torch.tensor(fore_valid_op[start:start+batch_size], dtype=torch.float32).cuda())\n",
    "        loss = torch.tensor(fore_valid_op[start:start+batch_size,:, 131:], dtype=torch.float32).cuda()*loss_func(\n",
    "            output, torch.tensor(fore_valid_op[start:start+batch_size,:, :131], dtype=torch.float32).cuda())\n",
    "        loss_list.extend(loss.sum(axis=-1).mean(axis=-1).detach().cpu().tolist())\n",
    "        loss = loss.sum(axis=-1).mean()\n",
    "        val_loss += loss.detach().cpu()\n",
    "        pbar.set_description('%f' % val_loss)\n",
    "    loss_p = e_loss*batch_size/samples_per_epoch\n",
    "    val_loss_p = val_loss*batch_size/len(fore_valid_op)\n",
    "    print('Epoch', e, 'loss', loss_p, 'val loss', val_loss_p, \"mean and std\", np.mean(loss_list), np.std(loss_list))\n",
    "    with open('loss_values_log', 'a') as f:\n",
    "        f.write(str(e)+' ' + str(loss_p.item()) + ' ' + str(val_loss_p.item())+ '\\n')\n",
    "    scheduler.step(val_loss_p.item())\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), fore_savepath)\n",
    "        best_epoch = e\n",
    "    if (e-best_epoch) > patience:\n",
    "        break\n",
    "print('Training has ended.')\n",
    "\n",
    "#STRATS, IMS, Sampling, Detach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82bbe9c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-28T19:17:41.437213Z",
     "iopub.status.busy": "2023-09-28T19:17:41.436888Z",
     "iopub.status.idle": "2023-09-29T15:51:31.450556Z",
     "shell.execute_reply": "2023-09-29T15:51:31.448074Z"
    }
   },
   "outputs": [],
   "source": [
    "class CVE(nn.Module):\n",
    "    def __init__(self, hid_units, output_dim):\n",
    "        super(CVE, self).__init__()\n",
    "        self.hid_units = hid_units\n",
    "        self.output_dim = output_dim\n",
    "        self.W1 = nn.Parameter(torch.Tensor(1, self.hid_units))\n",
    "        self.b1 = nn.Parameter(torch.Tensor(self.hid_units))\n",
    "        self.W2 = nn.Parameter(torch.Tensor(self.hid_units, self.output_dim))\n",
    "        nn.init.xavier_uniform_(self.W1)\n",
    "        nn.init.zeros_(self.b1)\n",
    "        nn.init.xavier_uniform_(self.W2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(-1)\n",
    "        x = torch.matmul(torch.tanh(torch.add(torch.matmul(x, self.W1), self.b1)), self.W2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hid_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.hid_dim = hid_dim\n",
    "        self.W = nn.Parameter(torch.Tensor(50, self.hid_dim))\n",
    "        self.b = nn.Parameter(torch.Tensor(self.hid_dim))\n",
    "        self.u = nn.Parameter(torch.Tensor(self.hid_dim, 1))\n",
    "        nn.init.xavier_uniform_(self.W)\n",
    "        nn.init.zeros_(self.b)\n",
    "        nn.init.xavier_uniform_(self.u)\n",
    "\n",
    "    def forward(self, x, mask, mask_value=-1e30):\n",
    "        attn_weights = torch.matmul(torch.tanh(torch.add(torch.matmul(x, self.W), self.b)), self.u)\n",
    "        mask = mask.unsqueeze(-1)\n",
    "        attn_weights = mask * attn_weights + (1 - mask) * mask_value\n",
    "        attn_weights = F.softmax(attn_weights, dim=-2)\n",
    "        return attn_weights\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, N=2, h=8, dk=None, dv=None, dff=None, dropout=0, d=8):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.N, self.h, self.dk, self.dv, self.dff, self.dropout = N, h, dk, dv, dff, dropout\n",
    "        self.epsilon = torch.finfo(torch.float32).eps * torch.finfo(torch.float32).eps\n",
    "        self.Wq = nn.Parameter(torch.Tensor(N, h, d, dk))\n",
    "        self.Wk = nn.Parameter(torch.Tensor(N, h, d, dk))\n",
    "        self.Wv = nn.Parameter(torch.Tensor(N, h, d, dv))\n",
    "        self.Wo = nn.Parameter(torch.Tensor(N, dv * h, d))\n",
    "        self.W1 = nn.Parameter(torch.Tensor(N, d, dff))\n",
    "        self.b1 = nn.Parameter(torch.Tensor(N, dff))\n",
    "        self.W2 = nn.Parameter(torch.Tensor(N, dff, d))\n",
    "        self.b2 = nn.Parameter(torch.Tensor(N, d))\n",
    "        self.gamma = nn.Parameter(torch.Tensor(2 * N))\n",
    "        self.beta = nn.Parameter(torch.Tensor(2 * N))\n",
    "        nn.init.xavier_uniform_(self.Wq)\n",
    "        nn.init.xavier_uniform_(self.Wk)\n",
    "        nn.init.xavier_uniform_(self.Wv)\n",
    "        nn.init.xavier_uniform_(self.Wo)\n",
    "        nn.init.xavier_uniform_(self.W1)\n",
    "        nn.init.zeros_(self.b1)\n",
    "        nn.init.xavier_uniform_(self.W2)\n",
    "        nn.init.zeros_(self.b2)\n",
    "        nn.init.ones_(self.gamma)\n",
    "        nn.init.zeros_(self.beta)\n",
    "        self.training = False\n",
    "        \n",
    "    def forward(self, x, mask, mask_value=-1e-30):\n",
    "        mask = mask.unsqueeze(-2)\n",
    "        for i in range(self.N):\n",
    "            # MHA\n",
    "            mha_ops = []\n",
    "            for j in range(self.h):\n",
    "                q = torch.matmul(x, self.Wq[i, j, :, :])          \n",
    "                k = torch.matmul(x, self.Wk[i, j, :, :]).permute(0, 2, 1)\n",
    "                v = torch.matmul(x, self.Wv[i, j, :, :])\n",
    "                A = torch.matmul(q, k)\n",
    "                # Mask unobserved steps.\n",
    "                A = mask * A + (1 - mask) * mask_value\n",
    "                # Mask for attention dropout.\n",
    "                if self.training:\n",
    "                    dp_mask = (torch.rand(A.shape, device=A.device) >= self.dropout).float()\n",
    "                    A = A * dp_mask + (1 - dp_mask) * mask_value\n",
    "                A = F.softmax(A, dim=-1)\n",
    "                mha_ops.append(torch.matmul(A, v))\n",
    "            conc = torch.cat(mha_ops, dim=-1)\n",
    "            proj = torch.matmul(conc, self.Wo[i, :, :])\n",
    "            # Dropout.\n",
    "            if self.training:\n",
    "                proj = F.dropout(proj, p=self.dropout)\n",
    "            # Add & LN\n",
    "            x = x + proj\n",
    "            mean = x.mean(dim=-1, keepdim=True)\n",
    "            variance = torch.mean(torch.square(x - mean), dim=-1, keepdim=True)\n",
    "            std = torch.sqrt(variance + self.epsilon)\n",
    "            x = (x - mean) / std\n",
    "            x = x * self.gamma[2 * i] + self.beta[2 * i]\n",
    "            # FFN\n",
    "            ffn_op = torch.add(torch.matmul(F.relu(torch.add(\n",
    "                torch.matmul(x, self.W1[i, :, :]), self.b1[i, :])),\n",
    "                                            self.W2[i, :, :]), self.b2[i, :])\n",
    "            # Dropout.\n",
    "            if self.training:\n",
    "                ffn_op = F.dropout(ffn_op, p=self.dropout)\n",
    "            # Add & LN\n",
    "            x = x + ffn_op\n",
    "            mean = x.mean(dim=-1, keepdim=True)\n",
    "            variance = torch.mean(torch.square(x - mean), dim=-1, keepdim=True)\n",
    "            std = torch.sqrt(variance + self.epsilon)\n",
    "            x = (x - mean) / std\n",
    "            x = x * self.gamma[2 * i + 1] + self.beta[2 * i + 1]\n",
    "        return x\n",
    "\n",
    "\n",
    "class StratsModel(nn.Module):\n",
    "    def __init__(self, D, max_len, V, d, N, he, dropout, forecast=False, autoregressive=True):\n",
    "        super(StratsModel, self).__init__()\n",
    "        self.forecast = forecast\n",
    "        self.autoregressive=autoregressive\n",
    "        self.demo_enc = nn.Sequential(nn.Linear(D, 2 * d),\n",
    "                                      nn.Tanh(),\n",
    "                                      nn.Linear(2 * d, d),\n",
    "                                      nn.Tanh())\n",
    "        self.varis_emb = nn.Embedding(V + 1, d)\n",
    "        nn.init.uniform_(self.varis_emb.weight, a=-0.05, b=0.05)\n",
    "        self.values_emb = CVE(int(np.sqrt(d)), d)\n",
    "        self.times_emb = CVE(int(np.sqrt(d)), d)\n",
    "        # self.comb_emb = nn.LayerNorm(d)\n",
    "        self.transformer = Transformer(N, he, dk=d//he, dv=d//he, dff=2*d, dropout=dropout, d=d)\n",
    "\n",
    "        if autoregressive:\n",
    "            self.linearize = nn.Linear(50, 131)\n",
    "            self.dec = nn.TransformerDecoderLayer(d_model=131, dim_feedforward=131, nhead=1, batch_first=True)\n",
    "        else:\n",
    "            self.attention = Attention(2 * d)\n",
    "            self.attentions = nn.ModuleList([Attention(2*d) for i in range(24)])\n",
    "            # self.concat = nn.Linear(2 * d, 1)\n",
    "            self.fore_op_calc = nn.Linear(2 * d, V)\n",
    "            self.op_calc = nn.Linear(V, 1)\n",
    "            self.sig = nn.Sigmoid()        \n",
    "        \n",
    "\n",
    "    def forward(self, demo, times, values, varis, trainn=False, tgt=torch.zeros(32, 1, 131).cuda()):\n",
    "        demo_enc = self.demo_enc(demo)\n",
    "        varis_emb = self.varis_emb(varis)\n",
    "        values_emb = self.values_emb(values)\n",
    "        times_emb = self.times_emb(times)\n",
    "\n",
    "        comb_emb = (torch.sum(torch.stack([varis_emb, values_emb, times_emb]), dim=0))\n",
    "        self.trainn = trainn\n",
    "        mask = torch.clip(varis, 0, 1)\n",
    "        cont_emb = self.transformer(comb_emb, mask)\n",
    "        \n",
    "        \n",
    "        \n",
    "        if self.autoregressive:\n",
    "            if self.trainn:\n",
    "                #print(cont_emb.size())\n",
    "                \n",
    "                mask_new = torch.nn.Transformer.generate_square_subsequent_mask(25).cuda()\n",
    "                linear_memory = self.linearize(cont_emb)\n",
    "                start_input = torch.zeros(32, 1, 131).cuda()\n",
    "                tgt = torch.concat([start_input, tgt], dim=1)\n",
    "                result=self.dec(tgt, linear_memory, mask_new)\n",
    "                return result[:, 1:, :]\n",
    "            if not self.trainn:\n",
    "                output = torch.zeros(32, 1, 131).cuda()\n",
    "                linear_memory  = self.linearize(cont_emb)\n",
    "                for i in range(24):\n",
    "                    #print(i)\n",
    "                    #print(linear_memory.size(), output.size())\n",
    "                    res = self.dec(output, linear_memory)\n",
    "                    #print(res.size(), output.size())\n",
    "                    output = torch.concat([output, res[:,-1:,:]], dim=1)\n",
    "                return output[:, 1:, :]\n",
    "                \n",
    "        if not self.autoregressive:\n",
    "            fused_embs = []\n",
    "            for attention in self.attentions:\n",
    "                attn_weights = attention(cont_emb, mask)\n",
    "                fused_emb = torch.matmul(attn_weights.transpose(-1, -2), cont_emb).squeeze(-2)\n",
    "                # fused_emb torch.Size([32, 50])\n",
    "                fused_embs.append(fused_emb)\n",
    "            # fused_embs Liste mit 6 torch.Size([32, 50])\n",
    "            fused_emb = torch.stack(fused_embs, axis=1)\n",
    "            # altes fused_emb: torch.Size([32, 50])\n",
    "            # neues fused_emb: torch.Size([32, 6, 50])\n",
    "            # demo_enc torch.Size([32, 50])\n",
    "\n",
    "            # demo_enc.unsqueeze(axis=-2).repeat(1, 6, 1):  torch.Size([32, 6, 50])\n",
    "            #print(fused_emb.size())\n",
    "            concat = torch.cat([fused_emb, demo_enc.unsqueeze(axis=-2).repeat(1, 24, 1)], axis=-1)\n",
    "            #concat torch.Size([32, 6, 100])\n",
    "\n",
    "            x = self.fore_op_calc(concat)\n",
    "            # neues x torch.Size([32, 6, 258])\n",
    "            # altes x: torch.Size([32, 258])\n",
    "            if self.forecast:\n",
    "                return x\n",
    "            else:\n",
    "                x = self.sig(self.op_calc(x)).mean(axis=-2)\n",
    "                return x\n",
    "\n",
    "print(\"Model definitions loaded\")\n",
    "\n",
    "lr, batch_size, samples_per_epoch, patience = 0.0005, 32, int(102400/sample_divisor), 6\n",
    "d, N, he, dropout = 50, 2, 4, 0.2\n",
    "print('number of parameters: ', V)\n",
    "\n",
    "model = StratsModel(2, fore_max_len, V, d, N, he, dropout, forecast=True).cuda()\n",
    "a = summary(model, [(32, 2), (32, 880), (32, 880), (32, 880)],  # shape of fore_train_ip\n",
    "            dtypes=[torch.float, torch.float, torch.float, torch.long])\n",
    "print(a)  # Model summary\n",
    "# raise Exception\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=3)\n",
    "# Pretrain fore_model.\n",
    "best_val_loss = np.inf\n",
    "N_fore = len(fore_train_op)\n",
    "#print(N_fore)\n",
    "fore_savepath = 'mimic_iii_sparse_ims_sampling_backprop.pytorch'\n",
    "loss_func = torch.nn.MSELoss(reduction=\"none\")\n",
    "\n",
    "# torch.compile(model)\n",
    "for e in range(number_of_epochs):\n",
    "    e_indices = np.random.choice(range(N_fore), size=samples_per_epoch, replace=False)\n",
    "    e_loss = 0\n",
    "    pbar = tqdm(range(0, len(e_indices), batch_size))\n",
    "    model.train()\n",
    "    for start in pbar:\n",
    "        ind = e_indices[start:start+batch_size]\n",
    "        # raise Exception\n",
    "\n",
    "        output = model(*[torch.tensor(ip[ind], dtype=torch.float32).cuda() if i < 3 \n",
    "                         else torch.tensor(ip[ind], dtype=torch.int32).cuda() \n",
    "                         for i, ip in enumerate(fore_train_ip)], trainn=False, tgt=torch.tensor(fore_train_op[ind, :, :131], dtype=torch.float32).cuda())\n",
    "        #raise Exception\n",
    "        #print(fore_train_op[ind].shape)\n",
    "        #raise Exception\n",
    "        loss = torch.tensor(fore_train_op[ind, :, 131:], dtype=torch.float32).cuda()*(\n",
    "            output-torch.tensor(fore_train_op[ind, :, :131], dtype=torch.float32).cuda())**2\n",
    "        loss = loss.sum(axis=-1).mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        e_loss += loss.detach()\n",
    "        pbar.set_description('%f' % e_loss)\n",
    "    val_loss = 0\n",
    "    model.eval()\n",
    "    loss_list = []\n",
    "    pbar = tqdm(range(0, len(fore_valid_op)-32, 32))  # len(fore_valid_op)           ####################   maybe also batch_size instead of 32\n",
    "    for start in pbar:\n",
    "        #print(start, start+batch_size)\n",
    "        output = model(*[torch.tensor(ip[start:start+batch_size], dtype=torch.float32).cuda() \n",
    "                         if i < 3 else torch.tensor(ip[start:start+batch_size], dtype=torch.int32).cuda() \n",
    "                         for i, ip in enumerate(fore_valid_ip)], trainn=False, tgt=torch.tensor(fore_valid_op[start:start+batch_size], dtype=torch.float32).cuda())\n",
    "        loss = torch.tensor(fore_valid_op[start:start+batch_size,:, 131:], dtype=torch.float32).cuda()*loss_func(\n",
    "            output, torch.tensor(fore_valid_op[start:start+batch_size,:, :131], dtype=torch.float32).cuda())\n",
    "        loss_list.extend(loss.sum(axis=-1).mean(axis=-1).detach().cpu().tolist())\n",
    "        loss = loss.sum(axis=-1).mean()\n",
    "        val_loss += loss.detach().cpu()\n",
    "        pbar.set_description('%f' % val_loss)\n",
    "    loss_p = e_loss*batch_size/samples_per_epoch\n",
    "    val_loss_p = val_loss*batch_size/len(fore_valid_op)\n",
    "    print('Epoch', e, 'loss', loss_p, 'val loss', val_loss_p)\n",
    "    with open('loss_values_log', 'a') as f:\n",
    "        f.write(str(e)+' ' + str(loss_p.item()) + ' ' + str(val_loss_p.item())+ '\\n')\n",
    "    scheduler.step(val_loss_p.item())\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), fore_savepath)\n",
    "        best_epoch = e\n",
    "    if (e-best_epoch) > patience:\n",
    "        break\n",
    "print('Training has ended.')\n",
    "#STRATS, IMS, Sampling, No Detach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303701d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-29T15:51:31.455322Z",
     "iopub.status.busy": "2023-09-29T15:51:31.454968Z",
     "iopub.status.idle": "2023-09-29T17:31:04.717761Z",
     "shell.execute_reply": "2023-09-29T17:31:04.715622Z"
    }
   },
   "outputs": [],
   "source": [
    "class CVE(nn.Module):\n",
    "    def __init__(self, hid_units, output_dim):\n",
    "        super(CVE, self).__init__()\n",
    "        self.hid_units = hid_units\n",
    "        self.output_dim = output_dim\n",
    "        self.W1 = nn.Parameter(torch.Tensor(1, self.hid_units))\n",
    "        self.b1 = nn.Parameter(torch.Tensor(self.hid_units))\n",
    "        self.W2 = nn.Parameter(torch.Tensor(self.hid_units, self.output_dim))\n",
    "        nn.init.xavier_uniform_(self.W1)\n",
    "        nn.init.zeros_(self.b1)\n",
    "        nn.init.xavier_uniform_(self.W2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(-1)\n",
    "        x = torch.matmul(torch.tanh(torch.add(torch.matmul(x, self.W1), self.b1)), self.W2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hid_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.hid_dim = hid_dim\n",
    "        self.W = nn.Parameter(torch.Tensor(50, self.hid_dim))\n",
    "        self.b = nn.Parameter(torch.Tensor(self.hid_dim))\n",
    "        self.u = nn.Parameter(torch.Tensor(self.hid_dim, 1))\n",
    "        nn.init.xavier_uniform_(self.W)\n",
    "        nn.init.zeros_(self.b)\n",
    "        nn.init.xavier_uniform_(self.u)\n",
    "\n",
    "    def forward(self, x, mask, mask_value=-1e30):\n",
    "        attn_weights = torch.matmul(torch.tanh(torch.add(torch.matmul(x, self.W), self.b)), self.u)\n",
    "        mask = mask.unsqueeze(-1)\n",
    "        attn_weights = mask * attn_weights + (1 - mask) * mask_value\n",
    "        attn_weights = F.softmax(attn_weights, dim=-2)\n",
    "        return attn_weights\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, N=2, h=8, dk=None, dv=None, dff=None, dropout=0, d=8):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.N, self.h, self.dk, self.dv, self.dff, self.dropout = N, h, dk, dv, dff, dropout\n",
    "        self.epsilon = torch.finfo(torch.float32).eps * torch.finfo(torch.float32).eps\n",
    "        self.Wq = nn.Parameter(torch.Tensor(N, h, d, dk))\n",
    "        self.Wk = nn.Parameter(torch.Tensor(N, h, d, dk))\n",
    "        self.Wv = nn.Parameter(torch.Tensor(N, h, d, dv))\n",
    "        self.Wo = nn.Parameter(torch.Tensor(N, dv * h, d))\n",
    "        self.W1 = nn.Parameter(torch.Tensor(N, d, dff))\n",
    "        self.b1 = nn.Parameter(torch.Tensor(N, dff))\n",
    "        self.W2 = nn.Parameter(torch.Tensor(N, dff, d))\n",
    "        self.b2 = nn.Parameter(torch.Tensor(N, d))\n",
    "        self.gamma = nn.Parameter(torch.Tensor(2 * N))\n",
    "        self.beta = nn.Parameter(torch.Tensor(2 * N))\n",
    "        nn.init.xavier_uniform_(self.Wq)\n",
    "        nn.init.xavier_uniform_(self.Wk)\n",
    "        nn.init.xavier_uniform_(self.Wv)\n",
    "        nn.init.xavier_uniform_(self.Wo)\n",
    "        nn.init.xavier_uniform_(self.W1)\n",
    "        nn.init.zeros_(self.b1)\n",
    "        nn.init.xavier_uniform_(self.W2)\n",
    "        nn.init.zeros_(self.b2)\n",
    "        nn.init.ones_(self.gamma)\n",
    "        nn.init.zeros_(self.beta)\n",
    "        self.training = False\n",
    "        \n",
    "    def forward(self, x, mask, mask_value=-1e-30):\n",
    "        mask = mask.unsqueeze(-2)\n",
    "        for i in range(self.N):\n",
    "            # MHA\n",
    "            mha_ops = []\n",
    "            for j in range(self.h):\n",
    "                q = torch.matmul(x, self.Wq[i, j, :, :])          \n",
    "                k = torch.matmul(x, self.Wk[i, j, :, :]).permute(0, 2, 1)\n",
    "                v = torch.matmul(x, self.Wv[i, j, :, :])\n",
    "                A = torch.matmul(q, k)\n",
    "                # Mask unobserved steps.\n",
    "                A = mask * A + (1 - mask) * mask_value\n",
    "                # Mask for attention dropout.\n",
    "                if self.training:\n",
    "                    dp_mask = (torch.rand(A.shape, device=A.device) >= self.dropout).float()\n",
    "                    A = A * dp_mask + (1 - dp_mask) * mask_value\n",
    "                A = F.softmax(A, dim=-1)\n",
    "                mha_ops.append(torch.matmul(A, v))\n",
    "            conc = torch.cat(mha_ops, dim=-1)\n",
    "            proj = torch.matmul(conc, self.Wo[i, :, :])\n",
    "            # Dropout.\n",
    "            if self.training:\n",
    "                proj = F.dropout(proj, p=self.dropout)\n",
    "            # Add & LN\n",
    "            x = x + proj\n",
    "            mean = x.mean(dim=-1, keepdim=True)\n",
    "            variance = torch.mean(torch.square(x - mean), dim=-1, keepdim=True)\n",
    "            std = torch.sqrt(variance + self.epsilon)\n",
    "            x = (x - mean) / std\n",
    "            x = x * self.gamma[2 * i] + self.beta[2 * i]\n",
    "            # FFN\n",
    "            ffn_op = torch.add(torch.matmul(F.relu(torch.add(\n",
    "                torch.matmul(x, self.W1[i, :, :]), self.b1[i, :])),\n",
    "                                            self.W2[i, :, :]), self.b2[i, :])\n",
    "            # Dropout.\n",
    "            if self.training:\n",
    "                ffn_op = F.dropout(ffn_op, p=self.dropout)\n",
    "            # Add & LN\n",
    "            x = x + ffn_op\n",
    "            mean = x.mean(dim=-1, keepdim=True)\n",
    "            variance = torch.mean(torch.square(x - mean), dim=-1, keepdim=True)\n",
    "            std = torch.sqrt(variance + self.epsilon)\n",
    "            x = (x - mean) / std\n",
    "            x = x * self.gamma[2 * i + 1] + self.beta[2 * i + 1]\n",
    "        return x\n",
    "\n",
    "\n",
    "class StratsModel(nn.Module):\n",
    "    def __init__(self, D, max_len, V, d, N, he, dropout, forecast=False, autoregressive=True):\n",
    "        super(StratsModel, self).__init__()\n",
    "        self.forecast = forecast\n",
    "        self.autoregressive=autoregressive\n",
    "        self.demo_enc = nn.Sequential(nn.Linear(D, 2 * d),\n",
    "                                      nn.Tanh(),\n",
    "                                      nn.Linear(2 * d, d),\n",
    "                                      nn.Tanh())\n",
    "        self.varis_emb = nn.Embedding(V + 1, d)\n",
    "        nn.init.uniform_(self.varis_emb.weight, a=-0.05, b=0.05)\n",
    "        self.values_emb = CVE(int(np.sqrt(d)), d)\n",
    "        self.times_emb = CVE(int(np.sqrt(d)), d)\n",
    "        # self.comb_emb = nn.LayerNorm(d)\n",
    "        self.transformer = Transformer(N, he, dk=d//he, dv=d//he, dff=2*d, dropout=dropout, d=d)\n",
    "\n",
    "        if autoregressive:\n",
    "            self.linearize = nn.Linear(50, 131)\n",
    "            self.dec = nn.TransformerDecoderLayer(d_model=131, dim_feedforward=131, nhead=1, batch_first=True)\n",
    "        else:\n",
    "            self.attention = Attention(2 * d)\n",
    "            self.attentions = nn.ModuleList([Attention(2*d) for i in range(24)])\n",
    "            # self.concat = nn.Linear(2 * d, 1)\n",
    "            self.fore_op_calc = nn.Linear(2 * d, V)\n",
    "            self.op_calc = nn.Linear(V, 1)\n",
    "            self.sig = nn.Sigmoid()        \n",
    "        \n",
    "\n",
    "    def forward(self, demo, times, values, varis, trainn=False, tgt=torch.zeros(32, 1, 131).cuda()):\n",
    "        demo_enc = self.demo_enc(demo)\n",
    "        varis_emb = self.varis_emb(varis)\n",
    "        values_emb = self.values_emb(values)\n",
    "        times_emb = self.times_emb(times)\n",
    "\n",
    "        comb_emb = (torch.sum(torch.stack([varis_emb, values_emb, times_emb]), dim=0))\n",
    "        self.trainn = trainn\n",
    "        mask = torch.clip(varis, 0, 1)\n",
    "        cont_emb = self.transformer(comb_emb, mask)\n",
    "        \n",
    "        \n",
    "        \n",
    "        if self.autoregressive:\n",
    "            if self.trainn:\n",
    "                #print(cont_emb.size())\n",
    "                \n",
    "                mask_new = torch.nn.Transformer.generate_square_subsequent_mask(25).cuda()\n",
    "                linear_memory = self.linearize(cont_emb)\n",
    "                start_input = torch.zeros(32, 1, 131).cuda()\n",
    "                tgt = torch.concat([start_input, tgt], dim=1)\n",
    "                result=self.dec(tgt, linear_memory, mask_new)\n",
    "                return result[:, 1:, :]\n",
    "            if not self.trainn:\n",
    "                output = torch.zeros(32, 1, 131).cuda()\n",
    "                linear_memory  = self.linearize(cont_emb)\n",
    "                for i in range(24):\n",
    "                    #print(i)\n",
    "                    #print(linear_memory.size(), output.size())\n",
    "                    res = self.dec(output.detach(), linear_memory)\n",
    "                    #print(res.size(), output.size())\n",
    "                    output = torch.concat([output, res[:,-1:,:]], dim=1)\n",
    "                return output[:, 1:, :]\n",
    "                \n",
    "        if not self.autoregressive:\n",
    "            fused_embs = []\n",
    "            for attention in self.attentions:\n",
    "                attn_weights = attention(cont_emb, mask)\n",
    "                fused_emb = torch.matmul(attn_weights.transpose(-1, -2), cont_emb).squeeze(-2)\n",
    "                # fused_emb torch.Size([32, 50])\n",
    "                fused_embs.append(fused_emb)\n",
    "            # fused_embs Liste mit 6 torch.Size([32, 50])\n",
    "            fused_emb = torch.stack(fused_embs, axis=1)\n",
    "            # altes fused_emb: torch.Size([32, 50])\n",
    "            # neues fused_emb: torch.Size([32, 6, 50])\n",
    "            # demo_enc torch.Size([32, 50])\n",
    "\n",
    "            # demo_enc.unsqueeze(axis=-2).repeat(1, 6, 1):  torch.Size([32, 6, 50])\n",
    "            #print(fused_emb.size())\n",
    "            concat = torch.cat([fused_emb, demo_enc.unsqueeze(axis=-2).repeat(1, 24, 1)], axis=-1)\n",
    "            #concat torch.Size([32, 6, 100])\n",
    "\n",
    "            x = self.fore_op_calc(concat)\n",
    "            # neues x torch.Size([32, 6, 258])\n",
    "            # altes x: torch.Size([32, 258])\n",
    "            if self.forecast:\n",
    "                return x\n",
    "            else:\n",
    "                x = self.sig(self.op_calc(x)).mean(axis=-2)\n",
    "                return x\n",
    "\n",
    "print(\"Model definitions loaded\")\n",
    "\n",
    "lr, batch_size, samples_per_epoch, patience = 0.0005, 32, int(102400/sample_divisor), 6\n",
    "d, N, he, dropout = 50, 2, 4, 0.2\n",
    "print('number of parameters: ', V)\n",
    "\n",
    "model = StratsModel(2, fore_max_len, V, d, N, he, dropout, forecast=True).cuda()\n",
    "a = summary(model, [(32, 2), (32, 880), (32, 880), (32, 880)],  # shape of fore_train_ip\n",
    "            dtypes=[torch.float, torch.float, torch.float, torch.long])\n",
    "print(a)  # Model summary\n",
    "# raise Exception\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=3)\n",
    "# Pretrain fore_model.\n",
    "best_val_loss = np.inf\n",
    "N_fore = len(fore_train_op)\n",
    "#print(N_fore)\n",
    "fore_savepath = 'mimic_iii_sparse_ims_tf.pytorch'\n",
    "loss_func = torch.nn.MSELoss(reduction=\"none\")\n",
    "\n",
    "# torch.compile(model)\n",
    "for e in range(number_of_epochs):\n",
    "    e_indices = np.random.choice(range(N_fore), size=samples_per_epoch, replace=False)\n",
    "    e_loss = 0\n",
    "    pbar = tqdm(range(0, len(e_indices), batch_size))\n",
    "    model.train()\n",
    "    for start in pbar:\n",
    "        ind = e_indices[start:start+batch_size]\n",
    "        # raise Exception\n",
    "\n",
    "        output = model(*[torch.tensor(ip[ind], dtype=torch.float32).cuda() if i < 3 \n",
    "                         else torch.tensor(ip[ind], dtype=torch.int32).cuda() \n",
    "                         for i, ip in enumerate(fore_train_ip)], trainn=True, tgt=torch.tensor(fore_train_op[ind, :, :131], dtype=torch.float32).cuda())\n",
    "        #raise Exception\n",
    "        #print(fore_train_op[ind].shape)\n",
    "        #raise Exception\n",
    "        loss = torch.tensor(fore_train_op[ind, :, 131:], dtype=torch.float32).cuda()*(\n",
    "            output-torch.tensor(fore_train_op[ind, :, :131], dtype=torch.float32).cuda())**2\n",
    "        loss = loss.sum(axis=-1).mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        e_loss += loss.detach()\n",
    "        pbar.set_description('%f' % e_loss)\n",
    "    val_loss = 0\n",
    "    model.eval()\n",
    "    loss_list = []\n",
    "    pbar = tqdm(range(0, len(fore_valid_op)-32, 32))  # len(fore_valid_op)           ####################   maybe also batch_size instead of 32\n",
    "    for start in pbar:\n",
    "        #print(start, start+batch_size)\n",
    "        output = model(*[torch.tensor(ip[start:start+batch_size], dtype=torch.float32).cuda() \n",
    "                         if i < 3 else torch.tensor(ip[start:start+batch_size], dtype=torch.int32).cuda() \n",
    "                         for i, ip in enumerate(fore_valid_ip)], trainn=False, tgt=torch.tensor(fore_valid_op[start:start+batch_size], dtype=torch.float32).cuda())\n",
    "        loss = torch.tensor(fore_valid_op[start:start+batch_size,:, 131:], dtype=torch.float32).cuda()*loss_func(\n",
    "            output, torch.tensor(fore_valid_op[start:start+batch_size,:, :131], dtype=torch.float32).cuda())\n",
    "        loss_list.extend(loss.sum(axis=-1).mean(axis=-1).detach().cpu().tolist())\n",
    "        loss = loss.sum(axis=-1).mean()\n",
    "        val_loss += loss.detach().cpu()\n",
    "        pbar.set_description('%f' % val_loss)\n",
    "    loss_p = e_loss*batch_size/samples_per_epoch\n",
    "    val_loss_p = val_loss*batch_size/len(fore_valid_op)\n",
    "    print('Epoch', e, 'loss', loss_p, 'val loss', val_loss_p)\n",
    "    with open('loss_values_log', 'a') as f:\n",
    "        f.write(str(e)+' ' + str(loss_p.item()) + ' ' + str(val_loss_p.item())+ '\\n')\n",
    "    scheduler.step(val_loss_p.item())\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), fore_savepath)\n",
    "        best_epoch = e\n",
    "    if (e-best_epoch) > patience:\n",
    "        break\n",
    "print('Training has ended.')\n",
    "\n",
    "\n",
    "#STRATS, IMS, Teacher Forcing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f146538",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-29T17:31:04.722046Z",
     "iopub.status.busy": "2023-09-29T17:31:04.721717Z",
     "iopub.status.idle": "2023-09-30T09:00:12.547431Z",
     "shell.execute_reply": "2023-09-30T09:00:12.544775Z"
    }
   },
   "outputs": [],
   "source": [
    "class CVE(nn.Module):\n",
    "    def __init__(self, hid_units, output_dim):\n",
    "        super(CVE, self).__init__()\n",
    "        self.hid_units = hid_units\n",
    "        self.output_dim = output_dim\n",
    "        self.W1 = nn.Parameter(torch.Tensor(1, self.hid_units))\n",
    "        self.b1 = nn.Parameter(torch.Tensor(self.hid_units))\n",
    "        self.W2 = nn.Parameter(torch.Tensor(self.hid_units, self.output_dim))\n",
    "        nn.init.xavier_uniform_(self.W1)\n",
    "        nn.init.zeros_(self.b1)\n",
    "        nn.init.xavier_uniform_(self.W2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(-1)\n",
    "        x = torch.matmul(torch.tanh(torch.add(torch.matmul(x, self.W1), self.b1)), self.W2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hid_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.hid_dim = hid_dim\n",
    "        self.W = nn.Parameter(torch.Tensor(50, self.hid_dim))\n",
    "        self.b = nn.Parameter(torch.Tensor(self.hid_dim))\n",
    "        self.u = nn.Parameter(torch.Tensor(self.hid_dim, 1))\n",
    "        nn.init.xavier_uniform_(self.W)\n",
    "        nn.init.zeros_(self.b)\n",
    "        nn.init.xavier_uniform_(self.u)\n",
    "\n",
    "    def forward(self, x, mask, mask_value=-1e30):\n",
    "        attn_weights = torch.matmul(torch.tanh(torch.add(torch.matmul(x, self.W), self.b)), self.u)\n",
    "        mask = mask.unsqueeze(-1)\n",
    "        attn_weights = mask * attn_weights + (1 - mask) * mask_value\n",
    "        attn_weights = F.softmax(attn_weights, dim=-2)\n",
    "        return attn_weights\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, N=2, h=8, dk=None, dv=None, dff=None, dropout=0, d=8):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.N, self.h, self.dk, self.dv, self.dff, self.dropout = N, h, dk, dv, dff, dropout\n",
    "        self.epsilon = torch.finfo(torch.float32).eps * torch.finfo(torch.float32).eps\n",
    "        self.Wq = nn.Parameter(torch.Tensor(N, h, d, dk))\n",
    "        self.Wk = nn.Parameter(torch.Tensor(N, h, d, dk))\n",
    "        self.Wv = nn.Parameter(torch.Tensor(N, h, d, dv))\n",
    "        self.Wo = nn.Parameter(torch.Tensor(N, dv * h, d))\n",
    "        self.W1 = nn.Parameter(torch.Tensor(N, d, dff))\n",
    "        self.b1 = nn.Parameter(torch.Tensor(N, dff))\n",
    "        self.W2 = nn.Parameter(torch.Tensor(N, dff, d))\n",
    "        self.b2 = nn.Parameter(torch.Tensor(N, d))\n",
    "        self.gamma = nn.Parameter(torch.Tensor(2 * N))\n",
    "        self.beta = nn.Parameter(torch.Tensor(2 * N))\n",
    "        nn.init.xavier_uniform_(self.Wq)\n",
    "        nn.init.xavier_uniform_(self.Wk)\n",
    "        nn.init.xavier_uniform_(self.Wv)\n",
    "        nn.init.xavier_uniform_(self.Wo)\n",
    "        nn.init.xavier_uniform_(self.W1)\n",
    "        nn.init.zeros_(self.b1)\n",
    "        nn.init.xavier_uniform_(self.W2)\n",
    "        nn.init.zeros_(self.b2)\n",
    "        nn.init.ones_(self.gamma)\n",
    "        nn.init.zeros_(self.beta)\n",
    "        self.training = False\n",
    "        \n",
    "    def forward(self, x, mask, mask_value=-1e-30):\n",
    "        mask = mask.unsqueeze(-2)\n",
    "        for i in range(self.N):\n",
    "            # MHA\n",
    "            mha_ops = []\n",
    "            for j in range(self.h):\n",
    "                q = torch.matmul(x, self.Wq[i, j, :, :])          \n",
    "                k = torch.matmul(x, self.Wk[i, j, :, :]).permute(0, 2, 1)\n",
    "                v = torch.matmul(x, self.Wv[i, j, :, :])\n",
    "                A = torch.matmul(q, k)\n",
    "                # Mask unobserved steps.\n",
    "                A = mask * A + (1 - mask) * mask_value\n",
    "                # Mask for attention dropout.\n",
    "                if self.training:\n",
    "                    dp_mask = (torch.rand(A.shape, device=A.device) >= self.dropout).float()\n",
    "                    A = A * dp_mask + (1 - dp_mask) * mask_value\n",
    "                A = F.softmax(A, dim=-1)\n",
    "                mha_ops.append(torch.matmul(A, v))\n",
    "            conc = torch.cat(mha_ops, dim=-1)\n",
    "            proj = torch.matmul(conc, self.Wo[i, :, :])\n",
    "            # Dropout.\n",
    "            if self.training:\n",
    "                proj = F.dropout(proj, p=self.dropout)\n",
    "            # Add & LN\n",
    "            x = x + proj\n",
    "            mean = x.mean(dim=-1, keepdim=True)\n",
    "            variance = torch.mean(torch.square(x - mean), dim=-1, keepdim=True)\n",
    "            std = torch.sqrt(variance + self.epsilon)\n",
    "            x = (x - mean) / std\n",
    "            x = x * self.gamma[2 * i] + self.beta[2 * i]\n",
    "            # FFN\n",
    "            ffn_op = torch.add(torch.matmul(F.relu(torch.add(\n",
    "                torch.matmul(x, self.W1[i, :, :]), self.b1[i, :])),\n",
    "                                            self.W2[i, :, :]), self.b2[i, :])\n",
    "            # Dropout.\n",
    "            if self.training:\n",
    "                ffn_op = F.dropout(ffn_op, p=self.dropout)\n",
    "            # Add & LN\n",
    "            x = x + ffn_op\n",
    "            mean = x.mean(dim=-1, keepdim=True)\n",
    "            variance = torch.mean(torch.square(x - mean), dim=-1, keepdim=True)\n",
    "            std = torch.sqrt(variance + self.epsilon)\n",
    "            x = (x - mean) / std\n",
    "            x = x * self.gamma[2 * i + 1] + self.beta[2 * i + 1]\n",
    "        return x\n",
    "\n",
    "\n",
    "class StratsModel(nn.Module):\n",
    "    def __init__(self, D, max_len, V, d, N, he, dropout, forecast=False, autoregressive=False):\n",
    "        super(StratsModel, self).__init__()\n",
    "        self.forecast = forecast\n",
    "        self.autoregressive=autoregressive\n",
    "        self.demo_enc = nn.Sequential(nn.Linear(D, 2 * d),\n",
    "                                      nn.Tanh(),\n",
    "                                      nn.Linear(2 * d, d),\n",
    "                                      nn.Tanh())\n",
    "        self.varis_emb = nn.Embedding(V + 1, d)\n",
    "        nn.init.uniform_(self.varis_emb.weight, a=-0.05, b=0.05)\n",
    "        self.values_emb = CVE(int(np.sqrt(d)), d)\n",
    "        self.times_emb = CVE(int(np.sqrt(d)), d)\n",
    "        # self.comb_emb = nn.LayerNorm(d)\n",
    "        self.transformer = Transformer(N, he, dk=d//he, dv=d//he, dff=2*d, dropout=dropout, d=d)\n",
    "\n",
    "        if autoregressive:\n",
    "            self.linearize = nn.Linear(50, 131)\n",
    "            self.dec = nn.TransformerDecoderLayer(d_model=131, dim_feedforward=131, nhead=1, batch_first=True)\n",
    "        else:\n",
    "            self.attention = Attention(2 * d)\n",
    "            self.attentions = nn.ModuleList([Attention(2*d) for i in range(24)])\n",
    "            # self.concat = nn.Linear(2 * d, 1)\n",
    "            self.fore_op_calc = nn.Linear(2 * d, V)\n",
    "            self.op_calc = nn.Linear(V, 1)\n",
    "            self.sig = nn.Sigmoid()        \n",
    "        \n",
    "\n",
    "    def forward(self, demo, times, values, varis, trainn=False, tgt=torch.zeros(32, 1, 131).cuda()):\n",
    "        demo_enc = self.demo_enc(demo)\n",
    "        varis_emb = self.varis_emb(varis)\n",
    "        values_emb = self.values_emb(values)\n",
    "        times_emb = self.times_emb(times)\n",
    "\n",
    "        comb_emb = (torch.sum(torch.stack([varis_emb, values_emb, times_emb]), dim=0))\n",
    "        self.trainn = trainn\n",
    "        mask = torch.clip(varis, 0, 1)\n",
    "        cont_emb = self.transformer(comb_emb, mask)\n",
    "        \n",
    "        \n",
    "        \n",
    "        if self.autoregressive:\n",
    "            if self.trainn:\n",
    "                #print(cont_emb.size())\n",
    "                \n",
    "                mask_new = torch.nn.Transformer.generate_square_subsequent_mask(25).cuda()\n",
    "                linear_memory = self.linearize(cont_emb)\n",
    "                start_input = torch.zeros(32, 1, 131).cuda()\n",
    "                tgt = torch.concat([start_input, tgt], dim=1)\n",
    "                result=self.dec(tgt, linear_memory, mask_new)\n",
    "                return result[:, 1:, :]\n",
    "            if not self.trainn:\n",
    "                output = torch.zeros(32, 1, 131).cuda()\n",
    "                linear_memory  = self.linearize(cont_emb)\n",
    "                for i in range(24):\n",
    "                    #print(i)\n",
    "                    #print(linear_memory.size(), output.size())\n",
    "                    res = self.dec(output.detach(), linear_memory)\n",
    "                    #print(res.size(), output.size())\n",
    "                    output = torch.concat([output, res[:,-1:,:]], dim=1)\n",
    "                return output[:, 1:, :]\n",
    "                \n",
    "        if not self.autoregressive:\n",
    "            fused_embs = []\n",
    "            for attention in self.attentions:\n",
    "                attn_weights = attention(cont_emb, mask)\n",
    "                fused_emb = torch.matmul(attn_weights.transpose(-1, -2), cont_emb).squeeze(-2)\n",
    "                # fused_emb torch.Size([32, 50])\n",
    "                fused_embs.append(fused_emb)\n",
    "            # fused_embs Liste mit 6 torch.Size([32, 50])\n",
    "            fused_emb = torch.stack(fused_embs, axis=1)\n",
    "            # altes fused_emb: torch.Size([32, 50])\n",
    "            # neues fused_emb: torch.Size([32, 6, 50])\n",
    "            # demo_enc torch.Size([32, 50])\n",
    "\n",
    "            # demo_enc.unsqueeze(axis=-2).repeat(1, 6, 1):  torch.Size([32, 6, 50])\n",
    "            #print(fused_emb.size())\n",
    "            concat = torch.cat([fused_emb, demo_enc.unsqueeze(axis=-2).repeat(1, 24, 1)], axis=-1)\n",
    "            #concat torch.Size([32, 6, 100])\n",
    "\n",
    "            x = self.fore_op_calc(concat)\n",
    "            # neues x torch.Size([32, 6, 258])\n",
    "            # altes x: torch.Size([32, 258])\n",
    "            if self.forecast:\n",
    "                return x\n",
    "            else:\n",
    "                x = self.sig(self.op_calc(x)).mean(axis=-2)\n",
    "                return x\n",
    "\n",
    "print(\"Model definitions loaded\")\n",
    "\n",
    "lr, batch_size, samples_per_epoch, patience = 0.0005, 32, int(102400/sample_divisor), 6\n",
    "d, N, he, dropout = 50, 2, 4, 0.2\n",
    "print('number of parameters: ', V)\n",
    "\n",
    "model = StratsModel(2, fore_max_len, V, d, N, he, dropout, forecast=True).cuda()\n",
    "a = summary(model, [(32, 2), (32, 880), (32, 880), (32, 880)],  # shape of fore_train_ip\n",
    "            dtypes=[torch.float, torch.float, torch.float, torch.long])\n",
    "print(a)  # Model summary\n",
    "# raise Exception\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=3)\n",
    "# Pretrain fore_model.\n",
    "best_val_loss = np.inf\n",
    "N_fore = len(fore_train_op)\n",
    "#print(N_fore)\n",
    "fore_savepath = 'mimic_iii_sparse_dms.pytorch'\n",
    "loss_func = torch.nn.MSELoss(reduction=\"none\")\n",
    "\n",
    "# torch.compile(model)\n",
    "for e in range(number_of_epochs):\n",
    "    e_indices = np.random.choice(range(N_fore), size=samples_per_epoch, replace=False)\n",
    "    e_loss = 0\n",
    "    pbar = tqdm(range(0, len(e_indices), batch_size))\n",
    "    model.train()\n",
    "    for start in pbar:\n",
    "        ind = e_indices[start:start+batch_size]\n",
    "        # raise Exception\n",
    "\n",
    "        output = model(*[torch.tensor(ip[ind], dtype=torch.float32).cuda() if i < 3 \n",
    "                         else torch.tensor(ip[ind], dtype=torch.int32).cuda() \n",
    "                         for i, ip in enumerate(fore_train_ip)], trainn=True, tgt=torch.tensor(fore_train_op[ind, :, :131], dtype=torch.float32).cuda())\n",
    "        #raise Exception\n",
    "        #print(fore_train_op[ind].shape)\n",
    "        #raise Exception\n",
    "        loss = torch.tensor(fore_train_op[ind, :, 131:], dtype=torch.float32).cuda()*(\n",
    "            output-torch.tensor(fore_train_op[ind, :, :131], dtype=torch.float32).cuda())**2\n",
    "        loss = loss.sum(axis=-1).mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        e_loss += loss.detach()\n",
    "        pbar.set_description('%f' % e_loss)\n",
    "    val_loss = 0\n",
    "    model.eval()\n",
    "    loss_list = []\n",
    "    pbar = tqdm(range(0, len(fore_valid_op)-32, 32))  # len(fore_valid_op)           ####################   maybe also batch_size instead of 32\n",
    "    for start in pbar:\n",
    "        #print(start, start+batch_size)\n",
    "        output = model(*[torch.tensor(ip[start:start+batch_size], dtype=torch.float32).cuda() \n",
    "                         if i < 3 else torch.tensor(ip[start:start+batch_size], dtype=torch.int32).cuda() \n",
    "                         for i, ip in enumerate(fore_valid_ip)], trainn=False, tgt=torch.tensor(fore_valid_op[start:start+batch_size], dtype=torch.float32).cuda())\n",
    "        loss = torch.tensor(fore_valid_op[start:start+batch_size,:, 131:], dtype=torch.float32).cuda()*loss_func(\n",
    "            output, torch.tensor(fore_valid_op[start:start+batch_size,:, :131], dtype=torch.float32).cuda())\n",
    "        loss_list.extend(loss.sum(axis=-1).mean(axis=-1).detach().cpu().tolist())\n",
    "        loss = loss.sum(axis=-1).mean()\n",
    "        val_loss += loss.detach().cpu()\n",
    "        pbar.set_description('%f' % val_loss)\n",
    "    loss_p = e_loss*batch_size/samples_per_epoch\n",
    "    val_loss_p = val_loss*batch_size/len(fore_valid_op)\n",
    "    print('Epoch', e, 'loss', loss_p, 'val loss', val_loss_p)\n",
    "    with open('loss_values_log', 'a') as f:\n",
    "        f.write(str(e)+' ' + str(loss_p.item()) + ' ' + str(val_loss_p.item())+ '\\n')\n",
    "    scheduler.step(val_loss_p.item())\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), fore_savepath)\n",
    "        best_epoch = e\n",
    "    if (e-best_epoch) > patience:\n",
    "        break\n",
    "print('Training has ended.')\n",
    "#STRATS, DMS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f5c3d1",
   "metadata": {},
   "source": [
    "# Scheduled Sampling DD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f449e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVE(nn.Module):\n",
    "    def __init__(self, hid_units, output_dim):\n",
    "        super(CVE, self).__init__()\n",
    "        self.hid_units = hid_units\n",
    "        self.output_dim = output_dim\n",
    "        self.W1 = nn.Parameter(torch.Tensor(1, self.hid_units))\n",
    "        self.b1 = nn.Parameter(torch.Tensor(self.hid_units))\n",
    "        self.W2 = nn.Parameter(torch.Tensor(self.hid_units, self.output_dim))\n",
    "        nn.init.xavier_uniform_(self.W1)\n",
    "        nn.init.zeros_(self.b1)\n",
    "        nn.init.xavier_uniform_(self.W2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(-1)\n",
    "        x = torch.matmul(torch.tanh(torch.add(torch.matmul(x, self.W1), self.b1)), self.W2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hid_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.hid_dim = hid_dim\n",
    "        self.W = nn.Parameter(torch.Tensor(50, self.hid_dim))\n",
    "        self.b = nn.Parameter(torch.Tensor(self.hid_dim))\n",
    "        self.u = nn.Parameter(torch.Tensor(self.hid_dim, 1))\n",
    "        nn.init.xavier_uniform_(self.W)\n",
    "        nn.init.zeros_(self.b)\n",
    "        nn.init.xavier_uniform_(self.u)\n",
    "\n",
    "    def forward(self, x, mask, mask_value=-1e30):\n",
    "        attn_weights = torch.matmul(torch.tanh(torch.add(torch.matmul(x, self.W), self.b)), self.u)\n",
    "        mask = mask.unsqueeze(-1)\n",
    "        attn_weights = mask * attn_weights + (1 - mask) * mask_value\n",
    "        attn_weights = F.softmax(attn_weights, dim=-2)\n",
    "        return attn_weights\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, N=2, h=8, dk=None, dv=None, dff=None, dropout=0, d=8):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.N, self.h, self.dk, self.dv, self.dff, self.dropout = N, h, dk, dv, dff, dropout\n",
    "        self.epsilon = torch.finfo(torch.float32).eps * torch.finfo(torch.float32).eps\n",
    "        self.Wq = nn.Parameter(torch.Tensor(N, h, d, dk))\n",
    "        self.Wk = nn.Parameter(torch.Tensor(N, h, d, dk))\n",
    "        self.Wv = nn.Parameter(torch.Tensor(N, h, d, dv))\n",
    "        self.Wo = nn.Parameter(torch.Tensor(N, dv * h, d))\n",
    "        self.W1 = nn.Parameter(torch.Tensor(N, d, dff))\n",
    "        self.b1 = nn.Parameter(torch.Tensor(N, dff))\n",
    "        self.W2 = nn.Parameter(torch.Tensor(N, dff, d))\n",
    "        self.b2 = nn.Parameter(torch.Tensor(N, d))\n",
    "        self.gamma = nn.Parameter(torch.Tensor(2 * N))\n",
    "        self.beta = nn.Parameter(torch.Tensor(2 * N))\n",
    "        nn.init.xavier_uniform_(self.Wq)\n",
    "        nn.init.xavier_uniform_(self.Wk)\n",
    "        nn.init.xavier_uniform_(self.Wv)\n",
    "        nn.init.xavier_uniform_(self.Wo)\n",
    "        nn.init.xavier_uniform_(self.W1)\n",
    "        nn.init.zeros_(self.b1)\n",
    "        nn.init.xavier_uniform_(self.W2)\n",
    "        nn.init.zeros_(self.b2)\n",
    "        nn.init.ones_(self.gamma)\n",
    "        nn.init.zeros_(self.beta)\n",
    "        self.training = False\n",
    "        \n",
    "    def forward(self, x, mask, mask_value=-1e-30):\n",
    "        mask = mask.unsqueeze(-2)\n",
    "        for i in range(self.N):\n",
    "            # MHA\n",
    "            mha_ops = []\n",
    "            for j in range(self.h):\n",
    "                q = torch.matmul(x, self.Wq[i, j, :, :])          \n",
    "                k = torch.matmul(x, self.Wk[i, j, :, :]).permute(0, 2, 1)\n",
    "                v = torch.matmul(x, self.Wv[i, j, :, :])\n",
    "                A = torch.matmul(q, k)\n",
    "                # Mask unobserved steps.\n",
    "                A = mask * A + (1 - mask) * mask_value\n",
    "                # Mask for attention dropout.\n",
    "                if self.training:\n",
    "                    dp_mask = (torch.rand(A.shape, device=A.device) >= self.dropout).float()\n",
    "                    A = A * dp_mask + (1 - dp_mask) * mask_value\n",
    "                A = F.softmax(A, dim=-1)\n",
    "                mha_ops.append(torch.matmul(A, v))\n",
    "            conc = torch.cat(mha_ops, dim=-1)\n",
    "            proj = torch.matmul(conc, self.Wo[i, :, :])\n",
    "            # Dropout.\n",
    "            if self.training:\n",
    "                proj = F.dropout(proj, p=self.dropout)\n",
    "            # Add & LN\n",
    "            x = x + proj\n",
    "            mean = x.mean(dim=-1, keepdim=True)\n",
    "            variance = torch.mean(torch.square(x - mean), dim=-1, keepdim=True)\n",
    "            std = torch.sqrt(variance + self.epsilon)\n",
    "            x = (x - mean) / std\n",
    "            x = x * self.gamma[2 * i] + self.beta[2 * i]\n",
    "            # FFN\n",
    "            ffn_op = torch.add(torch.matmul(F.relu(torch.add(\n",
    "                torch.matmul(x, self.W1[i, :, :]), self.b1[i, :])),\n",
    "                                            self.W2[i, :, :]), self.b2[i, :])\n",
    "            # Dropout.\n",
    "            if self.training:\n",
    "                ffn_op = F.dropout(ffn_op, p=self.dropout)\n",
    "            # Add & LN\n",
    "            x = x + ffn_op\n",
    "            mean = x.mean(dim=-1, keepdim=True)\n",
    "            variance = torch.mean(torch.square(x - mean), dim=-1, keepdim=True)\n",
    "            std = torch.sqrt(variance + self.epsilon)\n",
    "            x = (x - mean) / std\n",
    "            x = x * self.gamma[2 * i + 1] + self.beta[2 * i + 1]\n",
    "        return x\n",
    "\n",
    "\n",
    "class StratsModel(nn.Module):\n",
    "    def __init__(self, D, max_len, V, d, N, he, dropout, forecast=False, autoregressive=True):\n",
    "        super(StratsModel, self).__init__()\n",
    "        self.forecast = forecast\n",
    "        self.autoregressive=autoregressive\n",
    "        self.demo_enc = nn.Sequential(nn.Linear(D, 2 * d),\n",
    "                                      nn.Tanh(),\n",
    "                                      nn.Linear(2 * d, d),\n",
    "                                      nn.Tanh())\n",
    "        self.varis_emb = nn.Embedding(V + 1, d)\n",
    "        nn.init.uniform_(self.varis_emb.weight, a=-0.05, b=0.05)\n",
    "        self.values_emb = CVE(int(np.sqrt(d)), d)\n",
    "        self.times_emb = CVE(int(np.sqrt(d)), d)\n",
    "        # self.comb_emb = nn.LayerNorm(d)\n",
    "        self.transformer = Transformer(N, he, dk=d//he, dv=d//he, dff=2*d, dropout=dropout, d=d)\n",
    "\n",
    "        if autoregressive:\n",
    "            self.linearize = nn.Linear(50, 131)\n",
    "            self.dec = nn.TransformerDecoderLayer(d_model=131, dim_feedforward=131, nhead=1, batch_first=True)\n",
    "        else:\n",
    "            self.attention = Attention(2 * d)\n",
    "            self.attentions = nn.ModuleList([Attention(2*d) for i in range(24)])\n",
    "            # self.concat = nn.Linear(2 * d, 1)\n",
    "            self.fore_op_calc = nn.Linear(2 * d, V)\n",
    "            self.op_calc = nn.Linear(V, 1)\n",
    "            self.sig = nn.Sigmoid()        \n",
    "        \n",
    "\n",
    "    def forward(self, demo, times, values, varis, trainn=False, tgt=torch.zeros(32, 1, 131).cuda(), schedule=True, probability=0.5, deterministic=False):\n",
    "        demo_enc = self.demo_enc(demo)\n",
    "        varis_emb = self.varis_emb(varis)\n",
    "        values_emb = self.values_emb(values)\n",
    "        times_emb = self.times_emb(times)\n",
    "\n",
    "        comb_emb = (torch.sum(torch.stack([varis_emb, values_emb, times_emb]), dim=0))\n",
    "        self.trainn = trainn\n",
    "        mask = torch.clip(varis, 0, 1)\n",
    "        cont_emb = self.transformer(comb_emb, mask)\n",
    "        \n",
    "        \n",
    "        if trainn and schedule:\n",
    "            \n",
    "            output = torch.zeros(cont_emb.size(0), 1, 131).cuda()\n",
    "            output.requires_grad=True\n",
    "            linear_memory  = self.linearize(cont_emb)\n",
    "            for i in range(24):\n",
    "                #print(i)\n",
    "                #print(linear_memory.size(), output.size())\n",
    "                res = self.dec(output.detach(), linear_memory)\n",
    "                if deterministic:\n",
    "                    sample = i/24\n",
    "                else:\n",
    "                    sample = torch.rand(1).item()\n",
    "                #print(res.size(), output.size())\n",
    "                print(sample, probability)\n",
    "                if sample < probability:\n",
    "                \n",
    "                    \n",
    "                    output = torch.concat([output, tgt[:,i:i+1,:]], dim=1)\n",
    "                else:\n",
    "                    output = torch.concat([output, res[:,-1:,:]], dim=1)\n",
    "            return output[:, 1:, :] \n",
    "        if self.autoregressive:\n",
    "            if self.trainn:\n",
    "                #print(cont_emb.size())\n",
    "                \n",
    "                mask_new = torch.nn.Transformer.generate_square_subsequent_mask(25).cuda()\n",
    "                linear_memory = self.linearize(cont_emb)\n",
    "                start_input = torch.zeros(32, 1, 131).cuda()\n",
    "                tgt = torch.concat([start_input, tgt], dim=1)\n",
    "                result=self.dec(tgt, linear_memory, mask_new)\n",
    "                return result[:, 1:, :]\n",
    "            if not self.trainn:\n",
    "                output = torch.zeros(32, 1, 131).cuda()\n",
    "                linear_memory  = self.linearize(cont_emb)\n",
    "                for i in range(24):\n",
    "                    #print(i)\n",
    "                    #print(linear_memory.size(), output.size())\n",
    "                    res = self.dec(output.detach(), linear_memory)\n",
    "                    #print(res.size(), output.size())\n",
    "                    output = torch.concat([output, res[:,-1:,:]], dim=1)\n",
    "                return output[:, 1:, :]\n",
    "                \n",
    "        if not self.autoregressive:\n",
    "            fused_embs = []\n",
    "            for attention in self.attentions:\n",
    "                attn_weights = attention(cont_emb, mask)\n",
    "                fused_emb = torch.matmul(attn_weights.transpose(-1, -2), cont_emb).squeeze(-2)\n",
    "                # fused_emb torch.Size([32, 50])\n",
    "                fused_embs.append(fused_emb)\n",
    "            # fused_embs Liste mit 6 torch.Size([32, 50])\n",
    "            fused_emb = torch.stack(fused_embs, axis=1)\n",
    "            # altes fused_emb: torch.Size([32, 50])\n",
    "            # neues fused_emb: torch.Size([32, 6, 50])\n",
    "            # demo_enc torch.Size([32, 50])\n",
    "\n",
    "            # demo_enc.unsqueeze(axis=-2).repeat(1, 6, 1):  torch.Size([32, 6, 50])\n",
    "            #print(fused_emb.size())\n",
    "            concat = torch.cat([fused_emb, demo_enc.unsqueeze(axis=-2).repeat(1, 24, 1)], axis=-1)\n",
    "            #concat torch.Size([32, 6, 100])\n",
    "\n",
    "            x = self.fore_op_calc(concat)\n",
    "            # neues x torch.Size([32, 6, 258])\n",
    "            # altes x: torch.Size([32, 258])\n",
    "            if self.forecast:\n",
    "                return x\n",
    "            else:\n",
    "                x = self.sig(self.op_calc(x)).mean(axis=-2)\n",
    "                return x\n",
    "\n",
    "print(\"Model definitions loaded\")\n",
    "\n",
    "\n",
    "lr, batch_size, samples_per_epoch, patience = 0.0005, 32, int(102400/sample_divisor), 6\n",
    "d, N, he, dropout = 50, 2, 4, 0.2\n",
    "print('number of parameters: ', V)\n",
    "\n",
    "model = StratsModel(2, fore_max_len, V, d, N, he, dropout, forecast=True).cuda()\n",
    "a = summary(model, [(32, 2), (32, 880), (32, 880), (32, 880)],  # shape of fore_train_ip\n",
    "            dtypes=[torch.float, torch.float, torch.float, torch.long])\n",
    "print(a)  # Model summary\n",
    "# raise Exception\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=3)\n",
    "# Pretrain fore_model.\n",
    "best_val_loss = np.inf\n",
    "N_fore = len(fore_train_op)\n",
    "#print(N_fore)\n",
    "fore_savepath = 'mimic_iii_sparse_ims_sampling_detach_schedule.pytorch'\n",
    "loss_func = torch.nn.MSELoss(reduction=\"none\")\n",
    "\n",
    "# torch.compile(model)\n",
    "for e in range(number_of_epochs):\n",
    "    e_indices = np.random.choice(range(N_fore), size=samples_per_epoch, replace=False)\n",
    "    e_loss = 0\n",
    "    pbar = tqdm(range(0, len(e_indices), batch_size))\n",
    "    model.train()\n",
    "    for start in pbar:\n",
    "        ind = e_indices[start:start+batch_size]\n",
    "        # raise Exception\n",
    "        prob=min([1.0, 1.0+(0.25-1.0)*(1-e/200)])\n",
    "        output = model(*[torch.tensor(ip[ind], dtype=torch.float32).cuda() if i < 3 \n",
    "                         else torch.tensor(ip[ind], dtype=torch.int32).cuda() \n",
    "                         for i, ip in enumerate(fore_train_ip)], trainn=True, tgt=torch.tensor(fore_train_op[ind, :, :131], dtype=torch.float32).cuda(), schedule=True, deterministic=True, probability=prob)\n",
    "        #raise Exception\n",
    "        #print(fore_train_op[ind].shape)\n",
    "        #raise Exception\n",
    "        loss = torch.tensor(fore_train_op[ind, :, 131:], dtype=torch.float32).cuda()*(\n",
    "            output-torch.tensor(fore_train_op[ind, :, :131], dtype=torch.float32).cuda())**2\n",
    "        loss = loss.sum(axis=-1).mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        e_loss += loss.detach()\n",
    "        pbar.set_description('%f' % e_loss)\n",
    "    val_loss = 0\n",
    "    model.eval()\n",
    "    loss_list = []\n",
    "    pbar = tqdm(range(0, len(fore_valid_op)-32, 32))  # len(fore_valid_op)           ####################   maybe also batch_size instead of 32\n",
    "    for start in pbar:\n",
    "        #print(start, start+batch_size)\n",
    "        output = model(*[torch.tensor(ip[start:start+batch_size], dtype=torch.float32).cuda() \n",
    "                         if i < 3 else torch.tensor(ip[start:start+batch_size], dtype=torch.int32).cuda() \n",
    "                         for i, ip in enumerate(fore_valid_ip)], trainn=False, tgt=torch.tensor(fore_valid_op[start:start+batch_size], dtype=torch.float32).cuda())\n",
    "        loss = torch.tensor(fore_valid_op[start:start+batch_size,:, 131:], dtype=torch.float32).cuda()*loss_func(\n",
    "            output, torch.tensor(fore_valid_op[start:start+batch_size,:, :131], dtype=torch.float32).cuda())\n",
    "        loss_list.extend(loss.sum(axis=-1).mean(axis=-1).detach().cpu().tolist())\n",
    "        loss = loss.sum(axis=-1).mean()\n",
    "        val_loss += loss.detach().cpu()\n",
    "        pbar.set_description('%f' % val_loss)\n",
    "    loss_p = e_loss*batch_size/samples_per_epoch\n",
    "    val_loss_p = val_loss*batch_size/len(fore_valid_op)\n",
    "    print('Epoch', e, 'loss', loss_p, 'val loss', val_loss_p, \"mean and std\", np.mean(loss_list), np.std(loss_list))\n",
    "    with open('loss_values_log', 'a') as f:\n",
    "        f.write(str(e)+' ' + str(loss_p.item()) + ' ' + str(val_loss_p.item())+ '\\n')\n",
    "    scheduler.step(val_loss_p.item())\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), fore_savepath)\n",
    "        best_epoch = e\n",
    "    if (e-best_epoch) > patience:\n",
    "        break\n",
    "print('Training has ended.')\n",
    "\n",
    "#STRATS, IMS, Sampling, Detach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5d7f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVE(nn.Module):\n",
    "    def __init__(self, hid_units, output_dim):\n",
    "        super(CVE, self).__init__()\n",
    "        self.hid_units = hid_units\n",
    "        self.output_dim = output_dim\n",
    "        self.W1 = nn.Parameter(torch.Tensor(1, self.hid_units))\n",
    "        self.b1 = nn.Parameter(torch.Tensor(self.hid_units))\n",
    "        self.W2 = nn.Parameter(torch.Tensor(self.hid_units, self.output_dim))\n",
    "        nn.init.xavier_uniform_(self.W1)\n",
    "        nn.init.zeros_(self.b1)\n",
    "        nn.init.xavier_uniform_(self.W2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(-1)\n",
    "        x = torch.matmul(torch.tanh(torch.add(torch.matmul(x, self.W1), self.b1)), self.W2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hid_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.hid_dim = hid_dim\n",
    "        self.W = nn.Parameter(torch.Tensor(50, self.hid_dim))\n",
    "        self.b = nn.Parameter(torch.Tensor(self.hid_dim))\n",
    "        self.u = nn.Parameter(torch.Tensor(self.hid_dim, 1))\n",
    "        nn.init.xavier_uniform_(self.W)\n",
    "        nn.init.zeros_(self.b)\n",
    "        nn.init.xavier_uniform_(self.u)\n",
    "\n",
    "    def forward(self, x, mask, mask_value=-1e30):\n",
    "        attn_weights = torch.matmul(torch.tanh(torch.add(torch.matmul(x, self.W), self.b)), self.u)\n",
    "        mask = mask.unsqueeze(-1)\n",
    "        attn_weights = mask * attn_weights + (1 - mask) * mask_value\n",
    "        attn_weights = F.softmax(attn_weights, dim=-2)\n",
    "        return attn_weights\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, N=2, h=8, dk=None, dv=None, dff=None, dropout=0, d=8):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.N, self.h, self.dk, self.dv, self.dff, self.dropout = N, h, dk, dv, dff, dropout\n",
    "        self.epsilon = torch.finfo(torch.float32).eps * torch.finfo(torch.float32).eps\n",
    "        self.Wq = nn.Parameter(torch.Tensor(N, h, d, dk))\n",
    "        self.Wk = nn.Parameter(torch.Tensor(N, h, d, dk))\n",
    "        self.Wv = nn.Parameter(torch.Tensor(N, h, d, dv))\n",
    "        self.Wo = nn.Parameter(torch.Tensor(N, dv * h, d))\n",
    "        self.W1 = nn.Parameter(torch.Tensor(N, d, dff))\n",
    "        self.b1 = nn.Parameter(torch.Tensor(N, dff))\n",
    "        self.W2 = nn.Parameter(torch.Tensor(N, dff, d))\n",
    "        self.b2 = nn.Parameter(torch.Tensor(N, d))\n",
    "        self.gamma = nn.Parameter(torch.Tensor(2 * N))\n",
    "        self.beta = nn.Parameter(torch.Tensor(2 * N))\n",
    "        nn.init.xavier_uniform_(self.Wq)\n",
    "        nn.init.xavier_uniform_(self.Wk)\n",
    "        nn.init.xavier_uniform_(self.Wv)\n",
    "        nn.init.xavier_uniform_(self.Wo)\n",
    "        nn.init.xavier_uniform_(self.W1)\n",
    "        nn.init.zeros_(self.b1)\n",
    "        nn.init.xavier_uniform_(self.W2)\n",
    "        nn.init.zeros_(self.b2)\n",
    "        nn.init.ones_(self.gamma)\n",
    "        nn.init.zeros_(self.beta)\n",
    "        self.training = False\n",
    "        \n",
    "    def forward(self, x, mask, mask_value=-1e-30):\n",
    "        mask = mask.unsqueeze(-2)\n",
    "        for i in range(self.N):\n",
    "            # MHA\n",
    "            mha_ops = []\n",
    "            for j in range(self.h):\n",
    "                q = torch.matmul(x, self.Wq[i, j, :, :])          \n",
    "                k = torch.matmul(x, self.Wk[i, j, :, :]).permute(0, 2, 1)\n",
    "                v = torch.matmul(x, self.Wv[i, j, :, :])\n",
    "                A = torch.matmul(q, k)\n",
    "                # Mask unobserved steps.\n",
    "                A = mask * A + (1 - mask) * mask_value\n",
    "                # Mask for attention dropout.\n",
    "                if self.training:\n",
    "                    dp_mask = (torch.rand(A.shape, device=A.device) >= self.dropout).float()\n",
    "                    A = A * dp_mask + (1 - dp_mask) * mask_value\n",
    "                A = F.softmax(A, dim=-1)\n",
    "                mha_ops.append(torch.matmul(A, v))\n",
    "            conc = torch.cat(mha_ops, dim=-1)\n",
    "            proj = torch.matmul(conc, self.Wo[i, :, :])\n",
    "            # Dropout.\n",
    "            if self.training:\n",
    "                proj = F.dropout(proj, p=self.dropout)\n",
    "            # Add & LN\n",
    "            x = x + proj\n",
    "            mean = x.mean(dim=-1, keepdim=True)\n",
    "            variance = torch.mean(torch.square(x - mean), dim=-1, keepdim=True)\n",
    "            std = torch.sqrt(variance + self.epsilon)\n",
    "            x = (x - mean) / std\n",
    "            x = x * self.gamma[2 * i] + self.beta[2 * i]\n",
    "            # FFN\n",
    "            ffn_op = torch.add(torch.matmul(F.relu(torch.add(\n",
    "                torch.matmul(x, self.W1[i, :, :]), self.b1[i, :])),\n",
    "                                            self.W2[i, :, :]), self.b2[i, :])\n",
    "            # Dropout.\n",
    "            if self.training:\n",
    "                ffn_op = F.dropout(ffn_op, p=self.dropout)\n",
    "            # Add & LN\n",
    "            x = x + ffn_op\n",
    "            mean = x.mean(dim=-1, keepdim=True)\n",
    "            variance = torch.mean(torch.square(x - mean), dim=-1, keepdim=True)\n",
    "            std = torch.sqrt(variance + self.epsilon)\n",
    "            x = (x - mean) / std\n",
    "            x = x * self.gamma[2 * i + 1] + self.beta[2 * i + 1]\n",
    "        return x\n",
    "\n",
    "\n",
    "class StratsModel(nn.Module):\n",
    "    def __init__(self, D, max_len, V, d, N, he, dropout, forecast=False, autoregressive=True):\n",
    "        super(StratsModel, self).__init__()\n",
    "        self.forecast = forecast\n",
    "        self.autoregressive=autoregressive\n",
    "        self.demo_enc = nn.Sequential(nn.Linear(D, 2 * d),\n",
    "                                      nn.Tanh(),\n",
    "                                      nn.Linear(2 * d, d),\n",
    "                                      nn.Tanh())\n",
    "        self.varis_emb = nn.Embedding(V + 1, d)\n",
    "        nn.init.uniform_(self.varis_emb.weight, a=-0.05, b=0.05)\n",
    "        self.values_emb = CVE(int(np.sqrt(d)), d)\n",
    "        self.times_emb = CVE(int(np.sqrt(d)), d)\n",
    "        # self.comb_emb = nn.LayerNorm(d)\n",
    "        self.transformer = Transformer(N, he, dk=d//he, dv=d//he, dff=2*d, dropout=dropout, d=d)\n",
    "\n",
    "        if autoregressive:\n",
    "            self.linearize = nn.Linear(50, 131)\n",
    "            self.dec = nn.TransformerDecoderLayer(d_model=131, dim_feedforward=131, nhead=1, batch_first=True)\n",
    "        else:\n",
    "            self.attention = Attention(2 * d)\n",
    "            self.attentions = nn.ModuleList([Attention(2*d) for i in range(24)])\n",
    "            # self.concat = nn.Linear(2 * d, 1)\n",
    "            self.fore_op_calc = nn.Linear(2 * d, V)\n",
    "            self.op_calc = nn.Linear(V, 1)\n",
    "            self.sig = nn.Sigmoid()        \n",
    "        \n",
    "\n",
    "    def forward(self, demo, times, values, varis, trainn=False, tgt=torch.zeros(32, 1, 131).cuda(), schedule=True, probability=0.5, deterministic=False):\n",
    "        demo_enc = self.demo_enc(demo)\n",
    "        varis_emb = self.varis_emb(varis)\n",
    "        values_emb = self.values_emb(values)\n",
    "        times_emb = self.times_emb(times)\n",
    "\n",
    "        comb_emb = (torch.sum(torch.stack([varis_emb, values_emb, times_emb]), dim=0))\n",
    "        self.trainn = trainn\n",
    "        mask = torch.clip(varis, 0, 1)\n",
    "        cont_emb = self.transformer(comb_emb, mask)\n",
    "        \n",
    "        if trainn and schedule:\n",
    "            output = torch.zeros(cont_emb.size(0), 1, 131).cuda()\n",
    "            output.requires_grad=True\n",
    "            linear_memory  = self.linearize(cont_emb)\n",
    "            for i in range(24):\n",
    "                #print(i)\n",
    "                #print(linear_memory.size(), output.size())\n",
    "                res = self.dec(output, linear_memory)\n",
    "                if deterministic:\n",
    "                    sample = i/24\n",
    "                else:\n",
    "                    sample = torch.rand(1).item()\n",
    "                #print(res.size(), output.size())\n",
    "                print(sample, probability)\n",
    "                if sample < probability:\n",
    "                \n",
    "                    \n",
    "                    output = torch.concat([output, tgt[:,i:i+1,:]], dim=1)\n",
    "                else:\n",
    "                    output = torch.concat([output, res[:,-1:,:]], dim=1)\n",
    "            return output[:, 1:, :] \n",
    "        \n",
    "        if self.autoregressive:\n",
    "            if self.trainn:\n",
    "                #print(cont_emb.size())\n",
    "                \n",
    "                mask_new = torch.nn.Transformer.generate_square_subsequent_mask(25).cuda()\n",
    "                linear_memory = self.linearize(cont_emb)\n",
    "                start_input = torch.zeros(32, 1, 131).cuda()\n",
    "                tgt = torch.concat([start_input, tgt], dim=1)\n",
    "                result=self.dec(tgt, linear_memory, mask_new)\n",
    "                return result[:, 1:, :]\n",
    "            if not self.trainn:\n",
    "                output = torch.zeros(32, 1, 131).cuda()\n",
    "                linear_memory  = self.linearize(cont_emb)\n",
    "                for i in range(24):\n",
    "                    #print(i)\n",
    "                    #print(linear_memory.size(), output.size())\n",
    "                    res = self.dec(output, linear_memory)\n",
    "                    #print(res.size(), output.size())\n",
    "                    output = torch.concat([output, res[:,-1:,:]], dim=1)\n",
    "                return output[:, 1:, :]\n",
    "                \n",
    "        if not self.autoregressive:\n",
    "            fused_embs = []\n",
    "            for attention in self.attentions:\n",
    "                attn_weights = attention(cont_emb, mask)\n",
    "                fused_emb = torch.matmul(attn_weights.transpose(-1, -2), cont_emb).squeeze(-2)\n",
    "                # fused_emb torch.Size([32, 50])\n",
    "                fused_embs.append(fused_emb)\n",
    "            # fused_embs Liste mit 6 torch.Size([32, 50])\n",
    "            fused_emb = torch.stack(fused_embs, axis=1)\n",
    "            # altes fused_emb: torch.Size([32, 50])\n",
    "            # neues fused_emb: torch.Size([32, 6, 50])\n",
    "            # demo_enc torch.Size([32, 50])\n",
    "\n",
    "            # demo_enc.unsqueeze(axis=-2).repeat(1, 6, 1):  torch.Size([32, 6, 50])\n",
    "            #print(fused_emb.size())\n",
    "            concat = torch.cat([fused_emb, demo_enc.unsqueeze(axis=-2).repeat(1, 24, 1)], axis=-1)\n",
    "            #concat torch.Size([32, 6, 100])\n",
    "\n",
    "            x = self.fore_op_calc(concat)\n",
    "            # neues x torch.Size([32, 6, 258])\n",
    "            # altes x: torch.Size([32, 258])\n",
    "            if self.forecast:\n",
    "                return x\n",
    "            else:\n",
    "                x = self.sig(self.op_calc(x)).mean(axis=-2)\n",
    "                return x\n",
    "\n",
    "print(\"Model definitions loaded\")\n",
    "\n",
    "lr, batch_size, samples_per_epoch, patience = 0.0005, 32, int(102400/sample_divisor), 6\n",
    "d, N, he, dropout = 50, 2, 4, 0.2\n",
    "print('number of parameters: ', V)\n",
    "\n",
    "model = StratsModel(2, fore_max_len, V, d, N, he, dropout, forecast=True).cuda()\n",
    "a = summary(model, [(32, 2), (32, 880), (32, 880), (32, 880)],  # shape of fore_train_ip\n",
    "            dtypes=[torch.float, torch.float, torch.float, torch.long])\n",
    "print(a)  # Model summary\n",
    "# raise Exception\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=3)\n",
    "# Pretrain fore_model.\n",
    "best_val_loss = np.inf\n",
    "N_fore = len(fore_train_op)\n",
    "#print(N_fore)\n",
    "fore_savepath = 'mimic_iii_sparse_ims_sampling_backprop_schedule.pytorch'\n",
    "loss_func = torch.nn.MSELoss(reduction=\"none\")\n",
    "\n",
    "# torch.compile(model)\n",
    "for e in range(number_of_epochs):\n",
    "    e_indices = np.random.choice(range(N_fore), size=samples_per_epoch, replace=False)\n",
    "    e_loss = 0\n",
    "    pbar = tqdm(range(0, len(e_indices), batch_size))\n",
    "    model.train()\n",
    "    for start in pbar:\n",
    "        ind = e_indices[start:start+batch_size]\n",
    "        # raise Exception\n",
    "        prob=min([1.0, 1.0+(0.25-1.0)*(1-e/200)])\n",
    "        output = model(*[torch.tensor(ip[ind], dtype=torch.float32).cuda() if i < 3 \n",
    "                         else torch.tensor(ip[ind], dtype=torch.int32).cuda() \n",
    "                         for i, ip in enumerate(fore_train_ip)], trainn=True, tgt=torch.tensor(fore_train_op[ind, :, :131], dtype=torch.float32).cuda(), schedule=True, deterministic=True, probability=prob)\n",
    "        #raise Exception\n",
    "        #print(fore_train_op[ind].shape)\n",
    "        #raise Exception\n",
    "        loss = torch.tensor(fore_train_op[ind, :, 131:], dtype=torch.float32).cuda()*(\n",
    "            output-torch.tensor(fore_train_op[ind, :, :131], dtype=torch.float32).cuda())**2\n",
    "        loss = loss.sum(axis=-1).mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        e_loss += loss.detach()\n",
    "        pbar.set_description('%f' % e_loss)\n",
    "    val_loss = 0\n",
    "    model.eval()\n",
    "    loss_list = []\n",
    "    pbar = tqdm(range(0, len(fore_valid_op)-32, 32))  # len(fore_valid_op)           ####################   maybe also batch_size instead of 32\n",
    "    for start in pbar:\n",
    "        #print(start, start+batch_size)\n",
    "        output = model(*[torch.tensor(ip[start:start+batch_size], dtype=torch.float32).cuda() \n",
    "                         if i < 3 else torch.tensor(ip[start:start+batch_size], dtype=torch.int32).cuda() \n",
    "                         for i, ip in enumerate(fore_valid_ip)], trainn=False, tgt=torch.tensor(fore_valid_op[start:start+batch_size], dtype=torch.float32).cuda())\n",
    "        loss = torch.tensor(fore_valid_op[start:start+batch_size,:, 131:], dtype=torch.float32).cuda()*loss_func(\n",
    "            output, torch.tensor(fore_valid_op[start:start+batch_size,:, :131], dtype=torch.float32).cuda())\n",
    "        loss_list.extend(loss.sum(axis=-1).mean(axis=-1).detach().cpu().tolist())\n",
    "        loss = loss.sum(axis=-1).mean()\n",
    "        val_loss += loss.detach().cpu()\n",
    "        pbar.set_description('%f' % val_loss)\n",
    "    loss_p = e_loss*batch_size/samples_per_epoch\n",
    "    val_loss_p = val_loss*batch_size/len(fore_valid_op)\n",
    "    print('Epoch', e, 'loss', loss_p, 'val loss', val_loss_p)\n",
    "    with open('loss_values_log', 'a') as f:\n",
    "        f.write(str(e)+' ' + str(loss_p.item()) + ' ' + str(val_loss_p.item())+ '\\n')\n",
    "    scheduler.step(val_loss_p.item())\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), fore_savepath)\n",
    "        best_epoch = e\n",
    "    if (e-best_epoch) > patience:\n",
    "        break\n",
    "print('Training has ended.')\n",
    "#STRATS, IMS, Sampling, No Detach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d20217",
   "metadata": {},
   "source": [
    "# Scheduled Sampling DI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e3029e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVE(nn.Module):\n",
    "    def __init__(self, hid_units, output_dim):\n",
    "        super(CVE, self).__init__()\n",
    "        self.hid_units = hid_units\n",
    "        self.output_dim = output_dim\n",
    "        self.W1 = nn.Parameter(torch.Tensor(1, self.hid_units))\n",
    "        self.b1 = nn.Parameter(torch.Tensor(self.hid_units))\n",
    "        self.W2 = nn.Parameter(torch.Tensor(self.hid_units, self.output_dim))\n",
    "        nn.init.xavier_uniform_(self.W1)\n",
    "        nn.init.zeros_(self.b1)\n",
    "        nn.init.xavier_uniform_(self.W2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(-1)\n",
    "        x = torch.matmul(torch.tanh(torch.add(torch.matmul(x, self.W1), self.b1)), self.W2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hid_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.hid_dim = hid_dim\n",
    "        self.W = nn.Parameter(torch.Tensor(50, self.hid_dim))\n",
    "        self.b = nn.Parameter(torch.Tensor(self.hid_dim))\n",
    "        self.u = nn.Parameter(torch.Tensor(self.hid_dim, 1))\n",
    "        nn.init.xavier_uniform_(self.W)\n",
    "        nn.init.zeros_(self.b)\n",
    "        nn.init.xavier_uniform_(self.u)\n",
    "\n",
    "    def forward(self, x, mask, mask_value=-1e30):\n",
    "        attn_weights = torch.matmul(torch.tanh(torch.add(torch.matmul(x, self.W), self.b)), self.u)\n",
    "        mask = mask.unsqueeze(-1)\n",
    "        attn_weights = mask * attn_weights + (1 - mask) * mask_value\n",
    "        attn_weights = F.softmax(attn_weights, dim=-2)\n",
    "        return attn_weights\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, N=2, h=8, dk=None, dv=None, dff=None, dropout=0, d=8):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.N, self.h, self.dk, self.dv, self.dff, self.dropout = N, h, dk, dv, dff, dropout\n",
    "        self.epsilon = torch.finfo(torch.float32).eps * torch.finfo(torch.float32).eps\n",
    "        self.Wq = nn.Parameter(torch.Tensor(N, h, d, dk))\n",
    "        self.Wk = nn.Parameter(torch.Tensor(N, h, d, dk))\n",
    "        self.Wv = nn.Parameter(torch.Tensor(N, h, d, dv))\n",
    "        self.Wo = nn.Parameter(torch.Tensor(N, dv * h, d))\n",
    "        self.W1 = nn.Parameter(torch.Tensor(N, d, dff))\n",
    "        self.b1 = nn.Parameter(torch.Tensor(N, dff))\n",
    "        self.W2 = nn.Parameter(torch.Tensor(N, dff, d))\n",
    "        self.b2 = nn.Parameter(torch.Tensor(N, d))\n",
    "        self.gamma = nn.Parameter(torch.Tensor(2 * N))\n",
    "        self.beta = nn.Parameter(torch.Tensor(2 * N))\n",
    "        nn.init.xavier_uniform_(self.Wq)\n",
    "        nn.init.xavier_uniform_(self.Wk)\n",
    "        nn.init.xavier_uniform_(self.Wv)\n",
    "        nn.init.xavier_uniform_(self.Wo)\n",
    "        nn.init.xavier_uniform_(self.W1)\n",
    "        nn.init.zeros_(self.b1)\n",
    "        nn.init.xavier_uniform_(self.W2)\n",
    "        nn.init.zeros_(self.b2)\n",
    "        nn.init.ones_(self.gamma)\n",
    "        nn.init.zeros_(self.beta)\n",
    "        self.training = False\n",
    "        \n",
    "    def forward(self, x, mask, mask_value=-1e-30):\n",
    "        mask = mask.unsqueeze(-2)\n",
    "        for i in range(self.N):\n",
    "            # MHA\n",
    "            mha_ops = []\n",
    "            for j in range(self.h):\n",
    "                q = torch.matmul(x, self.Wq[i, j, :, :])          \n",
    "                k = torch.matmul(x, self.Wk[i, j, :, :]).permute(0, 2, 1)\n",
    "                v = torch.matmul(x, self.Wv[i, j, :, :])\n",
    "                A = torch.matmul(q, k)\n",
    "                # Mask unobserved steps.\n",
    "                A = mask * A + (1 - mask) * mask_value\n",
    "                # Mask for attention dropout.\n",
    "                if self.training:\n",
    "                    dp_mask = (torch.rand(A.shape, device=A.device) >= self.dropout).float()\n",
    "                    A = A * dp_mask + (1 - dp_mask) * mask_value\n",
    "                A = F.softmax(A, dim=-1)\n",
    "                mha_ops.append(torch.matmul(A, v))\n",
    "            conc = torch.cat(mha_ops, dim=-1)\n",
    "            proj = torch.matmul(conc, self.Wo[i, :, :])\n",
    "            # Dropout.\n",
    "            if self.training:\n",
    "                proj = F.dropout(proj, p=self.dropout)\n",
    "            # Add & LN\n",
    "            x = x + proj\n",
    "            mean = x.mean(dim=-1, keepdim=True)\n",
    "            variance = torch.mean(torch.square(x - mean), dim=-1, keepdim=True)\n",
    "            std = torch.sqrt(variance + self.epsilon)\n",
    "            x = (x - mean) / std\n",
    "            x = x * self.gamma[2 * i] + self.beta[2 * i]\n",
    "            # FFN\n",
    "            ffn_op = torch.add(torch.matmul(F.relu(torch.add(\n",
    "                torch.matmul(x, self.W1[i, :, :]), self.b1[i, :])),\n",
    "                                            self.W2[i, :, :]), self.b2[i, :])\n",
    "            # Dropout.\n",
    "            if self.training:\n",
    "                ffn_op = F.dropout(ffn_op, p=self.dropout)\n",
    "            # Add & LN\n",
    "            x = x + ffn_op\n",
    "            mean = x.mean(dim=-1, keepdim=True)\n",
    "            variance = torch.mean(torch.square(x - mean), dim=-1, keepdim=True)\n",
    "            std = torch.sqrt(variance + self.epsilon)\n",
    "            x = (x - mean) / std\n",
    "            x = x * self.gamma[2 * i + 1] + self.beta[2 * i + 1]\n",
    "        return x\n",
    "\n",
    "\n",
    "class StratsModel(nn.Module):\n",
    "    def __init__(self, D, max_len, V, d, N, he, dropout, forecast=False, autoregressive=True):\n",
    "        super(StratsModel, self).__init__()\n",
    "        self.forecast = forecast\n",
    "        self.autoregressive=autoregressive\n",
    "        self.demo_enc = nn.Sequential(nn.Linear(D, 2 * d),\n",
    "                                      nn.Tanh(),\n",
    "                                      nn.Linear(2 * d, d),\n",
    "                                      nn.Tanh())\n",
    "        self.varis_emb = nn.Embedding(V + 1, d)\n",
    "        nn.init.uniform_(self.varis_emb.weight, a=-0.05, b=0.05)\n",
    "        self.values_emb = CVE(int(np.sqrt(d)), d)\n",
    "        self.times_emb = CVE(int(np.sqrt(d)), d)\n",
    "        # self.comb_emb = nn.LayerNorm(d)\n",
    "        self.transformer = Transformer(N, he, dk=d//he, dv=d//he, dff=2*d, dropout=dropout, d=d)\n",
    "\n",
    "        if autoregressive:\n",
    "            self.linearize = nn.Linear(50, 131)\n",
    "            self.dec = nn.TransformerDecoderLayer(d_model=131, dim_feedforward=131, nhead=1, batch_first=True)\n",
    "        else:\n",
    "            self.attention = Attention(2 * d)\n",
    "            self.attentions = nn.ModuleList([Attention(2*d) for i in range(24)])\n",
    "            # self.concat = nn.Linear(2 * d, 1)\n",
    "            self.fore_op_calc = nn.Linear(2 * d, V)\n",
    "            self.op_calc = nn.Linear(V, 1)\n",
    "            self.sig = nn.Sigmoid()        \n",
    "        \n",
    "\n",
    "    def forward(self, demo, times, values, varis, trainn=False, tgt=torch.zeros(32, 1, 131).cuda(), schedule=True, probability=0.5, deterministic=False):\n",
    "        demo_enc = self.demo_enc(demo)\n",
    "        varis_emb = self.varis_emb(varis)\n",
    "        values_emb = self.values_emb(values)\n",
    "        times_emb = self.times_emb(times)\n",
    "\n",
    "        comb_emb = (torch.sum(torch.stack([varis_emb, values_emb, times_emb]), dim=0))\n",
    "        self.trainn = trainn\n",
    "        mask = torch.clip(varis, 0, 1)\n",
    "        cont_emb = self.transformer(comb_emb, mask)\n",
    "        \n",
    "        \n",
    "        if trainn and schedule:\n",
    "            \n",
    "            output = torch.zeros(cont_emb.size(0), 1, 131).cuda()\n",
    "            output.requires_grad=True\n",
    "            linear_memory  = self.linearize(cont_emb)\n",
    "            for i in range(24):\n",
    "                #print(i)\n",
    "                #print(linear_memory.size(), output.size())\n",
    "                res = self.dec(output.detach(), linear_memory)\n",
    "                if deterministic:\n",
    "                    sample = i/24\n",
    "                else:\n",
    "                    sample = torch.rand(1).item()\n",
    "                #print(res.size(), output.size())\n",
    "                print(sample, probability)\n",
    "                if sample < probability:\n",
    "                \n",
    "                    \n",
    "                    output = torch.concat([output, tgt[:,i:i+1,:]], dim=1)\n",
    "                else:\n",
    "                    output = torch.concat([output, res[:,-1:,:]], dim=1)\n",
    "            return output[:, 1:, :] \n",
    "        if self.autoregressive:\n",
    "            if self.trainn:\n",
    "                #print(cont_emb.size())\n",
    "                \n",
    "                mask_new = torch.nn.Transformer.generate_square_subsequent_mask(25).cuda()\n",
    "                linear_memory = self.linearize(cont_emb)\n",
    "                start_input = torch.zeros(32, 1, 131).cuda()\n",
    "                tgt = torch.concat([start_input, tgt], dim=1)\n",
    "                result=self.dec(tgt, linear_memory, mask_new)\n",
    "                return result[:, 1:, :]\n",
    "            if not self.trainn:\n",
    "                output = torch.zeros(32, 1, 131).cuda()\n",
    "                linear_memory  = self.linearize(cont_emb)\n",
    "                for i in range(24):\n",
    "                    #print(i)\n",
    "                    #print(linear_memory.size(), output.size())\n",
    "                    res = self.dec(output.detach(), linear_memory)\n",
    "                    #print(res.size(), output.size())\n",
    "                    output = torch.concat([output, res[:,-1:,:]], dim=1)\n",
    "                return output[:, 1:, :]\n",
    "                \n",
    "        if not self.autoregressive:\n",
    "            fused_embs = []\n",
    "            for attention in self.attentions:\n",
    "                attn_weights = attention(cont_emb, mask)\n",
    "                fused_emb = torch.matmul(attn_weights.transpose(-1, -2), cont_emb).squeeze(-2)\n",
    "                # fused_emb torch.Size([32, 50])\n",
    "                fused_embs.append(fused_emb)\n",
    "            # fused_embs Liste mit 6 torch.Size([32, 50])\n",
    "            fused_emb = torch.stack(fused_embs, axis=1)\n",
    "            # altes fused_emb: torch.Size([32, 50])\n",
    "            # neues fused_emb: torch.Size([32, 6, 50])\n",
    "            # demo_enc torch.Size([32, 50])\n",
    "\n",
    "            # demo_enc.unsqueeze(axis=-2).repeat(1, 6, 1):  torch.Size([32, 6, 50])\n",
    "            #print(fused_emb.size())\n",
    "            concat = torch.cat([fused_emb, demo_enc.unsqueeze(axis=-2).repeat(1, 24, 1)], axis=-1)\n",
    "            #concat torch.Size([32, 6, 100])\n",
    "\n",
    "            x = self.fore_op_calc(concat)\n",
    "            # neues x torch.Size([32, 6, 258])\n",
    "            # altes x: torch.Size([32, 258])\n",
    "            if self.forecast:\n",
    "                return x\n",
    "            else:\n",
    "                x = self.sig(self.op_calc(x)).mean(axis=-2)\n",
    "                return x\n",
    "\n",
    "print(\"Model definitions loaded\")\n",
    "\n",
    "lr, batch_size, samples_per_epoch, patience = 0.0005, 32, int(102400/sample_divisor), 6\n",
    "d, N, he, dropout = 50, 2, 4, 0.2\n",
    "print('number of parameters: ', V)\n",
    "\n",
    "model = StratsModel(2, fore_max_len, V, d, N, he, dropout, forecast=True).cuda()\n",
    "a = summary(model, [(32, 2), (32, 880), (32, 880), (32, 880)],  # shape of fore_train_ip\n",
    "            dtypes=[torch.float, torch.float, torch.float, torch.long])\n",
    "print(a)  # Model summary\n",
    "# raise Exception\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=3)\n",
    "# Pretrain fore_model.\n",
    "best_val_loss = np.inf\n",
    "N_fore = len(fore_train_op)\n",
    "#print(N_fore)\n",
    "fore_savepath = 'mimic_iii_sparse_ims_sampling_detach_schedule_dtf.pytorch'\n",
    "loss_func = torch.nn.MSELoss(reduction=\"none\")\n",
    "\n",
    "# torch.compile(model)\n",
    "for e in range(number_of_epochs):\n",
    "    e_indices = np.random.choice(range(N_fore), size=samples_per_epoch, replace=False)\n",
    "    e_loss = 0\n",
    "    pbar = tqdm(range(0, len(e_indices), batch_size))\n",
    "    model.train()\n",
    "    for start in pbar:\n",
    "        ind = e_indices[start:start+batch_size]\n",
    "        # raise Exception\n",
    "        prob=min([1.0, 1.0+(0.25-1.0)*(1-e/200)])\n",
    "        prob = max([0.25, 0.25+(1.0-0.25)*(1-e/200)]) #DTF\n",
    "        \n",
    "        output = model(*[torch.tensor(ip[ind], dtype=torch.float32).cuda() if i < 3 \n",
    "                         else torch.tensor(ip[ind], dtype=torch.int32).cuda() \n",
    "                         for i, ip in enumerate(fore_train_ip)], trainn=True, tgt=torch.tensor(fore_train_op[ind, :, :131], dtype=torch.float32).cuda(), schedule=True, deterministic=True, probability=prob)\n",
    "        #raise Exception\n",
    "        #print(fore_train_op[ind].shape)\n",
    "        #raise Exception\n",
    "        loss = torch.tensor(fore_train_op[ind, :, 131:], dtype=torch.float32).cuda()*(\n",
    "            output-torch.tensor(fore_train_op[ind, :, :131], dtype=torch.float32).cuda())**2\n",
    "        loss = loss.sum(axis=-1).mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        e_loss += loss.detach()\n",
    "        pbar.set_description('%f' % e_loss)\n",
    "    val_loss = 0\n",
    "    model.eval()\n",
    "    loss_list = []\n",
    "    pbar = tqdm(range(0, len(fore_valid_op)-32, 32))  # len(fore_valid_op)           ####################   maybe also batch_size instead of 32\n",
    "    for start in pbar:\n",
    "        #print(start, start+batch_size)\n",
    "        output = model(*[torch.tensor(ip[start:start+batch_size], dtype=torch.float32).cuda() \n",
    "                         if i < 3 else torch.tensor(ip[start:start+batch_size], dtype=torch.int32).cuda() \n",
    "                         for i, ip in enumerate(fore_valid_ip)], trainn=False, tgt=torch.tensor(fore_valid_op[start:start+batch_size], dtype=torch.float32).cuda())\n",
    "        loss = torch.tensor(fore_valid_op[start:start+batch_size,:, 131:], dtype=torch.float32).cuda()*loss_func(\n",
    "            output, torch.tensor(fore_valid_op[start:start+batch_size,:, :131], dtype=torch.float32).cuda())\n",
    "        loss_list.extend(loss.sum(axis=-1).mean(axis=-1).detach().cpu().tolist())\n",
    "        loss = loss.sum(axis=-1).mean()\n",
    "        val_loss += loss.detach().cpu()\n",
    "        pbar.set_description('%f' % val_loss)\n",
    "    loss_p = e_loss*batch_size/samples_per_epoch\n",
    "    val_loss_p = val_loss*batch_size/len(fore_valid_op)\n",
    "    print('Epoch', e, 'loss', loss_p, 'val loss', val_loss_p, \"mean and std\", np.mean(loss_list), np.std(loss_list))\n",
    "    with open('loss_values_log', 'a') as f:\n",
    "        f.write(str(e)+' ' + str(loss_p.item()) + ' ' + str(val_loss_p.item())+ '\\n')\n",
    "    scheduler.step(val_loss_p.item())\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), fore_savepath)\n",
    "        best_epoch = e\n",
    "    if (e-best_epoch) > patience:\n",
    "        break\n",
    "print('Training has ended.')\n",
    "\n",
    "#STRATS, IMS, Sampling, Detach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1df1ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVE(nn.Module):\n",
    "    def __init__(self, hid_units, output_dim):\n",
    "        super(CVE, self).__init__()\n",
    "        self.hid_units = hid_units\n",
    "        self.output_dim = output_dim\n",
    "        self.W1 = nn.Parameter(torch.Tensor(1, self.hid_units))\n",
    "        self.b1 = nn.Parameter(torch.Tensor(self.hid_units))\n",
    "        self.W2 = nn.Parameter(torch.Tensor(self.hid_units, self.output_dim))\n",
    "        nn.init.xavier_uniform_(self.W1)\n",
    "        nn.init.zeros_(self.b1)\n",
    "        nn.init.xavier_uniform_(self.W2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(-1)\n",
    "        x = torch.matmul(torch.tanh(torch.add(torch.matmul(x, self.W1), self.b1)), self.W2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hid_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.hid_dim = hid_dim\n",
    "        self.W = nn.Parameter(torch.Tensor(50, self.hid_dim))\n",
    "        self.b = nn.Parameter(torch.Tensor(self.hid_dim))\n",
    "        self.u = nn.Parameter(torch.Tensor(self.hid_dim, 1))\n",
    "        nn.init.xavier_uniform_(self.W)\n",
    "        nn.init.zeros_(self.b)\n",
    "        nn.init.xavier_uniform_(self.u)\n",
    "\n",
    "    def forward(self, x, mask, mask_value=-1e30):\n",
    "        attn_weights = torch.matmul(torch.tanh(torch.add(torch.matmul(x, self.W), self.b)), self.u)\n",
    "        mask = mask.unsqueeze(-1)\n",
    "        attn_weights = mask * attn_weights + (1 - mask) * mask_value\n",
    "        attn_weights = F.softmax(attn_weights, dim=-2)\n",
    "        return attn_weights\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, N=2, h=8, dk=None, dv=None, dff=None, dropout=0, d=8):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.N, self.h, self.dk, self.dv, self.dff, self.dropout = N, h, dk, dv, dff, dropout\n",
    "        self.epsilon = torch.finfo(torch.float32).eps * torch.finfo(torch.float32).eps\n",
    "        self.Wq = nn.Parameter(torch.Tensor(N, h, d, dk))\n",
    "        self.Wk = nn.Parameter(torch.Tensor(N, h, d, dk))\n",
    "        self.Wv = nn.Parameter(torch.Tensor(N, h, d, dv))\n",
    "        self.Wo = nn.Parameter(torch.Tensor(N, dv * h, d))\n",
    "        self.W1 = nn.Parameter(torch.Tensor(N, d, dff))\n",
    "        self.b1 = nn.Parameter(torch.Tensor(N, dff))\n",
    "        self.W2 = nn.Parameter(torch.Tensor(N, dff, d))\n",
    "        self.b2 = nn.Parameter(torch.Tensor(N, d))\n",
    "        self.gamma = nn.Parameter(torch.Tensor(2 * N))\n",
    "        self.beta = nn.Parameter(torch.Tensor(2 * N))\n",
    "        nn.init.xavier_uniform_(self.Wq)\n",
    "        nn.init.xavier_uniform_(self.Wk)\n",
    "        nn.init.xavier_uniform_(self.Wv)\n",
    "        nn.init.xavier_uniform_(self.Wo)\n",
    "        nn.init.xavier_uniform_(self.W1)\n",
    "        nn.init.zeros_(self.b1)\n",
    "        nn.init.xavier_uniform_(self.W2)\n",
    "        nn.init.zeros_(self.b2)\n",
    "        nn.init.ones_(self.gamma)\n",
    "        nn.init.zeros_(self.beta)\n",
    "        self.training = False\n",
    "        \n",
    "    def forward(self, x, mask, mask_value=-1e-30):\n",
    "        mask = mask.unsqueeze(-2)\n",
    "        for i in range(self.N):\n",
    "            # MHA\n",
    "            mha_ops = []\n",
    "            for j in range(self.h):\n",
    "                q = torch.matmul(x, self.Wq[i, j, :, :])          \n",
    "                k = torch.matmul(x, self.Wk[i, j, :, :]).permute(0, 2, 1)\n",
    "                v = torch.matmul(x, self.Wv[i, j, :, :])\n",
    "                A = torch.matmul(q, k)\n",
    "                # Mask unobserved steps.\n",
    "                A = mask * A + (1 - mask) * mask_value\n",
    "                # Mask for attention dropout.\n",
    "                if self.training:\n",
    "                    dp_mask = (torch.rand(A.shape, device=A.device) >= self.dropout).float()\n",
    "                    A = A * dp_mask + (1 - dp_mask) * mask_value\n",
    "                A = F.softmax(A, dim=-1)\n",
    "                mha_ops.append(torch.matmul(A, v))\n",
    "            conc = torch.cat(mha_ops, dim=-1)\n",
    "            proj = torch.matmul(conc, self.Wo[i, :, :])\n",
    "            # Dropout.\n",
    "            if self.training:\n",
    "                proj = F.dropout(proj, p=self.dropout)\n",
    "            # Add & LN\n",
    "            x = x + proj\n",
    "            mean = x.mean(dim=-1, keepdim=True)\n",
    "            variance = torch.mean(torch.square(x - mean), dim=-1, keepdim=True)\n",
    "            std = torch.sqrt(variance + self.epsilon)\n",
    "            x = (x - mean) / std\n",
    "            x = x * self.gamma[2 * i] + self.beta[2 * i]\n",
    "            # FFN\n",
    "            ffn_op = torch.add(torch.matmul(F.relu(torch.add(\n",
    "                torch.matmul(x, self.W1[i, :, :]), self.b1[i, :])),\n",
    "                                            self.W2[i, :, :]), self.b2[i, :])\n",
    "            # Dropout.\n",
    "            if self.training:\n",
    "                ffn_op = F.dropout(ffn_op, p=self.dropout)\n",
    "            # Add & LN\n",
    "            x = x + ffn_op\n",
    "            mean = x.mean(dim=-1, keepdim=True)\n",
    "            variance = torch.mean(torch.square(x - mean), dim=-1, keepdim=True)\n",
    "            std = torch.sqrt(variance + self.epsilon)\n",
    "            x = (x - mean) / std\n",
    "            x = x * self.gamma[2 * i + 1] + self.beta[2 * i + 1]\n",
    "        return x\n",
    "\n",
    "\n",
    "class StratsModel(nn.Module):\n",
    "    def __init__(self, D, max_len, V, d, N, he, dropout, forecast=False, autoregressive=True):\n",
    "        super(StratsModel, self).__init__()\n",
    "        self.forecast = forecast\n",
    "        self.autoregressive=autoregressive\n",
    "        self.demo_enc = nn.Sequential(nn.Linear(D, 2 * d),\n",
    "                                      nn.Tanh(),\n",
    "                                      nn.Linear(2 * d, d),\n",
    "                                      nn.Tanh())\n",
    "        self.varis_emb = nn.Embedding(V + 1, d)\n",
    "        nn.init.uniform_(self.varis_emb.weight, a=-0.05, b=0.05)\n",
    "        self.values_emb = CVE(int(np.sqrt(d)), d)\n",
    "        self.times_emb = CVE(int(np.sqrt(d)), d)\n",
    "        # self.comb_emb = nn.LayerNorm(d)\n",
    "        self.transformer = Transformer(N, he, dk=d//he, dv=d//he, dff=2*d, dropout=dropout, d=d)\n",
    "\n",
    "        if autoregressive:\n",
    "            self.linearize = nn.Linear(50, 131)\n",
    "            self.dec = nn.TransformerDecoderLayer(d_model=131, dim_feedforward=131, nhead=1, batch_first=True)\n",
    "        else:\n",
    "            self.attention = Attention(2 * d)\n",
    "            self.attentions = nn.ModuleList([Attention(2*d) for i in range(24)])\n",
    "            # self.concat = nn.Linear(2 * d, 1)\n",
    "            self.fore_op_calc = nn.Linear(2 * d, V)\n",
    "            self.op_calc = nn.Linear(V, 1)\n",
    "            self.sig = nn.Sigmoid()        \n",
    "        \n",
    "\n",
    "    def forward(self, demo, times, values, varis, trainn=False, tgt=torch.zeros(32, 1, 131).cuda(), schedule=True, probability=0.5, deterministic=False):\n",
    "        demo_enc = self.demo_enc(demo)\n",
    "        varis_emb = self.varis_emb(varis)\n",
    "        values_emb = self.values_emb(values)\n",
    "        times_emb = self.times_emb(times)\n",
    "\n",
    "        comb_emb = (torch.sum(torch.stack([varis_emb, values_emb, times_emb]), dim=0))\n",
    "        self.trainn = trainn\n",
    "        mask = torch.clip(varis, 0, 1)\n",
    "        cont_emb = self.transformer(comb_emb, mask)\n",
    "        \n",
    "        if trainn and schedule:\n",
    "            output = torch.zeros(cont_emb.size(0), 1, 131).cuda()\n",
    "            output.requires_grad=True\n",
    "            linear_memory  = self.linearize(cont_emb)\n",
    "            for i in range(24):\n",
    "                #print(i)\n",
    "                #print(linear_memory.size(), output.size())\n",
    "                res = self.dec(output, linear_memory)\n",
    "                if deterministic:\n",
    "                    sample = i/24\n",
    "                else:\n",
    "                    sample = torch.rand(1).item()\n",
    "                #print(res.size(), output.size())\n",
    "                print(sample, probability)\n",
    "                if sample < probability:\n",
    "                \n",
    "                    \n",
    "                    output = torch.concat([output, tgt[:,i:i+1,:]], dim=1)\n",
    "                else:\n",
    "                    output = torch.concat([output, res[:,-1:,:]], dim=1)\n",
    "            return output[:, 1:, :] \n",
    "        \n",
    "        if self.autoregressive:\n",
    "            if self.trainn:\n",
    "                #print(cont_emb.size())\n",
    "                \n",
    "                mask_new = torch.nn.Transformer.generate_square_subsequent_mask(25).cuda()\n",
    "                linear_memory = self.linearize(cont_emb)\n",
    "                start_input = torch.zeros(32, 1, 131).cuda()\n",
    "                tgt = torch.concat([start_input, tgt], dim=1)\n",
    "                result=self.dec(tgt, linear_memory, mask_new)\n",
    "                return result[:, 1:, :]\n",
    "            if not self.trainn:\n",
    "                output = torch.zeros(32, 1, 131).cuda()\n",
    "                linear_memory  = self.linearize(cont_emb)\n",
    "                for i in range(24):\n",
    "                    #print(i)\n",
    "                    #print(linear_memory.size(), output.size())\n",
    "                    res = self.dec(output, linear_memory)\n",
    "                    #print(res.size(), output.size())\n",
    "                    output = torch.concat([output, res[:,-1:,:]], dim=1)\n",
    "                return output[:, 1:, :]\n",
    "                \n",
    "        if not self.autoregressive:\n",
    "            fused_embs = []\n",
    "            for attention in self.attentions:\n",
    "                attn_weights = attention(cont_emb, mask)\n",
    "                fused_emb = torch.matmul(attn_weights.transpose(-1, -2), cont_emb).squeeze(-2)\n",
    "                # fused_emb torch.Size([32, 50])\n",
    "                fused_embs.append(fused_emb)\n",
    "            # fused_embs Liste mit 6 torch.Size([32, 50])\n",
    "            fused_emb = torch.stack(fused_embs, axis=1)\n",
    "            # altes fused_emb: torch.Size([32, 50])\n",
    "            # neues fused_emb: torch.Size([32, 6, 50])\n",
    "            # demo_enc torch.Size([32, 50])\n",
    "\n",
    "            # demo_enc.unsqueeze(axis=-2).repeat(1, 6, 1):  torch.Size([32, 6, 50])\n",
    "            #print(fused_emb.size())\n",
    "            concat = torch.cat([fused_emb, demo_enc.unsqueeze(axis=-2).repeat(1, 24, 1)], axis=-1)\n",
    "            #concat torch.Size([32, 6, 100])\n",
    "\n",
    "            x = self.fore_op_calc(concat)\n",
    "            # neues x torch.Size([32, 6, 258])\n",
    "            # altes x: torch.Size([32, 258])\n",
    "            if self.forecast:\n",
    "                return x\n",
    "            else:\n",
    "                x = self.sig(self.op_calc(x)).mean(axis=-2)\n",
    "                return x\n",
    "\n",
    "print(\"Model definitions loaded\")\n",
    "\n",
    "lr, batch_size, samples_per_epoch, patience = 0.0005, 32, int(102400/sample_divisor), 6\n",
    "d, N, he, dropout = 50, 2, 4, 0.2\n",
    "print('number of parameters: ', V)\n",
    "\n",
    "model = StratsModel(2, fore_max_len, V, d, N, he, dropout, forecast=True).cuda()\n",
    "a = summary(model, [(32, 2), (32, 880), (32, 880), (32, 880)],  # shape of fore_train_ip\n",
    "            dtypes=[torch.float, torch.float, torch.float, torch.long])\n",
    "print(a)  # Model summary\n",
    "# raise Exception\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=3)\n",
    "# Pretrain fore_model.\n",
    "best_val_loss = np.inf\n",
    "N_fore = len(fore_train_op)\n",
    "#print(N_fore)\n",
    "fore_savepath = 'mimic_iii_sparse_ims_sampling_backprop_schedule_dtf.pytorch'\n",
    "loss_func = torch.nn.MSELoss(reduction=\"none\")\n",
    "\n",
    "# torch.compile(model)\n",
    "for e in range(number_of_epochs):\n",
    "    e_indices = np.random.choice(range(N_fore), size=samples_per_epoch, replace=False)\n",
    "    e_loss = 0\n",
    "    pbar = tqdm(range(0, len(e_indices), batch_size))\n",
    "    model.train()\n",
    "    for start in pbar:\n",
    "        ind = e_indices[start:start+batch_size]\n",
    "        # raise Exception\n",
    "        prob=min([1.0, 1.0+(0.25-1.0)*(1-e/200)])\n",
    "        prob = max([0.25, 0.25+(1.0-0.25)*(1-e/200)]) #DTF\n",
    "        output = model(*[torch.tensor(ip[ind], dtype=torch.float32).cuda() if i < 3 \n",
    "                         else torch.tensor(ip[ind], dtype=torch.int32).cuda() \n",
    "                         for i, ip in enumerate(fore_train_ip)], trainn=True, tgt=torch.tensor(fore_train_op[ind, :, :131], dtype=torch.float32).cuda(), schedule=True, deterministic=True, probability=prob)\n",
    "        #raise Exception\n",
    "        #print(fore_train_op[ind].shape)\n",
    "        #raise Exception\n",
    "        loss = torch.tensor(fore_train_op[ind, :, 131:], dtype=torch.float32).cuda()*(\n",
    "            output-torch.tensor(fore_train_op[ind, :, :131], dtype=torch.float32).cuda())**2\n",
    "        loss = loss.sum(axis=-1).mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        e_loss += loss.detach()\n",
    "        pbar.set_description('%f' % e_loss)\n",
    "    val_loss = 0\n",
    "    model.eval()\n",
    "    loss_list = []\n",
    "    pbar = tqdm(range(0, len(fore_valid_op)-32, 32))  # len(fore_valid_op)           ####################   maybe also batch_size instead of 32\n",
    "    for start in pbar:\n",
    "        #print(start, start+batch_size)\n",
    "        output = model(*[torch.tensor(ip[start:start+batch_size], dtype=torch.float32).cuda() \n",
    "                         if i < 3 else torch.tensor(ip[start:start+batch_size], dtype=torch.int32).cuda() \n",
    "                         for i, ip in enumerate(fore_valid_ip)], trainn=False, tgt=torch.tensor(fore_valid_op[start:start+batch_size], dtype=torch.float32).cuda())\n",
    "        loss = torch.tensor(fore_valid_op[start:start+batch_size,:, 131:], dtype=torch.float32).cuda()*loss_func(\n",
    "            output, torch.tensor(fore_valid_op[start:start+batch_size,:, :131], dtype=torch.float32).cuda())\n",
    "        loss_list.extend(loss.sum(axis=-1).mean(axis=-1).detach().cpu().tolist())\n",
    "        loss = loss.sum(axis=-1).mean()\n",
    "        val_loss += loss.detach().cpu()\n",
    "        pbar.set_description('%f' % val_loss)\n",
    "    loss_p = e_loss*batch_size/samples_per_epoch\n",
    "    val_loss_p = val_loss*batch_size/len(fore_valid_op)\n",
    "    print('Epoch', e, 'loss', loss_p, 'val loss', val_loss_p)\n",
    "    with open('loss_values_log', 'a') as f:\n",
    "        f.write(str(e)+' ' + str(loss_p.item()) + ' ' + str(val_loss_p.item())+ '\\n')\n",
    "    scheduler.step(val_loss_p.item())\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), fore_savepath)\n",
    "        best_epoch = e\n",
    "    if (e-best_epoch) > patience:\n",
    "        break\n",
    "print('Training has ended.')\n",
    "#STRATS, IMS, Sampling, No Detach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc67798",
   "metadata": {},
   "source": [
    "# Scheduled Sampling RI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350a1f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVE(nn.Module):\n",
    "    def __init__(self, hid_units, output_dim):\n",
    "        super(CVE, self).__init__()\n",
    "        self.hid_units = hid_units\n",
    "        self.output_dim = output_dim\n",
    "        self.W1 = nn.Parameter(torch.Tensor(1, self.hid_units))\n",
    "        self.b1 = nn.Parameter(torch.Tensor(self.hid_units))\n",
    "        self.W2 = nn.Parameter(torch.Tensor(self.hid_units, self.output_dim))\n",
    "        nn.init.xavier_uniform_(self.W1)\n",
    "        nn.init.zeros_(self.b1)\n",
    "        nn.init.xavier_uniform_(self.W2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(-1)\n",
    "        x = torch.matmul(torch.tanh(torch.add(torch.matmul(x, self.W1), self.b1)), self.W2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hid_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.hid_dim = hid_dim\n",
    "        self.W = nn.Parameter(torch.Tensor(50, self.hid_dim))\n",
    "        self.b = nn.Parameter(torch.Tensor(self.hid_dim))\n",
    "        self.u = nn.Parameter(torch.Tensor(self.hid_dim, 1))\n",
    "        nn.init.xavier_uniform_(self.W)\n",
    "        nn.init.zeros_(self.b)\n",
    "        nn.init.xavier_uniform_(self.u)\n",
    "\n",
    "    def forward(self, x, mask, mask_value=-1e30):\n",
    "        attn_weights = torch.matmul(torch.tanh(torch.add(torch.matmul(x, self.W), self.b)), self.u)\n",
    "        mask = mask.unsqueeze(-1)\n",
    "        attn_weights = mask * attn_weights + (1 - mask) * mask_value\n",
    "        attn_weights = F.softmax(attn_weights, dim=-2)\n",
    "        return attn_weights\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, N=2, h=8, dk=None, dv=None, dff=None, dropout=0, d=8):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.N, self.h, self.dk, self.dv, self.dff, self.dropout = N, h, dk, dv, dff, dropout\n",
    "        self.epsilon = torch.finfo(torch.float32).eps * torch.finfo(torch.float32).eps\n",
    "        self.Wq = nn.Parameter(torch.Tensor(N, h, d, dk))\n",
    "        self.Wk = nn.Parameter(torch.Tensor(N, h, d, dk))\n",
    "        self.Wv = nn.Parameter(torch.Tensor(N, h, d, dv))\n",
    "        self.Wo = nn.Parameter(torch.Tensor(N, dv * h, d))\n",
    "        self.W1 = nn.Parameter(torch.Tensor(N, d, dff))\n",
    "        self.b1 = nn.Parameter(torch.Tensor(N, dff))\n",
    "        self.W2 = nn.Parameter(torch.Tensor(N, dff, d))\n",
    "        self.b2 = nn.Parameter(torch.Tensor(N, d))\n",
    "        self.gamma = nn.Parameter(torch.Tensor(2 * N))\n",
    "        self.beta = nn.Parameter(torch.Tensor(2 * N))\n",
    "        nn.init.xavier_uniform_(self.Wq)\n",
    "        nn.init.xavier_uniform_(self.Wk)\n",
    "        nn.init.xavier_uniform_(self.Wv)\n",
    "        nn.init.xavier_uniform_(self.Wo)\n",
    "        nn.init.xavier_uniform_(self.W1)\n",
    "        nn.init.zeros_(self.b1)\n",
    "        nn.init.xavier_uniform_(self.W2)\n",
    "        nn.init.zeros_(self.b2)\n",
    "        nn.init.ones_(self.gamma)\n",
    "        nn.init.zeros_(self.beta)\n",
    "        self.training = False\n",
    "        \n",
    "    def forward(self, x, mask, mask_value=-1e-30):\n",
    "        mask = mask.unsqueeze(-2)\n",
    "        for i in range(self.N):\n",
    "            # MHA\n",
    "            mha_ops = []\n",
    "            for j in range(self.h):\n",
    "                q = torch.matmul(x, self.Wq[i, j, :, :])          \n",
    "                k = torch.matmul(x, self.Wk[i, j, :, :]).permute(0, 2, 1)\n",
    "                v = torch.matmul(x, self.Wv[i, j, :, :])\n",
    "                A = torch.matmul(q, k)\n",
    "                # Mask unobserved steps.\n",
    "                A = mask * A + (1 - mask) * mask_value\n",
    "                # Mask for attention dropout.\n",
    "                if self.training:\n",
    "                    dp_mask = (torch.rand(A.shape, device=A.device) >= self.dropout).float()\n",
    "                    A = A * dp_mask + (1 - dp_mask) * mask_value\n",
    "                A = F.softmax(A, dim=-1)\n",
    "                mha_ops.append(torch.matmul(A, v))\n",
    "            conc = torch.cat(mha_ops, dim=-1)\n",
    "            proj = torch.matmul(conc, self.Wo[i, :, :])\n",
    "            # Dropout.\n",
    "            if self.training:\n",
    "                proj = F.dropout(proj, p=self.dropout)\n",
    "            # Add & LN\n",
    "            x = x + proj\n",
    "            mean = x.mean(dim=-1, keepdim=True)\n",
    "            variance = torch.mean(torch.square(x - mean), dim=-1, keepdim=True)\n",
    "            std = torch.sqrt(variance + self.epsilon)\n",
    "            x = (x - mean) / std\n",
    "            x = x * self.gamma[2 * i] + self.beta[2 * i]\n",
    "            # FFN\n",
    "            ffn_op = torch.add(torch.matmul(F.relu(torch.add(\n",
    "                torch.matmul(x, self.W1[i, :, :]), self.b1[i, :])),\n",
    "                                            self.W2[i, :, :]), self.b2[i, :])\n",
    "            # Dropout.\n",
    "            if self.training:\n",
    "                ffn_op = F.dropout(ffn_op, p=self.dropout)\n",
    "            # Add & LN\n",
    "            x = x + ffn_op\n",
    "            mean = x.mean(dim=-1, keepdim=True)\n",
    "            variance = torch.mean(torch.square(x - mean), dim=-1, keepdim=True)\n",
    "            std = torch.sqrt(variance + self.epsilon)\n",
    "            x = (x - mean) / std\n",
    "            x = x * self.gamma[2 * i + 1] + self.beta[2 * i + 1]\n",
    "        return x\n",
    "\n",
    "\n",
    "class StratsModel(nn.Module):\n",
    "    def __init__(self, D, max_len, V, d, N, he, dropout, forecast=False, autoregressive=True):\n",
    "        super(StratsModel, self).__init__()\n",
    "        self.forecast = forecast\n",
    "        self.autoregressive=autoregressive\n",
    "        self.demo_enc = nn.Sequential(nn.Linear(D, 2 * d),\n",
    "                                      nn.Tanh(),\n",
    "                                      nn.Linear(2 * d, d),\n",
    "                                      nn.Tanh())\n",
    "        self.varis_emb = nn.Embedding(V + 1, d)\n",
    "        nn.init.uniform_(self.varis_emb.weight, a=-0.05, b=0.05)\n",
    "        self.values_emb = CVE(int(np.sqrt(d)), d)\n",
    "        self.times_emb = CVE(int(np.sqrt(d)), d)\n",
    "        # self.comb_emb = nn.LayerNorm(d)\n",
    "        self.transformer = Transformer(N, he, dk=d//he, dv=d//he, dff=2*d, dropout=dropout, d=d)\n",
    "\n",
    "        if autoregressive:\n",
    "            self.linearize = nn.Linear(50, 131)\n",
    "            self.dec = nn.TransformerDecoderLayer(d_model=131, dim_feedforward=131, nhead=1, batch_first=True)\n",
    "        else:\n",
    "            self.attention = Attention(2 * d)\n",
    "            self.attentions = nn.ModuleList([Attention(2*d) for i in range(24)])\n",
    "            # self.concat = nn.Linear(2 * d, 1)\n",
    "            self.fore_op_calc = nn.Linear(2 * d, V)\n",
    "            self.op_calc = nn.Linear(V, 1)\n",
    "            self.sig = nn.Sigmoid()        \n",
    "        \n",
    "\n",
    "    def forward(self, demo, times, values, varis, trainn=False, tgt=torch.zeros(32, 1, 131).cuda(), schedule=True, probability=0.5, deterministic=False):\n",
    "        demo_enc = self.demo_enc(demo)\n",
    "        varis_emb = self.varis_emb(varis)\n",
    "        values_emb = self.values_emb(values)\n",
    "        times_emb = self.times_emb(times)\n",
    "\n",
    "        comb_emb = (torch.sum(torch.stack([varis_emb, values_emb, times_emb]), dim=0))\n",
    "        self.trainn = trainn\n",
    "        mask = torch.clip(varis, 0, 1)\n",
    "        cont_emb = self.transformer(comb_emb, mask)\n",
    "        \n",
    "        \n",
    "        if trainn and schedule:\n",
    "            \n",
    "            output = torch.zeros(cont_emb.size(0), 1, 131).cuda()\n",
    "            output.requires_grad=True\n",
    "            linear_memory  = self.linearize(cont_emb)\n",
    "            for i in range(24):\n",
    "                #print(i)\n",
    "                #print(linear_memory.size(), output.size())\n",
    "                res = self.dec(output.detach(), linear_memory)\n",
    "                if deterministic:\n",
    "                    sample = i/24\n",
    "                else:\n",
    "                    sample = torch.rand(1).item()\n",
    "                #print(res.size(), output.size())\n",
    "                print(sample, probability)\n",
    "                if sample < probability:\n",
    "                \n",
    "                    \n",
    "                    output = torch.concat([output, tgt[:,i:i+1,:]], dim=1)\n",
    "                else:\n",
    "                    output = torch.concat([output, res[:,-1:,:]], dim=1)\n",
    "            return output[:, 1:, :] \n",
    "        if self.autoregressive:\n",
    "            if self.trainn:\n",
    "                #print(cont_emb.size())\n",
    "                \n",
    "                mask_new = torch.nn.Transformer.generate_square_subsequent_mask(25).cuda()\n",
    "                linear_memory = self.linearize(cont_emb)\n",
    "                start_input = torch.zeros(32, 1, 131).cuda()\n",
    "                tgt = torch.concat([start_input, tgt], dim=1)\n",
    "                result=self.dec(tgt, linear_memory, mask_new)\n",
    "                return result[:, 1:, :]\n",
    "            if not self.trainn:\n",
    "                output = torch.zeros(32, 1, 131).cuda()\n",
    "                linear_memory  = self.linearize(cont_emb)\n",
    "                for i in range(24):\n",
    "                    #print(i)\n",
    "                    #print(linear_memory.size(), output.size())\n",
    "                    res = self.dec(output.detach(), linear_memory)\n",
    "                    #print(res.size(), output.size())\n",
    "                    output = torch.concat([output, res[:,-1:,:]], dim=1)\n",
    "                return output[:, 1:, :]\n",
    "                \n",
    "        if not self.autoregressive:\n",
    "            fused_embs = []\n",
    "            for attention in self.attentions:\n",
    "                attn_weights = attention(cont_emb, mask)\n",
    "                fused_emb = torch.matmul(attn_weights.transpose(-1, -2), cont_emb).squeeze(-2)\n",
    "                # fused_emb torch.Size([32, 50])\n",
    "                fused_embs.append(fused_emb)\n",
    "            # fused_embs Liste mit 6 torch.Size([32, 50])\n",
    "            fused_emb = torch.stack(fused_embs, axis=1)\n",
    "            # altes fused_emb: torch.Size([32, 50])\n",
    "            # neues fused_emb: torch.Size([32, 6, 50])\n",
    "            # demo_enc torch.Size([32, 50])\n",
    "\n",
    "            # demo_enc.unsqueeze(axis=-2).repeat(1, 6, 1):  torch.Size([32, 6, 50])\n",
    "            #print(fused_emb.size())\n",
    "            concat = torch.cat([fused_emb, demo_enc.unsqueeze(axis=-2).repeat(1, 24, 1)], axis=-1)\n",
    "            #concat torch.Size([32, 6, 100])\n",
    "\n",
    "            x = self.fore_op_calc(concat)\n",
    "            # neues x torch.Size([32, 6, 258])\n",
    "            # altes x: torch.Size([32, 258])\n",
    "            if self.forecast:\n",
    "                return x\n",
    "            else:\n",
    "                x = self.sig(self.op_calc(x)).mean(axis=-2)\n",
    "                return x\n",
    "\n",
    "print(\"Model definitions loaded\")\n",
    "\n",
    "lr, batch_size, samples_per_epoch, patience = 0.0005, 32, int(102400/sample_divisor), 6\n",
    "d, N, he, dropout = 50, 2, 4, 0.2\n",
    "print('number of parameters: ', V)\n",
    "\n",
    "model = StratsModel(2, fore_max_len, V, d, N, he, dropout, forecast=True).cuda()\n",
    "a = summary(model, [(32, 2), (32, 880), (32, 880), (32, 880)],  # shape of fore_train_ip\n",
    "            dtypes=[torch.float, torch.float, torch.float, torch.long])\n",
    "print(a)  # Model summary\n",
    "# raise Exception\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=3)\n",
    "# Pretrain fore_model.\n",
    "best_val_loss = np.inf\n",
    "N_fore = len(fore_train_op)\n",
    "#print(N_fore)\n",
    "fore_savepath = 'mimic_iii_sparse_ims_sampling_detach_schedule_random.pytorch'\n",
    "loss_func = torch.nn.MSELoss(reduction=\"none\")\n",
    "\n",
    "# torch.compile(model)\n",
    "for e in range(number_of_epochs):\n",
    "    e_indices = np.random.choice(range(N_fore), size=samples_per_epoch, replace=False)\n",
    "    e_loss = 0\n",
    "    pbar = tqdm(range(0, len(e_indices), batch_size))\n",
    "    model.train()\n",
    "    for start in pbar:\n",
    "        ind = e_indices[start:start+batch_size]\n",
    "        # raise Exception\n",
    "        prob=min([1.0, 1.0+(0.25-1.0)*(1-e/200)])\n",
    "        output = model(*[torch.tensor(ip[ind], dtype=torch.float32).cuda() if i < 3 \n",
    "                         else torch.tensor(ip[ind], dtype=torch.int32).cuda() \n",
    "                         for i, ip in enumerate(fore_train_ip)], trainn=True, tgt=torch.tensor(fore_train_op[ind, :, :131], dtype=torch.float32).cuda(), schedule=True, deterministic=False, probability=prob)\n",
    "        #raise Exception\n",
    "        #print(fore_train_op[ind].shape)\n",
    "        #raise Exception\n",
    "        loss = torch.tensor(fore_train_op[ind, :, 131:], dtype=torch.float32).cuda()*(\n",
    "            output-torch.tensor(fore_train_op[ind, :, :131], dtype=torch.float32).cuda())**2\n",
    "        loss = loss.sum(axis=-1).mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        e_loss += loss.detach()\n",
    "        pbar.set_description('%f' % e_loss)\n",
    "    val_loss = 0\n",
    "    model.eval()\n",
    "    loss_list = []\n",
    "    pbar = tqdm(range(0, len(fore_valid_op)-32, 32))  # len(fore_valid_op)           ####################   maybe also batch_size instead of 32\n",
    "    for start in pbar:\n",
    "        #print(start, start+batch_size)\n",
    "        output = model(*[torch.tensor(ip[start:start+batch_size], dtype=torch.float32).cuda() \n",
    "                         if i < 3 else torch.tensor(ip[start:start+batch_size], dtype=torch.int32).cuda() \n",
    "                         for i, ip in enumerate(fore_valid_ip)], trainn=False, tgt=torch.tensor(fore_valid_op[start:start+batch_size], dtype=torch.float32).cuda())\n",
    "        loss = torch.tensor(fore_valid_op[start:start+batch_size,:, 131:], dtype=torch.float32).cuda()*loss_func(\n",
    "            output, torch.tensor(fore_valid_op[start:start+batch_size,:, :131], dtype=torch.float32).cuda())\n",
    "        loss_list.extend(loss.sum(axis=-1).mean(axis=-1).detach().cpu().tolist())\n",
    "        loss = loss.sum(axis=-1).mean()\n",
    "        val_loss += loss.detach().cpu()\n",
    "        pbar.set_description('%f' % val_loss)\n",
    "    loss_p = e_loss*batch_size/samples_per_epoch\n",
    "    val_loss_p = val_loss*batch_size/len(fore_valid_op)\n",
    "    print('Epoch', e, 'loss', loss_p, 'val loss', val_loss_p, \"mean and std\", np.mean(loss_list), np.std(loss_list))\n",
    "    with open('loss_values_log', 'a') as f:\n",
    "        f.write(str(e)+' ' + str(loss_p.item()) + ' ' + str(val_loss_p.item())+ '\\n')\n",
    "    scheduler.step(val_loss_p.item())\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), fore_savepath)\n",
    "        best_epoch = e\n",
    "    if (e-best_epoch) > patience:\n",
    "        break\n",
    "print('Training has ended.')\n",
    "\n",
    "#STRATS, IMS, Sampling, Detach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb05a158",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVE(nn.Module):\n",
    "    def __init__(self, hid_units, output_dim):\n",
    "        super(CVE, self).__init__()\n",
    "        self.hid_units = hid_units\n",
    "        self.output_dim = output_dim\n",
    "        self.W1 = nn.Parameter(torch.Tensor(1, self.hid_units))\n",
    "        self.b1 = nn.Parameter(torch.Tensor(self.hid_units))\n",
    "        self.W2 = nn.Parameter(torch.Tensor(self.hid_units, self.output_dim))\n",
    "        nn.init.xavier_uniform_(self.W1)\n",
    "        nn.init.zeros_(self.b1)\n",
    "        nn.init.xavier_uniform_(self.W2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(-1)\n",
    "        x = torch.matmul(torch.tanh(torch.add(torch.matmul(x, self.W1), self.b1)), self.W2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hid_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.hid_dim = hid_dim\n",
    "        self.W = nn.Parameter(torch.Tensor(50, self.hid_dim))\n",
    "        self.b = nn.Parameter(torch.Tensor(self.hid_dim))\n",
    "        self.u = nn.Parameter(torch.Tensor(self.hid_dim, 1))\n",
    "        nn.init.xavier_uniform_(self.W)\n",
    "        nn.init.zeros_(self.b)\n",
    "        nn.init.xavier_uniform_(self.u)\n",
    "\n",
    "    def forward(self, x, mask, mask_value=-1e30):\n",
    "        attn_weights = torch.matmul(torch.tanh(torch.add(torch.matmul(x, self.W), self.b)), self.u)\n",
    "        mask = mask.unsqueeze(-1)\n",
    "        attn_weights = mask * attn_weights + (1 - mask) * mask_value\n",
    "        attn_weights = F.softmax(attn_weights, dim=-2)\n",
    "        return attn_weights\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, N=2, h=8, dk=None, dv=None, dff=None, dropout=0, d=8):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.N, self.h, self.dk, self.dv, self.dff, self.dropout = N, h, dk, dv, dff, dropout\n",
    "        self.epsilon = torch.finfo(torch.float32).eps * torch.finfo(torch.float32).eps\n",
    "        self.Wq = nn.Parameter(torch.Tensor(N, h, d, dk))\n",
    "        self.Wk = nn.Parameter(torch.Tensor(N, h, d, dk))\n",
    "        self.Wv = nn.Parameter(torch.Tensor(N, h, d, dv))\n",
    "        self.Wo = nn.Parameter(torch.Tensor(N, dv * h, d))\n",
    "        self.W1 = nn.Parameter(torch.Tensor(N, d, dff))\n",
    "        self.b1 = nn.Parameter(torch.Tensor(N, dff))\n",
    "        self.W2 = nn.Parameter(torch.Tensor(N, dff, d))\n",
    "        self.b2 = nn.Parameter(torch.Tensor(N, d))\n",
    "        self.gamma = nn.Parameter(torch.Tensor(2 * N))\n",
    "        self.beta = nn.Parameter(torch.Tensor(2 * N))\n",
    "        nn.init.xavier_uniform_(self.Wq)\n",
    "        nn.init.xavier_uniform_(self.Wk)\n",
    "        nn.init.xavier_uniform_(self.Wv)\n",
    "        nn.init.xavier_uniform_(self.Wo)\n",
    "        nn.init.xavier_uniform_(self.W1)\n",
    "        nn.init.zeros_(self.b1)\n",
    "        nn.init.xavier_uniform_(self.W2)\n",
    "        nn.init.zeros_(self.b2)\n",
    "        nn.init.ones_(self.gamma)\n",
    "        nn.init.zeros_(self.beta)\n",
    "        self.training = False\n",
    "        \n",
    "    def forward(self, x, mask, mask_value=-1e-30):\n",
    "        mask = mask.unsqueeze(-2)\n",
    "        for i in range(self.N):\n",
    "            # MHA\n",
    "            mha_ops = []\n",
    "            for j in range(self.h):\n",
    "                q = torch.matmul(x, self.Wq[i, j, :, :])          \n",
    "                k = torch.matmul(x, self.Wk[i, j, :, :]).permute(0, 2, 1)\n",
    "                v = torch.matmul(x, self.Wv[i, j, :, :])\n",
    "                A = torch.matmul(q, k)\n",
    "                # Mask unobserved steps.\n",
    "                A = mask * A + (1 - mask) * mask_value\n",
    "                # Mask for attention dropout.\n",
    "                if self.training:\n",
    "                    dp_mask = (torch.rand(A.shape, device=A.device) >= self.dropout).float()\n",
    "                    A = A * dp_mask + (1 - dp_mask) * mask_value\n",
    "                A = F.softmax(A, dim=-1)\n",
    "                mha_ops.append(torch.matmul(A, v))\n",
    "            conc = torch.cat(mha_ops, dim=-1)\n",
    "            proj = torch.matmul(conc, self.Wo[i, :, :])\n",
    "            # Dropout.\n",
    "            if self.training:\n",
    "                proj = F.dropout(proj, p=self.dropout)\n",
    "            # Add & LN\n",
    "            x = x + proj\n",
    "            mean = x.mean(dim=-1, keepdim=True)\n",
    "            variance = torch.mean(torch.square(x - mean), dim=-1, keepdim=True)\n",
    "            std = torch.sqrt(variance + self.epsilon)\n",
    "            x = (x - mean) / std\n",
    "            x = x * self.gamma[2 * i] + self.beta[2 * i]\n",
    "            # FFN\n",
    "            ffn_op = torch.add(torch.matmul(F.relu(torch.add(\n",
    "                torch.matmul(x, self.W1[i, :, :]), self.b1[i, :])),\n",
    "                                            self.W2[i, :, :]), self.b2[i, :])\n",
    "            # Dropout.\n",
    "            if self.training:\n",
    "                ffn_op = F.dropout(ffn_op, p=self.dropout)\n",
    "            # Add & LN\n",
    "            x = x + ffn_op\n",
    "            mean = x.mean(dim=-1, keepdim=True)\n",
    "            variance = torch.mean(torch.square(x - mean), dim=-1, keepdim=True)\n",
    "            std = torch.sqrt(variance + self.epsilon)\n",
    "            x = (x - mean) / std\n",
    "            x = x * self.gamma[2 * i + 1] + self.beta[2 * i + 1]\n",
    "        return x\n",
    "\n",
    "\n",
    "class StratsModel(nn.Module):\n",
    "    def __init__(self, D, max_len, V, d, N, he, dropout, forecast=False, autoregressive=True):\n",
    "        super(StratsModel, self).__init__()\n",
    "        self.forecast = forecast\n",
    "        self.autoregressive=autoregressive\n",
    "        self.demo_enc = nn.Sequential(nn.Linear(D, 2 * d),\n",
    "                                      nn.Tanh(),\n",
    "                                      nn.Linear(2 * d, d),\n",
    "                                      nn.Tanh())\n",
    "        self.varis_emb = nn.Embedding(V + 1, d)\n",
    "        nn.init.uniform_(self.varis_emb.weight, a=-0.05, b=0.05)\n",
    "        self.values_emb = CVE(int(np.sqrt(d)), d)\n",
    "        self.times_emb = CVE(int(np.sqrt(d)), d)\n",
    "        # self.comb_emb = nn.LayerNorm(d)\n",
    "        self.transformer = Transformer(N, he, dk=d//he, dv=d//he, dff=2*d, dropout=dropout, d=d)\n",
    "\n",
    "        if autoregressive:\n",
    "            self.linearize = nn.Linear(50, 131)\n",
    "            self.dec = nn.TransformerDecoderLayer(d_model=131, dim_feedforward=131, nhead=1, batch_first=True)\n",
    "        else:\n",
    "            self.attention = Attention(2 * d)\n",
    "            self.attentions = nn.ModuleList([Attention(2*d) for i in range(24)])\n",
    "            # self.concat = nn.Linear(2 * d, 1)\n",
    "            self.fore_op_calc = nn.Linear(2 * d, V)\n",
    "            self.op_calc = nn.Linear(V, 1)\n",
    "            self.sig = nn.Sigmoid()        \n",
    "        \n",
    "\n",
    "    def forward(self, demo, times, values, varis, trainn=False, tgt=torch.zeros(32, 1, 131).cuda(), schedule=True, probability=0.5, deterministic=False):\n",
    "        demo_enc = self.demo_enc(demo)\n",
    "        varis_emb = self.varis_emb(varis)\n",
    "        values_emb = self.values_emb(values)\n",
    "        times_emb = self.times_emb(times)\n",
    "\n",
    "        comb_emb = (torch.sum(torch.stack([varis_emb, values_emb, times_emb]), dim=0))\n",
    "        self.trainn = trainn\n",
    "        mask = torch.clip(varis, 0, 1)\n",
    "        cont_emb = self.transformer(comb_emb, mask)\n",
    "        \n",
    "        if trainn and schedule:\n",
    "            output = torch.zeros(cont_emb.size(0), 1, 131).cuda()\n",
    "            output.requires_grad=True\n",
    "            linear_memory  = self.linearize(cont_emb)\n",
    "            for i in range(24):\n",
    "                #print(i)\n",
    "                #print(linear_memory.size(), output.size())\n",
    "                res = self.dec(output, linear_memory)\n",
    "                if deterministic:\n",
    "                    sample = i/24\n",
    "                else:\n",
    "                    sample = torch.rand(1).item()\n",
    "                #print(res.size(), output.size())\n",
    "                print(sample, probability)\n",
    "                if sample < probability:\n",
    "                \n",
    "                    \n",
    "                    output = torch.concat([output, tgt[:,i:i+1,:]], dim=1)\n",
    "                else:\n",
    "                    output = torch.concat([output, res[:,-1:,:]], dim=1)\n",
    "            return output[:, 1:, :] \n",
    "        \n",
    "        if self.autoregressive:\n",
    "            if self.trainn:\n",
    "                #print(cont_emb.size())\n",
    "                \n",
    "                mask_new = torch.nn.Transformer.generate_square_subsequent_mask(25).cuda()\n",
    "                linear_memory = self.linearize(cont_emb)\n",
    "                start_input = torch.zeros(32, 1, 131).cuda()\n",
    "                tgt = torch.concat([start_input, tgt], dim=1)\n",
    "                result=self.dec(tgt, linear_memory, mask_new)\n",
    "                return result[:, 1:, :]\n",
    "            if not self.trainn:\n",
    "                output = torch.zeros(32, 1, 131).cuda()\n",
    "                linear_memory  = self.linearize(cont_emb)\n",
    "                for i in range(24):\n",
    "                    #print(i)\n",
    "                    #print(linear_memory.size(), output.size())\n",
    "                    res = self.dec(output, linear_memory)\n",
    "                    #print(res.size(), output.size())\n",
    "                    output = torch.concat([output, res[:,-1:,:]], dim=1)\n",
    "                return output[:, 1:, :]\n",
    "                \n",
    "        if not self.autoregressive:\n",
    "            fused_embs = []\n",
    "            for attention in self.attentions:\n",
    "                attn_weights = attention(cont_emb, mask)\n",
    "                fused_emb = torch.matmul(attn_weights.transpose(-1, -2), cont_emb).squeeze(-2)\n",
    "                # fused_emb torch.Size([32, 50])\n",
    "                fused_embs.append(fused_emb)\n",
    "            # fused_embs Liste mit 6 torch.Size([32, 50])\n",
    "            fused_emb = torch.stack(fused_embs, axis=1)\n",
    "            # altes fused_emb: torch.Size([32, 50])\n",
    "            # neues fused_emb: torch.Size([32, 6, 50])\n",
    "            # demo_enc torch.Size([32, 50])\n",
    "\n",
    "            # demo_enc.unsqueeze(axis=-2).repeat(1, 6, 1):  torch.Size([32, 6, 50])\n",
    "            #print(fused_emb.size())\n",
    "            concat = torch.cat([fused_emb, demo_enc.unsqueeze(axis=-2).repeat(1, 24, 1)], axis=-1)\n",
    "            #concat torch.Size([32, 6, 100])\n",
    "\n",
    "            x = self.fore_op_calc(concat)\n",
    "            # neues x torch.Size([32, 6, 258])\n",
    "            # altes x: torch.Size([32, 258])\n",
    "            if self.forecast:\n",
    "                return x\n",
    "            else:\n",
    "                x = self.sig(self.op_calc(x)).mean(axis=-2)\n",
    "                return x\n",
    "\n",
    "print(\"Model definitions loaded\")\n",
    "\n",
    "lr, batch_size, samples_per_epoch, patience = 0.0005, 32, int(102400/sample_divisor), 6\n",
    "d, N, he, dropout = 50, 2, 4, 0.2\n",
    "print('number of parameters: ', V)\n",
    "\n",
    "model = StratsModel(2, fore_max_len, V, d, N, he, dropout, forecast=True).cuda()\n",
    "a = summary(model, [(32, 2), (32, 880), (32, 880), (32, 880)],  # shape of fore_train_ip\n",
    "            dtypes=[torch.float, torch.float, torch.float, torch.long])\n",
    "print(a)  # Model summary\n",
    "# raise Exception\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=3)\n",
    "# Pretrain fore_model.\n",
    "best_val_loss = np.inf\n",
    "N_fore = len(fore_train_op)\n",
    "#print(N_fore)\n",
    "fore_savepath = 'mimic_iii_sparse_ims_sampling_backprop_schedule_random.pytorch'\n",
    "loss_func = torch.nn.MSELoss(reduction=\"none\")\n",
    "\n",
    "# torch.compile(model)\n",
    "for e in range(number_of_epochs):\n",
    "    e_indices = np.random.choice(range(N_fore), size=samples_per_epoch, replace=False)\n",
    "    e_loss = 0\n",
    "    pbar = tqdm(range(0, len(e_indices), batch_size))\n",
    "    model.train()\n",
    "    for start in pbar:\n",
    "        ind = e_indices[start:start+batch_size]\n",
    "        # raise Exception\n",
    "        prob=min([1.0, 1.0+(0.25-1.0)*(1-e/200)])\n",
    "        output = model(*[torch.tensor(ip[ind], dtype=torch.float32).cuda() if i < 3 \n",
    "                         else torch.tensor(ip[ind], dtype=torch.int32).cuda() \n",
    "                         for i, ip in enumerate(fore_train_ip)], trainn=True, tgt=torch.tensor(fore_train_op[ind, :, :131], dtype=torch.float32).cuda(), schedule=True, deterministic=False, probability=prob)\n",
    "        #raise Exception\n",
    "        #print(fore_train_op[ind].shape)\n",
    "        #raise Exception\n",
    "        loss = torch.tensor(fore_train_op[ind, :, 131:], dtype=torch.float32).cuda()*(\n",
    "            output-torch.tensor(fore_train_op[ind, :, :131], dtype=torch.float32).cuda())**2\n",
    "        loss = loss.sum(axis=-1).mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        e_loss += loss.detach()\n",
    "        pbar.set_description('%f' % e_loss)\n",
    "    val_loss = 0\n",
    "    model.eval()\n",
    "    loss_list = []\n",
    "    pbar = tqdm(range(0, len(fore_valid_op)-32, 32))  # len(fore_valid_op)           ####################   maybe also batch_size instead of 32\n",
    "    for start in pbar:\n",
    "        #print(start, start+batch_size)\n",
    "        output = model(*[torch.tensor(ip[start:start+batch_size], dtype=torch.float32).cuda() \n",
    "                         if i < 3 else torch.tensor(ip[start:start+batch_size], dtype=torch.int32).cuda() \n",
    "                         for i, ip in enumerate(fore_valid_ip)], trainn=False, tgt=torch.tensor(fore_valid_op[start:start+batch_size], dtype=torch.float32).cuda())\n",
    "        loss = torch.tensor(fore_valid_op[start:start+batch_size,:, 131:], dtype=torch.float32).cuda()*loss_func(\n",
    "            output, torch.tensor(fore_valid_op[start:start+batch_size,:, :131], dtype=torch.float32).cuda())\n",
    "        loss_list.extend(loss.sum(axis=-1).mean(axis=-1).detach().cpu().tolist())\n",
    "        loss = loss.sum(axis=-1).mean()\n",
    "        val_loss += loss.detach().cpu()\n",
    "        pbar.set_description('%f' % val_loss)\n",
    "    loss_p = e_loss*batch_size/samples_per_epoch\n",
    "    val_loss_p = val_loss*batch_size/len(fore_valid_op)\n",
    "    print('Epoch', e, 'loss', loss_p, 'val loss', val_loss_p)\n",
    "    with open('loss_values_log', 'a') as f:\n",
    "        f.write(str(e)+' ' + str(loss_p.item()) + ' ' + str(val_loss_p.item())+ '\\n')\n",
    "    scheduler.step(val_loss_p.item())\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), fore_savepath)\n",
    "        best_epoch = e\n",
    "    if (e-best_epoch) > patience:\n",
    "        break\n",
    "print('Training has ended.')\n",
    "#STRATS, IMS, Sampling, No Detach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d84a02",
   "metadata": {},
   "source": [
    "# Scheduled Sampling RD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea09fff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVE(nn.Module):\n",
    "    def __init__(self, hid_units, output_dim):\n",
    "        super(CVE, self).__init__()\n",
    "        self.hid_units = hid_units\n",
    "        self.output_dim = output_dim\n",
    "        self.W1 = nn.Parameter(torch.Tensor(1, self.hid_units))\n",
    "        self.b1 = nn.Parameter(torch.Tensor(self.hid_units))\n",
    "        self.W2 = nn.Parameter(torch.Tensor(self.hid_units, self.output_dim))\n",
    "        nn.init.xavier_uniform_(self.W1)\n",
    "        nn.init.zeros_(self.b1)\n",
    "        nn.init.xavier_uniform_(self.W2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(-1)\n",
    "        x = torch.matmul(torch.tanh(torch.add(torch.matmul(x, self.W1), self.b1)), self.W2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hid_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.hid_dim = hid_dim\n",
    "        self.W = nn.Parameter(torch.Tensor(50, self.hid_dim))\n",
    "        self.b = nn.Parameter(torch.Tensor(self.hid_dim))\n",
    "        self.u = nn.Parameter(torch.Tensor(self.hid_dim, 1))\n",
    "        nn.init.xavier_uniform_(self.W)\n",
    "        nn.init.zeros_(self.b)\n",
    "        nn.init.xavier_uniform_(self.u)\n",
    "\n",
    "    def forward(self, x, mask, mask_value=-1e30):\n",
    "        attn_weights = torch.matmul(torch.tanh(torch.add(torch.matmul(x, self.W), self.b)), self.u)\n",
    "        mask = mask.unsqueeze(-1)\n",
    "        attn_weights = mask * attn_weights + (1 - mask) * mask_value\n",
    "        attn_weights = F.softmax(attn_weights, dim=-2)\n",
    "        return attn_weights\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, N=2, h=8, dk=None, dv=None, dff=None, dropout=0, d=8):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.N, self.h, self.dk, self.dv, self.dff, self.dropout = N, h, dk, dv, dff, dropout\n",
    "        self.epsilon = torch.finfo(torch.float32).eps * torch.finfo(torch.float32).eps\n",
    "        self.Wq = nn.Parameter(torch.Tensor(N, h, d, dk))\n",
    "        self.Wk = nn.Parameter(torch.Tensor(N, h, d, dk))\n",
    "        self.Wv = nn.Parameter(torch.Tensor(N, h, d, dv))\n",
    "        self.Wo = nn.Parameter(torch.Tensor(N, dv * h, d))\n",
    "        self.W1 = nn.Parameter(torch.Tensor(N, d, dff))\n",
    "        self.b1 = nn.Parameter(torch.Tensor(N, dff))\n",
    "        self.W2 = nn.Parameter(torch.Tensor(N, dff, d))\n",
    "        self.b2 = nn.Parameter(torch.Tensor(N, d))\n",
    "        self.gamma = nn.Parameter(torch.Tensor(2 * N))\n",
    "        self.beta = nn.Parameter(torch.Tensor(2 * N))\n",
    "        nn.init.xavier_uniform_(self.Wq)\n",
    "        nn.init.xavier_uniform_(self.Wk)\n",
    "        nn.init.xavier_uniform_(self.Wv)\n",
    "        nn.init.xavier_uniform_(self.Wo)\n",
    "        nn.init.xavier_uniform_(self.W1)\n",
    "        nn.init.zeros_(self.b1)\n",
    "        nn.init.xavier_uniform_(self.W2)\n",
    "        nn.init.zeros_(self.b2)\n",
    "        nn.init.ones_(self.gamma)\n",
    "        nn.init.zeros_(self.beta)\n",
    "        self.training = False\n",
    "        \n",
    "    def forward(self, x, mask, mask_value=-1e-30):\n",
    "        mask = mask.unsqueeze(-2)\n",
    "        for i in range(self.N):\n",
    "            # MHA\n",
    "            mha_ops = []\n",
    "            for j in range(self.h):\n",
    "                q = torch.matmul(x, self.Wq[i, j, :, :])          \n",
    "                k = torch.matmul(x, self.Wk[i, j, :, :]).permute(0, 2, 1)\n",
    "                v = torch.matmul(x, self.Wv[i, j, :, :])\n",
    "                A = torch.matmul(q, k)\n",
    "                # Mask unobserved steps.\n",
    "                A = mask * A + (1 - mask) * mask_value\n",
    "                # Mask for attention dropout.\n",
    "                if self.training:\n",
    "                    dp_mask = (torch.rand(A.shape, device=A.device) >= self.dropout).float()\n",
    "                    A = A * dp_mask + (1 - dp_mask) * mask_value\n",
    "                A = F.softmax(A, dim=-1)\n",
    "                mha_ops.append(torch.matmul(A, v))\n",
    "            conc = torch.cat(mha_ops, dim=-1)\n",
    "            proj = torch.matmul(conc, self.Wo[i, :, :])\n",
    "            # Dropout.\n",
    "            if self.training:\n",
    "                proj = F.dropout(proj, p=self.dropout)\n",
    "            # Add & LN\n",
    "            x = x + proj\n",
    "            mean = x.mean(dim=-1, keepdim=True)\n",
    "            variance = torch.mean(torch.square(x - mean), dim=-1, keepdim=True)\n",
    "            std = torch.sqrt(variance + self.epsilon)\n",
    "            x = (x - mean) / std\n",
    "            x = x * self.gamma[2 * i] + self.beta[2 * i]\n",
    "            # FFN\n",
    "            ffn_op = torch.add(torch.matmul(F.relu(torch.add(\n",
    "                torch.matmul(x, self.W1[i, :, :]), self.b1[i, :])),\n",
    "                                            self.W2[i, :, :]), self.b2[i, :])\n",
    "            # Dropout.\n",
    "            if self.training:\n",
    "                ffn_op = F.dropout(ffn_op, p=self.dropout)\n",
    "            # Add & LN\n",
    "            x = x + ffn_op\n",
    "            mean = x.mean(dim=-1, keepdim=True)\n",
    "            variance = torch.mean(torch.square(x - mean), dim=-1, keepdim=True)\n",
    "            std = torch.sqrt(variance + self.epsilon)\n",
    "            x = (x - mean) / std\n",
    "            x = x * self.gamma[2 * i + 1] + self.beta[2 * i + 1]\n",
    "        return x\n",
    "\n",
    "\n",
    "class StratsModel(nn.Module):\n",
    "    def __init__(self, D, max_len, V, d, N, he, dropout, forecast=False, autoregressive=True):\n",
    "        super(StratsModel, self).__init__()\n",
    "        self.forecast = forecast\n",
    "        self.autoregressive=autoregressive\n",
    "        self.demo_enc = nn.Sequential(nn.Linear(D, 2 * d),\n",
    "                                      nn.Tanh(),\n",
    "                                      nn.Linear(2 * d, d),\n",
    "                                      nn.Tanh())\n",
    "        self.varis_emb = nn.Embedding(V + 1, d)\n",
    "        nn.init.uniform_(self.varis_emb.weight, a=-0.05, b=0.05)\n",
    "        self.values_emb = CVE(int(np.sqrt(d)), d)\n",
    "        self.times_emb = CVE(int(np.sqrt(d)), d)\n",
    "        # self.comb_emb = nn.LayerNorm(d)\n",
    "        self.transformer = Transformer(N, he, dk=d//he, dv=d//he, dff=2*d, dropout=dropout, d=d)\n",
    "\n",
    "        if autoregressive:\n",
    "            self.linearize = nn.Linear(50, 131)\n",
    "            self.dec = nn.TransformerDecoderLayer(d_model=131, dim_feedforward=131, nhead=1, batch_first=True)\n",
    "        else:\n",
    "            self.attention = Attention(2 * d)\n",
    "            self.attentions = nn.ModuleList([Attention(2*d) for i in range(24)])\n",
    "            # self.concat = nn.Linear(2 * d, 1)\n",
    "            self.fore_op_calc = nn.Linear(2 * d, V)\n",
    "            self.op_calc = nn.Linear(V, 1)\n",
    "            self.sig = nn.Sigmoid()        \n",
    "        \n",
    "\n",
    "    def forward(self, demo, times, values, varis, trainn=False, tgt=torch.zeros(32, 1, 131).cuda(), schedule=True, probability=0.5, deterministic=False):\n",
    "        demo_enc = self.demo_enc(demo)\n",
    "        varis_emb = self.varis_emb(varis)\n",
    "        values_emb = self.values_emb(values)\n",
    "        times_emb = self.times_emb(times)\n",
    "\n",
    "        comb_emb = (torch.sum(torch.stack([varis_emb, values_emb, times_emb]), dim=0))\n",
    "        self.trainn = trainn\n",
    "        mask = torch.clip(varis, 0, 1)\n",
    "        cont_emb = self.transformer(comb_emb, mask)\n",
    "        \n",
    "        \n",
    "        if trainn and schedule:\n",
    "            \n",
    "            output = torch.zeros(cont_emb.size(0), 1, 131).cuda()\n",
    "            output.requires_grad=True\n",
    "            linear_memory  = self.linearize(cont_emb)\n",
    "            for i in range(24):\n",
    "                #print(i)\n",
    "                #print(linear_memory.size(), output.size())\n",
    "                res = self.dec(output.detach(), linear_memory)\n",
    "                if deterministic:\n",
    "                    sample = i/24\n",
    "                else:\n",
    "                    sample = torch.rand(1).item()\n",
    "                #print(res.size(), output.size())\n",
    "                print(sample, probability)\n",
    "                if sample < probability:\n",
    "                \n",
    "                    \n",
    "                    output = torch.concat([output, tgt[:,i:i+1,:]], dim=1)\n",
    "                else:\n",
    "                    output = torch.concat([output, res[:,-1:,:]], dim=1)\n",
    "            return output[:, 1:, :] \n",
    "        if self.autoregressive:\n",
    "            if self.trainn:\n",
    "                #print(cont_emb.size())\n",
    "                \n",
    "                mask_new = torch.nn.Transformer.generate_square_subsequent_mask(25).cuda()\n",
    "                linear_memory = self.linearize(cont_emb)\n",
    "                start_input = torch.zeros(32, 1, 131).cuda()\n",
    "                tgt = torch.concat([start_input, tgt], dim=1)\n",
    "                result=self.dec(tgt, linear_memory, mask_new)\n",
    "                return result[:, 1:, :]\n",
    "            if not self.trainn:\n",
    "                output = torch.zeros(32, 1, 131).cuda()\n",
    "                linear_memory  = self.linearize(cont_emb)\n",
    "                for i in range(24):\n",
    "                    #print(i)\n",
    "                    #print(linear_memory.size(), output.size())\n",
    "                    res = self.dec(output.detach(), linear_memory)\n",
    "                    #print(res.size(), output.size())\n",
    "                    output = torch.concat([output, res[:,-1:,:]], dim=1)\n",
    "                return output[:, 1:, :]\n",
    "                \n",
    "        if not self.autoregressive:\n",
    "            fused_embs = []\n",
    "            for attention in self.attentions:\n",
    "                attn_weights = attention(cont_emb, mask)\n",
    "                fused_emb = torch.matmul(attn_weights.transpose(-1, -2), cont_emb).squeeze(-2)\n",
    "                # fused_emb torch.Size([32, 50])\n",
    "                fused_embs.append(fused_emb)\n",
    "            # fused_embs Liste mit 6 torch.Size([32, 50])\n",
    "            fused_emb = torch.stack(fused_embs, axis=1)\n",
    "            # altes fused_emb: torch.Size([32, 50])\n",
    "            # neues fused_emb: torch.Size([32, 6, 50])\n",
    "            # demo_enc torch.Size([32, 50])\n",
    "\n",
    "            # demo_enc.unsqueeze(axis=-2).repeat(1, 6, 1):  torch.Size([32, 6, 50])\n",
    "            #print(fused_emb.size())\n",
    "            concat = torch.cat([fused_emb, demo_enc.unsqueeze(axis=-2).repeat(1, 24, 1)], axis=-1)\n",
    "            #concat torch.Size([32, 6, 100])\n",
    "\n",
    "            x = self.fore_op_calc(concat)\n",
    "            # neues x torch.Size([32, 6, 258])\n",
    "            # altes x: torch.Size([32, 258])\n",
    "            if self.forecast:\n",
    "                return x\n",
    "            else:\n",
    "                x = self.sig(self.op_calc(x)).mean(axis=-2)\n",
    "                return x\n",
    "\n",
    "print(\"Model definitions loaded\")\n",
    "\n",
    "\n",
    "lr, batch_size, samples_per_epoch, patience = 0.0005, 32, int(102400/sample_divisor), 6\n",
    "d, N, he, dropout = 50, 2, 4, 0.2\n",
    "print('number of parameters: ', V)\n",
    "\n",
    "model = StratsModel(2, fore_max_len, V, d, N, he, dropout, forecast=True).cuda()\n",
    "a = summary(model, [(32, 2), (32, 880), (32, 880), (32, 880)],  # shape of fore_train_ip\n",
    "            dtypes=[torch.float, torch.float, torch.float, torch.long])\n",
    "print(a)  # Model summary\n",
    "# raise Exception\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=3)\n",
    "# Pretrain fore_model.\n",
    "best_val_loss = np.inf\n",
    "N_fore = len(fore_train_op)\n",
    "#print(N_fore)\n",
    "fore_savepath = 'mimic_iii_sparse_ims_sampling_detach_schedule_random_dtf.pytorch'\n",
    "loss_func = torch.nn.MSELoss(reduction=\"none\")\n",
    "\n",
    "# torch.compile(model)\n",
    "for e in range(number_of_epochs):\n",
    "    e_indices = np.random.choice(range(N_fore), size=samples_per_epoch, replace=False)\n",
    "    e_loss = 0\n",
    "    pbar = tqdm(range(0, len(e_indices), batch_size))\n",
    "    model.train()\n",
    "    for start in pbar:\n",
    "        ind = e_indices[start:start+batch_size]\n",
    "        # raise Exception\n",
    "        prob=min([1.0, 1.0+(0.25-1.0)*(1-e/200)])\n",
    "        prob = max([0.25, 0.25+(1.0-0.25)*(1-e/200)]) #DTF\n",
    "        output = model(*[torch.tensor(ip[ind], dtype=torch.float32).cuda() if i < 3 \n",
    "                         else torch.tensor(ip[ind], dtype=torch.int32).cuda() \n",
    "                         for i, ip in enumerate(fore_train_ip)], trainn=True, tgt=torch.tensor(fore_train_op[ind, :, :131], dtype=torch.float32).cuda(), schedule=True, deterministic=False, probability=prob)\n",
    "        #raise Exception\n",
    "        #print(fore_train_op[ind].shape)\n",
    "        #raise Exception\n",
    "        loss = torch.tensor(fore_train_op[ind, :, 131:], dtype=torch.float32).cuda()*(\n",
    "            output-torch.tensor(fore_train_op[ind, :, :131], dtype=torch.float32).cuda())**2\n",
    "        loss = loss.sum(axis=-1).mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        e_loss += loss.detach()\n",
    "        pbar.set_description('%f' % e_loss)\n",
    "    val_loss = 0\n",
    "    model.eval()\n",
    "    loss_list = []\n",
    "    pbar = tqdm(range(0, len(fore_valid_op)-32, 32))  # len(fore_valid_op)           ####################   maybe also batch_size instead of 32\n",
    "    for start in pbar:\n",
    "        #print(start, start+batch_size)\n",
    "        output = model(*[torch.tensor(ip[start:start+batch_size], dtype=torch.float32).cuda() \n",
    "                         if i < 3 else torch.tensor(ip[start:start+batch_size], dtype=torch.int32).cuda() \n",
    "                         for i, ip in enumerate(fore_valid_ip)], trainn=False, tgt=torch.tensor(fore_valid_op[start:start+batch_size], dtype=torch.float32).cuda())\n",
    "        loss = torch.tensor(fore_valid_op[start:start+batch_size,:, 131:], dtype=torch.float32).cuda()*loss_func(\n",
    "            output, torch.tensor(fore_valid_op[start:start+batch_size,:, :131], dtype=torch.float32).cuda())\n",
    "        loss_list.extend(loss.sum(axis=-1).mean(axis=-1).detach().cpu().tolist())\n",
    "        loss = loss.sum(axis=-1).mean()\n",
    "        val_loss += loss.detach().cpu()\n",
    "        pbar.set_description('%f' % val_loss)\n",
    "    loss_p = e_loss*batch_size/samples_per_epoch\n",
    "    val_loss_p = val_loss*batch_size/len(fore_valid_op)\n",
    "    print('Epoch', e, 'loss', loss_p, 'val loss', val_loss_p, \"mean and std\", np.mean(loss_list), np.std(loss_list))\n",
    "    with open('loss_values_log', 'a') as f:\n",
    "        f.write(str(e)+' ' + str(loss_p.item()) + ' ' + str(val_loss_p.item())+ '\\n')\n",
    "    scheduler.step(val_loss_p.item())\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), fore_savepath)\n",
    "        best_epoch = e\n",
    "    if (e-best_epoch) > patience:\n",
    "        break\n",
    "print('Training has ended.')\n",
    "\n",
    "#STRATS, IMS, Sampling, Detach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7fb7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVE(nn.Module):\n",
    "    def __init__(self, hid_units, output_dim):\n",
    "        super(CVE, self).__init__()\n",
    "        self.hid_units = hid_units\n",
    "        self.output_dim = output_dim\n",
    "        self.W1 = nn.Parameter(torch.Tensor(1, self.hid_units))\n",
    "        self.b1 = nn.Parameter(torch.Tensor(self.hid_units))\n",
    "        self.W2 = nn.Parameter(torch.Tensor(self.hid_units, self.output_dim))\n",
    "        nn.init.xavier_uniform_(self.W1)\n",
    "        nn.init.zeros_(self.b1)\n",
    "        nn.init.xavier_uniform_(self.W2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(-1)\n",
    "        x = torch.matmul(torch.tanh(torch.add(torch.matmul(x, self.W1), self.b1)), self.W2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hid_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.hid_dim = hid_dim\n",
    "        self.W = nn.Parameter(torch.Tensor(50, self.hid_dim))\n",
    "        self.b = nn.Parameter(torch.Tensor(self.hid_dim))\n",
    "        self.u = nn.Parameter(torch.Tensor(self.hid_dim, 1))\n",
    "        nn.init.xavier_uniform_(self.W)\n",
    "        nn.init.zeros_(self.b)\n",
    "        nn.init.xavier_uniform_(self.u)\n",
    "\n",
    "    def forward(self, x, mask, mask_value=-1e30):\n",
    "        attn_weights = torch.matmul(torch.tanh(torch.add(torch.matmul(x, self.W), self.b)), self.u)\n",
    "        mask = mask.unsqueeze(-1)\n",
    "        attn_weights = mask * attn_weights + (1 - mask) * mask_value\n",
    "        attn_weights = F.softmax(attn_weights, dim=-2)\n",
    "        return attn_weights\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, N=2, h=8, dk=None, dv=None, dff=None, dropout=0, d=8):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.N, self.h, self.dk, self.dv, self.dff, self.dropout = N, h, dk, dv, dff, dropout\n",
    "        self.epsilon = torch.finfo(torch.float32).eps * torch.finfo(torch.float32).eps\n",
    "        self.Wq = nn.Parameter(torch.Tensor(N, h, d, dk))\n",
    "        self.Wk = nn.Parameter(torch.Tensor(N, h, d, dk))\n",
    "        self.Wv = nn.Parameter(torch.Tensor(N, h, d, dv))\n",
    "        self.Wo = nn.Parameter(torch.Tensor(N, dv * h, d))\n",
    "        self.W1 = nn.Parameter(torch.Tensor(N, d, dff))\n",
    "        self.b1 = nn.Parameter(torch.Tensor(N, dff))\n",
    "        self.W2 = nn.Parameter(torch.Tensor(N, dff, d))\n",
    "        self.b2 = nn.Parameter(torch.Tensor(N, d))\n",
    "        self.gamma = nn.Parameter(torch.Tensor(2 * N))\n",
    "        self.beta = nn.Parameter(torch.Tensor(2 * N))\n",
    "        nn.init.xavier_uniform_(self.Wq)\n",
    "        nn.init.xavier_uniform_(self.Wk)\n",
    "        nn.init.xavier_uniform_(self.Wv)\n",
    "        nn.init.xavier_uniform_(self.Wo)\n",
    "        nn.init.xavier_uniform_(self.W1)\n",
    "        nn.init.zeros_(self.b1)\n",
    "        nn.init.xavier_uniform_(self.W2)\n",
    "        nn.init.zeros_(self.b2)\n",
    "        nn.init.ones_(self.gamma)\n",
    "        nn.init.zeros_(self.beta)\n",
    "        self.training = False\n",
    "        \n",
    "    def forward(self, x, mask, mask_value=-1e-30):\n",
    "        mask = mask.unsqueeze(-2)\n",
    "        for i in range(self.N):\n",
    "            # MHA\n",
    "            mha_ops = []\n",
    "            for j in range(self.h):\n",
    "                q = torch.matmul(x, self.Wq[i, j, :, :])          \n",
    "                k = torch.matmul(x, self.Wk[i, j, :, :]).permute(0, 2, 1)\n",
    "                v = torch.matmul(x, self.Wv[i, j, :, :])\n",
    "                A = torch.matmul(q, k)\n",
    "                # Mask unobserved steps.\n",
    "                A = mask * A + (1 - mask) * mask_value\n",
    "                # Mask for attention dropout.\n",
    "                if self.training:\n",
    "                    dp_mask = (torch.rand(A.shape, device=A.device) >= self.dropout).float()\n",
    "                    A = A * dp_mask + (1 - dp_mask) * mask_value\n",
    "                A = F.softmax(A, dim=-1)\n",
    "                mha_ops.append(torch.matmul(A, v))\n",
    "            conc = torch.cat(mha_ops, dim=-1)\n",
    "            proj = torch.matmul(conc, self.Wo[i, :, :])\n",
    "            # Dropout.\n",
    "            if self.training:\n",
    "                proj = F.dropout(proj, p=self.dropout)\n",
    "            # Add & LN\n",
    "            x = x + proj\n",
    "            mean = x.mean(dim=-1, keepdim=True)\n",
    "            variance = torch.mean(torch.square(x - mean), dim=-1, keepdim=True)\n",
    "            std = torch.sqrt(variance + self.epsilon)\n",
    "            x = (x - mean) / std\n",
    "            x = x * self.gamma[2 * i] + self.beta[2 * i]\n",
    "            # FFN\n",
    "            ffn_op = torch.add(torch.matmul(F.relu(torch.add(\n",
    "                torch.matmul(x, self.W1[i, :, :]), self.b1[i, :])),\n",
    "                                            self.W2[i, :, :]), self.b2[i, :])\n",
    "            # Dropout.\n",
    "            if self.training:\n",
    "                ffn_op = F.dropout(ffn_op, p=self.dropout)\n",
    "            # Add & LN\n",
    "            x = x + ffn_op\n",
    "            mean = x.mean(dim=-1, keepdim=True)\n",
    "            variance = torch.mean(torch.square(x - mean), dim=-1, keepdim=True)\n",
    "            std = torch.sqrt(variance + self.epsilon)\n",
    "            x = (x - mean) / std\n",
    "            x = x * self.gamma[2 * i + 1] + self.beta[2 * i + 1]\n",
    "        return x\n",
    "\n",
    "\n",
    "class StratsModel(nn.Module):\n",
    "    def __init__(self, D, max_len, V, d, N, he, dropout, forecast=False, autoregressive=True):\n",
    "        super(StratsModel, self).__init__()\n",
    "        self.forecast = forecast\n",
    "        self.autoregressive=autoregressive\n",
    "        self.demo_enc = nn.Sequential(nn.Linear(D, 2 * d),\n",
    "                                      nn.Tanh(),\n",
    "                                      nn.Linear(2 * d, d),\n",
    "                                      nn.Tanh())\n",
    "        self.varis_emb = nn.Embedding(V + 1, d)\n",
    "        nn.init.uniform_(self.varis_emb.weight, a=-0.05, b=0.05)\n",
    "        self.values_emb = CVE(int(np.sqrt(d)), d)\n",
    "        self.times_emb = CVE(int(np.sqrt(d)), d)\n",
    "        # self.comb_emb = nn.LayerNorm(d)\n",
    "        self.transformer = Transformer(N, he, dk=d//he, dv=d//he, dff=2*d, dropout=dropout, d=d)\n",
    "\n",
    "        if autoregressive:\n",
    "            self.linearize = nn.Linear(50, 131)\n",
    "            self.dec = nn.TransformerDecoderLayer(d_model=131, dim_feedforward=131, nhead=1, batch_first=True)\n",
    "        else:\n",
    "            self.attention = Attention(2 * d)\n",
    "            self.attentions = nn.ModuleList([Attention(2*d) for i in range(24)])\n",
    "            # self.concat = nn.Linear(2 * d, 1)\n",
    "            self.fore_op_calc = nn.Linear(2 * d, V)\n",
    "            self.op_calc = nn.Linear(V, 1)\n",
    "            self.sig = nn.Sigmoid()        \n",
    "        \n",
    "\n",
    "    def forward(self, demo, times, values, varis, trainn=False, tgt=torch.zeros(32, 1, 131).cuda(), schedule=True, probability=0.5, deterministic=False):\n",
    "        demo_enc = self.demo_enc(demo)\n",
    "        varis_emb = self.varis_emb(varis)\n",
    "        values_emb = self.values_emb(values)\n",
    "        times_emb = self.times_emb(times)\n",
    "\n",
    "        comb_emb = (torch.sum(torch.stack([varis_emb, values_emb, times_emb]), dim=0))\n",
    "        self.trainn = trainn\n",
    "        mask = torch.clip(varis, 0, 1)\n",
    "        cont_emb = self.transformer(comb_emb, mask)\n",
    "        \n",
    "        if trainn and schedule:\n",
    "            output = torch.zeros(cont_emb.size(0), 1, 131).cuda()\n",
    "            output.requires_grad=True\n",
    "            linear_memory  = self.linearize(cont_emb)\n",
    "            for i in range(24):\n",
    "                #print(i)\n",
    "                #print(linear_memory.size(), output.size())\n",
    "                res = self.dec(output, linear_memory)\n",
    "                if deterministic:\n",
    "                    sample = i/24\n",
    "                else:\n",
    "                    sample = torch.rand(1).item()\n",
    "                #print(res.size(), output.size())\n",
    "                print(sample, probability)\n",
    "                if sample < probability:\n",
    "                \n",
    "                    \n",
    "                    output = torch.concat([output, tgt[:,i:i+1,:]], dim=1)\n",
    "                else:\n",
    "                    output = torch.concat([output, res[:,-1:,:]], dim=1)\n",
    "            return output[:, 1:, :] \n",
    "        \n",
    "        if self.autoregressive:\n",
    "            if self.trainn:\n",
    "                #print(cont_emb.size())\n",
    "                \n",
    "                mask_new = torch.nn.Transformer.generate_square_subsequent_mask(25).cuda()\n",
    "                linear_memory = self.linearize(cont_emb)\n",
    "                start_input = torch.zeros(32, 1, 131).cuda()\n",
    "                tgt = torch.concat([start_input, tgt], dim=1)\n",
    "                result=self.dec(tgt, linear_memory, mask_new)\n",
    "                return result[:, 1:, :]\n",
    "            if not self.trainn:\n",
    "                output = torch.zeros(32, 1, 131).cuda()\n",
    "                linear_memory  = self.linearize(cont_emb)\n",
    "                for i in range(24):\n",
    "                    #print(i)\n",
    "                    #print(linear_memory.size(), output.size())\n",
    "                    res = self.dec(output, linear_memory)\n",
    "                    #print(res.size(), output.size())\n",
    "                    output = torch.concat([output, res[:,-1:,:]], dim=1)\n",
    "                return output[:, 1:, :]\n",
    "                \n",
    "        if not self.autoregressive:\n",
    "            fused_embs = []\n",
    "            for attention in self.attentions:\n",
    "                attn_weights = attention(cont_emb, mask)\n",
    "                fused_emb = torch.matmul(attn_weights.transpose(-1, -2), cont_emb).squeeze(-2)\n",
    "                # fused_emb torch.Size([32, 50])\n",
    "                fused_embs.append(fused_emb)\n",
    "            # fused_embs Liste mit 6 torch.Size([32, 50])\n",
    "            fused_emb = torch.stack(fused_embs, axis=1)\n",
    "            # altes fused_emb: torch.Size([32, 50])\n",
    "            # neues fused_emb: torch.Size([32, 6, 50])\n",
    "            # demo_enc torch.Size([32, 50])\n",
    "\n",
    "            # demo_enc.unsqueeze(axis=-2).repeat(1, 6, 1):  torch.Size([32, 6, 50])\n",
    "            #print(fused_emb.size())\n",
    "            concat = torch.cat([fused_emb, demo_enc.unsqueeze(axis=-2).repeat(1, 24, 1)], axis=-1)\n",
    "            #concat torch.Size([32, 6, 100])\n",
    "\n",
    "            x = self.fore_op_calc(concat)\n",
    "            # neues x torch.Size([32, 6, 258])\n",
    "            # altes x: torch.Size([32, 258])\n",
    "            if self.forecast:\n",
    "                return x\n",
    "            else:\n",
    "                x = self.sig(self.op_calc(x)).mean(axis=-2)\n",
    "                return x\n",
    "\n",
    "print(\"Model definitions loaded\")\n",
    "\n",
    "lr, batch_size, samples_per_epoch, patience = 0.0005, 32, int(102400/sample_divisor), 6\n",
    "d, N, he, dropout = 50, 2, 4, 0.2\n",
    "print('number of parameters: ', V)\n",
    "\n",
    "model = StratsModel(2, fore_max_len, V, d, N, he, dropout, forecast=True).cuda()\n",
    "a = summary(model, [(32, 2), (32, 880), (32, 880), (32, 880)],  # shape of fore_train_ip\n",
    "            dtypes=[torch.float, torch.float, torch.float, torch.long])\n",
    "print(a)  # Model summary\n",
    "# raise Exception\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=3)\n",
    "# Pretrain fore_model.\n",
    "best_val_loss = np.inf\n",
    "N_fore = len(fore_train_op)\n",
    "#print(N_fore)\n",
    "fore_savepath = 'mimic_iii_sparse_ims_sampling_backprop_schedule_random_dtf.pytorch'\n",
    "loss_func = torch.nn.MSELoss(reduction=\"none\")\n",
    "\n",
    "# torch.compile(model)\n",
    "for e in range(number_of_epochs):\n",
    "    e_indices = np.random.choice(range(N_fore), size=samples_per_epoch, replace=False)\n",
    "    e_loss = 0\n",
    "    pbar = tqdm(range(0, len(e_indices), batch_size))\n",
    "    model.train()\n",
    "    for start in pbar:\n",
    "        ind = e_indices[start:start+batch_size]\n",
    "        # raise Exception\n",
    "        prob=min([1.0, 1.0+(0.25-1.0)*(1-e/200)])\n",
    "        prob = max([0.25, 0.25+(1.0-0.25)*(1-e/200)]) #DTF\n",
    "        output = model(*[torch.tensor(ip[ind], dtype=torch.float32).cuda() if i < 3 \n",
    "                         else torch.tensor(ip[ind], dtype=torch.int32).cuda() \n",
    "                         for i, ip in enumerate(fore_train_ip)], trainn=True, tgt=torch.tensor(fore_train_op[ind, :, :131], dtype=torch.float32).cuda(), schedule=True, deterministic=False, probability=prob)\n",
    "        #raise Exception\n",
    "        #print(fore_train_op[ind].shape)\n",
    "        #raise Exception\n",
    "        loss = torch.tensor(fore_train_op[ind, :, 131:], dtype=torch.float32).cuda()*(\n",
    "            output-torch.tensor(fore_train_op[ind, :, :131], dtype=torch.float32).cuda())**2\n",
    "        loss = loss.sum(axis=-1).mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        e_loss += loss.detach()\n",
    "        pbar.set_description('%f' % e_loss)\n",
    "    val_loss = 0\n",
    "    model.eval()\n",
    "    loss_list = []\n",
    "    pbar = tqdm(range(0, len(fore_valid_op)-32, 32))  # len(fore_valid_op)           ####################   maybe also batch_size instead of 32\n",
    "    for start in pbar:\n",
    "        #print(start, start+batch_size)\n",
    "        output = model(*[torch.tensor(ip[start:start+batch_size], dtype=torch.float32).cuda() \n",
    "                         if i < 3 else torch.tensor(ip[start:start+batch_size], dtype=torch.int32).cuda() \n",
    "                         for i, ip in enumerate(fore_valid_ip)], trainn=False, tgt=torch.tensor(fore_valid_op[start:start+batch_size], dtype=torch.float32).cuda())\n",
    "        loss = torch.tensor(fore_valid_op[start:start+batch_size,:, 131:], dtype=torch.float32).cuda()*loss_func(\n",
    "            output, torch.tensor(fore_valid_op[start:start+batch_size,:, :131], dtype=torch.float32).cuda())\n",
    "        loss_list.extend(loss.sum(axis=-1).mean(axis=-1).detach().cpu().tolist())\n",
    "        loss = loss.sum(axis=-1).mean()\n",
    "        val_loss += loss.detach().cpu()\n",
    "        pbar.set_description('%f' % val_loss)\n",
    "    loss_p = e_loss*batch_size/samples_per_epoch\n",
    "    val_loss_p = val_loss*batch_size/len(fore_valid_op)\n",
    "    print('Epoch', e, 'loss', loss_p, 'val loss', val_loss_p)\n",
    "    with open('loss_values_log', 'a') as f:\n",
    "        f.write(str(e)+' ' + str(loss_p.item()) + ' ' + str(val_loss_p.item())+ '\\n')\n",
    "    scheduler.step(val_loss_p.item())\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), fore_savepath)\n",
    "        best_epoch = e\n",
    "    if (e-best_epoch) > patience:\n",
    "        break\n",
    "print('Training has ended.')\n",
    "#STRATS, IMS, Sampling, No Detach"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
