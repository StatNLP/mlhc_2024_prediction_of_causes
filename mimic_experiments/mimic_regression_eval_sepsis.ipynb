{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99d86537",
   "metadata": {},
   "source": [
    "*ToDo*\n",
    "\n",
    "02) pred_window und obs_window ausprobieren.\n",
    "03) output nochmal genau anschauen.\n",
    "04) was ist maskiert anschauen - und wie genau?\n",
    "05) VRAM wenig ausgelastet. Batch size mal mit 320 ausprobieren, aber auch durchziehen.\n",
    "06) Task pred: scheduler einfügen.\n",
    "07) Analyse each var (sparsity)\n",
    "08) loss per var/quality of varis loss (is this already ablation to forecast only one var)\n",
    "09) Include sepsis definition\n",
    "10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "driven-termination",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-23T11:52:56.289283Z",
     "iopub.status.busy": "2023-11-23T11:52:56.288453Z",
     "iopub.status.idle": "2023-11-23T11:52:59.823096Z",
     "shell.execute_reply": "2023-11-23T11:52:59.822088Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random \n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, auc\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchinfo import summary\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c94544",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-23T11:52:59.826920Z",
     "iopub.status.busy": "2023-11-23T11:52:59.826550Z",
     "iopub.status.idle": "2023-11-23T11:52:59.832492Z",
     "shell.execute_reply": "2023-11-23T11:52:59.831649Z"
    }
   },
   "outputs": [],
   "source": [
    "# data settings\n",
    "test_cond = 0\n",
    "sepsis_check = 1\n",
    "\n",
    "if test_cond == 1:\n",
    "    data_path = '/home/mitarb/fracarolli/files/231113_STraTS_new/mimic_iii_preprocessed_new_z_err20092.pkl'\n",
    "    sample_divisor = 100\n",
    "    number_of_epochs = 20\n",
    "else:\n",
    "    data_path = '/home/mitarb/fracarolli/files/231113_STraTS_new/mimic_iii_preprocessed_new_z_err20092.pkl'\n",
    "    sample_divisor = 1\n",
    "    number_of_epochs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5237f27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int) -> None:\n",
    "    \"\"\"\n",
    "    Set the random seed for modules torch, numpy and random.\n",
    "\n",
    "    :param seed: random seed\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    if torch.cuda.is_available() and torch.cuda.device_count() > 0:\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "#set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recreational-record",
   "metadata": {},
   "source": [
    "## Load forecast dataset into matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fdc7de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-23T11:52:59.835708Z",
     "iopub.status.busy": "2023-11-23T11:52:59.835482Z",
     "iopub.status.idle": "2023-11-23T11:52:59.842763Z",
     "shell.execute_reply": "2023-11-23T11:52:59.841782Z"
    }
   },
   "outputs": [],
   "source": [
    "def inv_list(l, start=0):  # Create vind\n",
    "    d = {}\n",
    "    for i in range(len(l)):\n",
    "        d[l[i]] = i+start\n",
    "    return d\n",
    "\n",
    "\n",
    "def f(x):\n",
    "    mask   = [0 for i in range(V)]\n",
    "    values = [0 for i in range(V)]\n",
    "    for vv in x:  # tuple of ['vind','value']\n",
    "        v = int(vv[0])-1  # shift index of vind\n",
    "        mask[v] = 1\n",
    "        values[v] = vv[1]  # get value\n",
    "    return values+mask  # concat\n",
    "\n",
    "\n",
    "def pad(x):\n",
    "    if len(x) > 880:\n",
    "        print(len(x))\n",
    "    return x+[0]*(fore_max_len-len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "executive-annual",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-23T11:52:59.846164Z",
     "iopub.status.busy": "2023-11-23T11:52:59.845815Z",
     "iopub.status.idle": "2023-11-23T12:50:22.923913Z",
     "shell.execute_reply": "2023-11-23T12:50:22.922643Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "# Read data.\n",
    "data, oc, train_ind, valid_ind, test_ind = pickle.load(open(data_path, 'rb'))\n",
    "import itertools\n",
    "if sepsis_check:\n",
    "    test_ind = pd.read_csv('infec_test.csv', header=None)\n",
    "    test_ind = list(itertools.chain(*test_ind.values.tolist()))\n",
    "    test_ind = list(map(int, test_ind))\n",
    "\n",
    "# Remove test patients.\n",
    "print(len(train_ind))  # 28708\n",
    "print(len(valid_ind))  # 7270\n",
    "print(len(test_ind))  # 8880, 743 with sepsis_check\n",
    "#raise Exception\n",
    "data = data.merge(oc[['ts_ind', 'SUBJECT_ID']], on='ts_ind', how='left')\n",
    "test_sub = oc.loc[oc.ts_ind.isin(test_ind)].SUBJECT_ID.unique()\n",
    "data = data.loc[~data.SUBJECT_ID.isin(test_sub)]\n",
    "oc = oc.loc[~oc.SUBJECT_ID.isin(test_sub)]\n",
    "data.drop(columns=['SUBJECT_ID', 'TABLE'], inplace=True)\n",
    "# Fix age.\n",
    "data.loc[(data.variable == 'Age') & (data.value > 200), 'value'] = 91.4\n",
    "# Get static data with mean fill and missingness indicator.\n",
    "static_varis = ['Age', 'Gender']\n",
    "ii = data.variable.isin(static_varis)\n",
    "static_data = data.loc[ii]  # static data are 'Age' and 'Gender'\n",
    "data = data.loc[~ii]  # ~ binary flip\n",
    "# print('data\\n',data)\n",
    "\n",
    "static_var_to_ind = inv_list(static_varis)  # {'Age': 0, 'Gender': 1}\n",
    "D = len(static_varis)  # 2 by definition\n",
    "N = data.ts_ind.max()+1  # /= 12: 52861\n",
    "print(N, D)\n",
    "demo = np.zeros((int(N), int(D)))\n",
    "for row in tqdm(static_data.itertuples()):\n",
    "    print(row.ts_ind)\n",
    "    demo[int(row.ts_ind), static_var_to_ind[row.variable]] = row.value\n",
    "# print('Demo after tqdm command \\n',demo[:10])\n",
    "# Normalize static data.\n",
    "means = demo.mean(axis=0, keepdims=True)  # quite sparse\n",
    "stds = demo.std(axis=0, keepdims=True)\n",
    "stds = (stds == 0)*1 + (stds != 0)*stds\n",
    "demo = (demo-means)/stds\n",
    "# print('Demo after normalisation \\n',demo[:10])\n",
    "# Get variable indices.\n",
    "varis = sorted(list(set(data.variable)))\n",
    "V = len(varis)  # 129 for \\=12 with varis all variables except for static ones\n",
    "print('varis', varis, V)\n",
    "var_to_ind = inv_list(varis, start=1)\n",
    "data['vind'] = data.variable.map(var_to_ind)\n",
    "data = data[['ts_ind', 'vind', 'hour', 'value']].sort_values(by=['ts_ind', 'vind', 'hour'])\n",
    "# print('data vind\\n', data['vind'], '\\n data\\n',data)\n",
    "# Find max_len.\n",
    "fore_max_len = 880  # hard coded max_len of vars\n",
    "# Get forecast inputs and outputs.\n",
    "fore_times_ip = []\n",
    "fore_values_ip = []\n",
    "fore_varis_ip = []\n",
    "fore_op = []\n",
    "fore_op_awesome = []\n",
    "fore_inds = []\n",
    "for w in tqdm(range(25, 124, 4)):  # range(20, 124, 4), pred_window=2\n",
    "    pred_data = data.loc[(data.hour>=w)&(data.hour<=w+24)]\n",
    "    pred_data = pred_data.groupby(['ts_ind', 'vind']).agg({'value':'first'}).reset_index()\n",
    "    pred_data['vind_value'] = pred_data[['vind', 'value']].values.tolist()\n",
    "    pred_data = pred_data.groupby('ts_ind').agg({'vind_value':list}).reset_index()\n",
    "    pred_data['vind_value'] = pred_data['vind_value'].apply(f)   \n",
    "\n",
    "    obs_data = data.loc[(data.hour < w) & (data.hour >= w-24)]\n",
    "    obs_data = obs_data.loc[obs_data.ts_ind.isin(pred_data.ts_ind)]\n",
    "    obs_data = obs_data.groupby('ts_ind').head(fore_max_len)\n",
    "    obs_data = obs_data.groupby('ts_ind').agg({'vind': list, 'hour': list, 'value': list}).reset_index()\n",
    "    #Michi: Hier hole ich mir die 24 Stunden vor, und 24 Stunden nach einem bestimmten Zeitpunkt raus\n",
    "    for pred_window  in range(-24, 24, 1):\n",
    "        pred_data = data.loc[(data.hour >= w+pred_window) & (data.hour <= w+1 +pred_window)]\n",
    "        pred_data = pred_data.groupby(['ts_ind', 'vind']).agg({'value': 'first'}).reset_index()\n",
    "        pred_data['vind_value'+str(pred_window)] = pred_data[['vind', 'value']].values.tolist()\n",
    "        pred_data = pred_data.groupby('ts_ind').agg({'vind_value'+str(pred_window): list}).reset_index()\n",
    "        pred_data['vind_value'+str(pred_window)] = pred_data['vind_value'+str(pred_window)].apply(f)  # 721 entries with 2*129 vind_values\n",
    "        obs_data = obs_data.merge(pred_data, on='ts_ind')\n",
    "\n",
    "    for col in ['vind', 'hour', 'value']:\n",
    "        obs_data[col] = obs_data[col].apply(pad)\n",
    "    fore_op_awesome.append(np.array(list([list(obs_data['vind_value'+str(pred_window)]) for pred_window in range(-24, 24, 1)])))\n",
    "    #print(fore_op_awesome[-1].shape)\n",
    "    #raise Exception\n",
    "    #fore_op.append(np.array(list(obs_data.vind_value)))\n",
    "    fore_inds.append(np.array([int(x) for x in list(obs_data.ts_ind)]))\n",
    "    fore_times_ip.append(np.array(list(obs_data.hour)))\n",
    "    fore_values_ip.append(np.array(list(obs_data.value)))\n",
    "    fore_varis_ip.append(np.array(list(obs_data.vind)))\n",
    "    \n",
    "del data\n",
    "fore_times_ip = np.concatenate(fore_times_ip, axis=0)\n",
    "fore_values_ip = np.concatenate(fore_values_ip, axis=0)\n",
    "fore_varis_ip = np.concatenate(fore_varis_ip, axis=0)\n",
    "\n",
    "fore_op_awesome = np.concatenate(fore_op_awesome, axis=1)\n",
    "print(fore_op_awesome.shape)\n",
    "fore_op = np.swapaxes(fore_op_awesome, 0, 1)\n",
    "print(fore_op.shape)\n",
    "\n",
    "#raise Exception\n",
    "fore_inds = np.concatenate(fore_inds, axis=0)\n",
    "fore_inds = fore_inds.astype(int)\n",
    "fore_demo = demo[fore_inds]\n",
    "# Get train and valid ts_ind for forecast task.\n",
    "train_sub = oc.loc[oc.ts_ind.isin(train_ind)].SUBJECT_ID.unique()\n",
    "valid_sub = oc.loc[oc.ts_ind.isin(valid_ind)].SUBJECT_ID.unique()\n",
    "rem_sub = oc.loc[~oc.SUBJECT_ID.isin(np.concatenate((train_ind, valid_ind)))].SUBJECT_ID.unique()\n",
    "bp = int(0.8*len(rem_sub))\n",
    "train_sub = np.concatenate((train_sub, rem_sub[:bp]))\n",
    "valid_sub = np.concatenate((valid_sub, rem_sub[bp:]))\n",
    "train_ind = oc.loc[oc.SUBJECT_ID.isin(train_sub)].ts_ind.unique()  # Add remaining ts_ind s of train subjects.\n",
    "valid_ind = oc.loc[oc.SUBJECT_ID.isin(valid_sub)].ts_ind.unique()  # Add remaining ts_ind s of train subjects.\n",
    "# Generate 3 sets of inputs and outputs.\n",
    "train_ind = np.argwhere(np.in1d(fore_inds, train_ind)).flatten()\n",
    "valid_ind = np.argwhere(np.in1d(fore_inds, valid_ind)).flatten()\n",
    "fore_train_ip = [ip[train_ind] for ip in [fore_demo, fore_times_ip, fore_values_ip, fore_varis_ip]]\n",
    "fore_valid_ip = [ip[valid_ind] for ip in [fore_demo, fore_times_ip, fore_values_ip, fore_varis_ip]]\n",
    "del fore_times_ip, fore_values_ip, fore_varis_ip, demo, fore_demo\n",
    "fore_train_op = fore_op[train_ind]\n",
    "fore_valid_op = fore_op[valid_ind]\n",
    "del fore_op\n",
    "\n",
    "print('lengths of rem_sub, fore_train_ip[1], fore_valid_ip[0]')\n",
    "print(len(rem_sub), fore_train_ip[1].shape, fore_valid_ip[0].shape)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c06d2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33632ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533146ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-23T12:50:22.928639Z",
     "iopub.status.busy": "2023-11-23T12:50:22.928294Z",
     "iopub.status.idle": "2023-11-23T12:54:40.326822Z",
     "shell.execute_reply": "2023-11-23T12:54:40.325636Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "V=131\n",
    "fore_max_len = 880\n",
    "# Read data.\n",
    "# data_path = '/home/mitarb/fracarolli/files/230613_STraTS_preprocessed/mimic_iii_preprocessed.pkl'\n",
    "data, oc, train_ind, valid_ind, test_ind = pickle.load(open(data_path, 'rb'))\n",
    "\n",
    "import itertools\n",
    "if sepsis_check:\n",
    "    test_ind = pd.read_csv('infec_test.csv', header=None)\n",
    "    test_ind = list(itertools.chain(*test_ind.values.tolist()))\n",
    "    test_ind = list(map(int, test_ind))\n",
    "    print('hi')\n",
    "\n",
    "means_stds = data.groupby(\"variable\").agg({\"mean\":\"first\", \"std\":\"first\"})\n",
    "#print(means_stds)\n",
    "mean_std_dict = dict()\n",
    "print(means_stds.keys)\n",
    "for pos, row in means_stds.iterrows():\n",
    "    print(pos)\n",
    "    print(row)\n",
    "    mean_std_dict[pos] = (float(row[\"mean\"]), float(row[\"std\"]))\n",
    "print(mean_std_dict)\n",
    "#raise Exception\n",
    "# Filter labeled data in first 24h.\n",
    "data = data.loc[data.ts_ind.isin(np.concatenate((train_ind, valid_ind, test_ind), axis=-1))]\n",
    "data = data.loc[(data.hour >= 0) & (data.hour <= 48)]\n",
    "old_oc = oc\n",
    "oc = oc.loc[oc.ts_ind.isin(np.concatenate((train_ind, valid_ind, test_ind), axis=-1))]\n",
    "# Fix age.\n",
    "data.loc[(data.variable == 'Age') & (data.value > 200), 'value'] = 91.4\n",
    "# Get y and N.\n",
    "y = np.array(oc.sort_values(by='ts_ind')['in_hospital_mortality']).astype('float32')\n",
    "print(y.shape)\n",
    "#raise Exception\n",
    "data['ts_ind'] = data['ts_ind'].astype(int) \n",
    "N = int(data.ts_ind.max() + 1)\n",
    "\n",
    "# Create demographic/static data\n",
    "# Get static data with mean fill and missingness indicator.\n",
    "static_varis = ['Age', 'Gender']\n",
    "ii = data.variable.isin(static_varis)\n",
    "static_data = data.loc[ii]\n",
    "data = data.loc[~ii]  # reduce data to non-demo/non-static\n",
    "static_var_to_ind = inv_list(static_varis)\n",
    "D = len(static_varis)\n",
    "demo = np.zeros((int(N), D))\n",
    "for row in tqdm(static_data.itertuples()):\n",
    "    demo[int(row.ts_ind), static_var_to_ind[row.variable]] = row.value\n",
    "# Normalize static data.\n",
    "means = demo.mean(axis=0, keepdims=True)\n",
    "stds = demo.std(axis=0, keepdims=True)\n",
    "stds = (stds == 0)*1 + (stds != 0)*stds\n",
    "demo = (demo-means)/stds\n",
    "\n",
    "# Get N, V, var_to_ind.\n",
    "varis = sorted(list(set(data.variable)))\n",
    "var_to_ind = inv_list(varis, start=1)\n",
    "data['vind'] = data.variable.map(var_to_ind)\n",
    "data = data[['ts_ind', 'vind', 'hour', 'value']].sort_values(by=['ts_ind', 'vind', 'hour'])\n",
    "# Add obs index.\n",
    "data = data.sort_values(by=['ts_ind']).reset_index(drop=True)\n",
    "data = data.reset_index().rename(columns={'index': 'obs_ind'})\n",
    "data = data.merge(data.groupby('ts_ind').agg({'obs_ind': 'min'}).reset_index().rename(columns={\n",
    "                                                            'obs_ind': 'first_obs_ind'}), on='ts_ind')\n",
    "data['obs_ind'] = data['obs_ind'] - data['first_obs_ind']\n",
    "# Find max_len.\n",
    "data2 = data.loc[(data.hour >= 0) & (data.hour <= 24)]\n",
    "max_len = data2.obs_ind.max()+1\n",
    "print('max_len', max_len)\n",
    "# Generate times_ip and values_ip matrices.\n",
    "times_inp = np.zeros((N, max_len), dtype='float32')\n",
    "values_inp = np.zeros((N, max_len), dtype='float32')\n",
    "varis_inp = np.zeros((N, max_len), dtype='int32')\n",
    "for row in tqdm(data2.itertuples()):\n",
    "    ts_ind = row.ts_ind\n",
    "    l = row.obs_ind\n",
    "    times_inp[ts_ind, l] = row.hour\n",
    "    values_inp[ts_ind, l] = row.value\n",
    "    varis_inp[ts_ind, l] = row.vind\n",
    "\n",
    "w=0\n",
    "\n",
    "fore_in = []\n",
    "\n",
    "pred_data = data.loc[(data.hour>=0)&(data.hour<=48)]\n",
    "pred_data = pred_data.groupby(['ts_ind', 'vind']).agg({'value':'first'}).reset_index()\n",
    "pred_data['vind_value'] = pred_data[['vind', 'value']].values.tolist()\n",
    "pred_data = pred_data.groupby('ts_ind').agg({'vind_value':list}).reset_index()\n",
    "pred_data['vind_value'] = pred_data['vind_value'].apply(f)   \n",
    "\n",
    "obs_data = data.loc[(data.hour < 24) & (data.hour >= 0)]\n",
    "resultdict = dict()\n",
    "for ts_ind in obs_data.ts_ind:\n",
    "    resultdict[ts_ind] = [[]]\n",
    "obs_data = obs_data.loc[obs_data.ts_ind.isin(pred_data.ts_ind)]\n",
    "obs_data = obs_data.groupby('ts_ind').head(fore_max_len)\n",
    "obs_data = obs_data.groupby('ts_ind').agg({'vind': list, 'hour': list, 'value': list}).reset_index()\n",
    "\n",
    "for pred_window  in range(0, 48, 1):\n",
    "    print(pred_window)\n",
    "    pred_data = data.loc[(data.hour >= w+pred_window) & (data.hour <= w+1 +pred_window)]\n",
    "    pred_data = pred_data.groupby(['ts_ind', 'vind']).agg({'value': 'first'}).reset_index()\n",
    "    pred_data['vind_value'+str(pred_window)] = pred_data[['vind', 'value']].values.tolist()\n",
    "    pred_data = pred_data.groupby('ts_ind').agg({'vind_value'+str(pred_window): list}).reset_index()\n",
    "    pred_data['vind_value'+str(pred_window)] = pred_data['vind_value'+str(pred_window)].apply(f)  # 721 entries with 2*129 vind_values\n",
    "    obs_data = obs_data.merge(pred_data, on='ts_ind')\n",
    "\n",
    "\n",
    "obs_data = obs_data.sort_values(by=['ts_ind']).reset_index(drop=True)\n",
    "obs_data = obs_data.reset_index().rename(columns={'index': 'obs_ind'})\n",
    "obs_data = obs_data.merge(data.groupby('ts_ind').agg({'obs_ind': 'min'}).reset_index().rename(columns={\n",
    "                                                            'obs_ind': 'first_obs_ind'}), on='ts_ind')\n",
    "obs_data['obs_ind'] = obs_data['obs_ind'] - obs_data['first_obs_ind']\n",
    "\n",
    "\n",
    "N = int(obs_data.ts_ind.max() + 1)\n",
    "times_inp = np.zeros((N, max_len), dtype='float32')\n",
    "values_inp = np.zeros((N, max_len), dtype='float32')\n",
    "varis_inp = np.zeros((N, max_len), dtype='int32')\n",
    "for col in ['vind', 'hour', 'value']:\n",
    "        obs_data[col] = obs_data[col].apply(pad)\n",
    "\n",
    "\n",
    "times_inp=(np.array(list(obs_data.hour)))\n",
    "values_inp=(np.array(list(obs_data.value)))\n",
    "varis_inp=(np.array(list(obs_data.vind)))\n",
    "\n",
    "print(\"times_shape_blub\", times_inp.shape)\n",
    "index=np.array([int(x) for x in list(obs_data.ts_ind)])\n",
    "demo = demo[index]\n",
    "blub = (np.array(list([list(obs_data['vind_value'+str(pred_window)]) for pred_window in range(0, 48, 1)])))\n",
    "print(\"bleb\", blub.shape)\n",
    "op = np.swapaxes(blub, 0, 1)\n",
    "\n",
    "weird_oc = oc.loc[oc.ts_ind.isin(obs_data.ts_ind)]\n",
    "y = np.array(weird_oc.sort_values(by='ts_ind')['in_hospital_mortality']).astype('float32')\n",
    "print(y.shape)\n",
    "data.drop(columns=['obs_ind', 'first_obs_ind'], inplace=True)\n",
    "\n",
    "train_ind = [x for x in train_ind if x < op.shape[0]]\n",
    "valid_ind = [x for x in valid_ind if x < op.shape[0]]\n",
    "test_ind = [x for x in test_ind if x < op.shape[0]]\n",
    "\n",
    "train_input = op[train_ind]\n",
    "valid_input = op[valid_ind]\n",
    "test_input = op[test_ind]\n",
    "\n",
    "# Generate 3 sets of inputs and outputs.\n",
    "train_ip = [ip[train_ind] for ip in [demo, times_inp, values_inp, varis_inp]]\n",
    "valid_ip = [ip[valid_ind] for ip in [demo, times_inp, values_inp, varis_inp]]\n",
    "test_ip = [ip[test_ind] for ip in [demo, times_inp, values_inp, varis_inp]]\n",
    "del demo, times_inp, values_inp, varis_inp  # warum wird demo nicht gelöscht?\n",
    "\n",
    "if test_cond == 1:\n",
    "    tr_ind = [divmod(tr, 12)[0] for tr in train_ind]\n",
    "    va_ind = [divmod(tr, 12)[0] for tr in valid_ind]\n",
    "    te_ind = [divmod(tr, 12)[0] for tr in test_ind]\n",
    "    \n",
    "    train_op = y[tr_ind]  # is a problem for the test case...\n",
    "    valid_op = y[va_ind]\n",
    "    test_op = y[te_ind]\n",
    "else:\n",
    "    train_op = y[train_ind]  # is a problem for the test case...\n",
    "    valid_op = y[valid_ind]\n",
    "    test_op = y[test_ind]\n",
    "print('y is:', y)\n",
    "del y\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4a17fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sofa(matrix, var_to_ind): #24x131 matrix\n",
    "    # GCS: min_eye, min_motor, min_verbal = 5, 5, 5\n",
    "    print(matrix.size())\n",
    "    #raise Exception\n",
    "    key =\"GCS_eye\"\n",
    "    var_to_ind = {x:i-1 for x,i in var_to_ind.items()}\n",
    "    a=matrix[:, var_to_ind[key]][torch.nonzero(matrix[:, var_to_ind[key]], as_tuple=True)]\n",
    "    print(a)\n",
    "    min_eye = min(matrix[:, var_to_ind[key]][torch.nonzero(matrix[:, var_to_ind[key]], as_tuple=True)]*mean_std_dict[key][1]+mean_std_dict[key][0], default=4)\n",
    "    key = \"GCS_motor\"\n",
    "    min_motor = min(matrix[:, var_to_ind[key]][torch.nonzero(matrix[:, var_to_ind[key]], as_tuple=True)]*mean_std_dict[key][1]+mean_std_dict[key][0], default=6)\n",
    "    key = \"GCS_verbal\"\n",
    "    min_verbal = min(matrix[:, var_to_ind[key]][torch.nonzero(matrix[:, var_to_ind[key]], as_tuple=True)]*mean_std_dict[key][1]+mean_std_dict[key][0], default=5)\n",
    "    \n",
    "\n",
    "    GCS = min_eye + min_motor + min_verbal\n",
    "    if GCS > 14: GCS_sofa = 0\n",
    "    elif GCS > 12: GCS_sofa = 1\n",
    "    elif GCS > 9:  GCS_sofa = 2\n",
    "    elif GCS > 5:  GCS_sofa = 3\n",
    "    else: GCS_sofa = 4\n",
    "    #print('GCS_sofa is', GCS_sofa, ';     GCS is', GCS,'; GCS eye', min_eye, '; GCS motor', min_motor, '; GCS verbal', min_verbal)\n",
    "\n",
    "    key = \"Bilirubin (Total)\"\n",
    "    bilir = max(matrix[:, var_to_ind[key]][torch.nonzero(matrix[:, var_to_ind[key]], as_tuple=True)]*mean_std_dict[key][1]+mean_std_dict[key][0], default=1)\n",
    "    if bilir > 12: bilir_sofa = 4\n",
    "    elif bilir > 6: bilir_sofa = 3\n",
    "    elif bilir > 2: bilir_sofa = 2\n",
    "    elif bilir > 1.2: bilir_sofa = 1\n",
    "    else: bilir_sofa = 0\n",
    "    #print('bilir_Sofa is', bilir_sofa, ';   bilirubin is', bilir)\n",
    "    \n",
    "    # Coagulation (Platelets)\n",
    "    key = \"Platelet Count\"\n",
    "    plate = max(matrix[:, var_to_ind[key]][torch.nonzero(matrix[:, var_to_ind[key]], as_tuple=True)]*mean_std_dict[key][1]+mean_std_dict[key][0], default=160)\n",
    "    if plate > 150: plate_sofa = 0\n",
    "    elif plate > 100: plate_sofa = 1\n",
    "    elif plate > 50: plate_sofa = 2\n",
    "    elif plate > 20: plate_sofa = 3\n",
    "    else: plate_sofa = 4\n",
    "    #print('plate_sofa is', plate_sofa, ';   platelet count is', plate)\n",
    "    \n",
    "    # print('Urinmenge 24h', sum(data_var[data_var['variable']=='Urine']['value2']))\n",
    "\n",
    "    key = \"Urine\"\n",
    "    urine = sum(matrix[:, var_to_ind[key]][torch.nonzero(matrix[:, var_to_ind[key]], as_tuple=True)]*mean_std_dict[key][1]+mean_std_dict[key][0])\n",
    "    key = \"Creatinine Blood\"\n",
    "    creat = max(matrix[:, var_to_ind[key]][torch.nonzero(matrix[:, var_to_ind[key]], as_tuple=True)]*mean_std_dict[key][1]+mean_std_dict[key][0], default=1)\n",
    "    \n",
    "    if (urine < 200) or (creat > 5): renal_sofa = 4\n",
    "    elif  (urine < 500) or (creat > 3.5): renal_sofa = 3\n",
    "    elif creat > 2.0: renal_sofa = 2\n",
    "    elif creat > 1.2: renal_sofa = 1\n",
    "    else: renal_sofa = 0\n",
    "    #print('renal_sofa:',renal_sofa,';       urine 24:',urine,'; creat:', creat)\n",
    "    \n",
    "    CS_data = get_CS(matrix, var_to_ind)\n",
    "    cs_sofa = CS_SOFA(CS_data)\n",
    "    \n",
    "    #cs_sofa = 0\n",
    "    key=\"FiO2\"\n",
    "    fio2 = (matrix[:, var_to_ind[key]]*mean_std_dict[key][1]+mean_std_dict[key][0])\n",
    "    key=\"PO2\"\n",
    "    po2 = (matrix[:, var_to_ind[key]]*mean_std_dict[key][1]+mean_std_dict[key][0])\n",
    "    PaO2FiO2 = 100*po2/fio2\n",
    "    print(\"size\", PaO2FiO2.size())\n",
    "    PaO2FiO2 = PaO2FiO2[torch.nonzero(PaO2FiO2, as_tuple=True)]\n",
    "    pao2fio2 = min(PaO2FiO2)\n",
    "    if pao2fio2<100: resp=4\n",
    "    elif pao2fio2<200: resp=3\n",
    "    elif pao2fio2<300:resp=2\n",
    "    elif pao2fio2<400:resp=1\n",
    "    else: resp=0\n",
    "    return GCS_sofa, cs_sofa, resp, plate_sofa, bilir_sofa, renal_sofa\n",
    "\n",
    "def get_CS(matrix, var_to_ind):\n",
    "    #data_var = data_pat[data_pat['variable'].isin(['Dobutamine','Dopamine','Epinephrine','Norepinephrine','Weight'])]\n",
    "    #data_var['value2'] = data_var['value']*data_var['std']+data_var['mean']\n",
    "    \n",
    "    #weight = min(data_var[data_var['variable']=='Weight']['value2'], default=80)  # set default weight to 80kg.\n",
    "    key = \"Weight\"\n",
    "    weight = min(matrix[:, var_to_ind[key]][torch.nonzero(matrix[:, var_to_ind[key]], as_tuple=True)], default=80)#*mean_std_dict[key][1]+mean_std_dict[key][0], default=1)\n",
    "    key = \"Dopamine\"\n",
    "    try:\n",
    "        data_dop = max(matrix[:, var_to_ind[key]][torch.nonzero(matrix[:, var_to_ind[key]], as_tuple=True)]*mean_std_dict[key][1]+mean_std_dict[key][0], default=1)\n",
    "        data_dop = data_dop /60/weight*1000\n",
    "    except:\n",
    "        data_dop = 0\n",
    "\n",
    "    key = \"Dobutamine\"\n",
    "    \n",
    "    try:\n",
    "        data_dobu = max(matrix[:, var_to_ind[key]][torch.nonzero(matrix[:, var_to_ind[key]], as_tuple=True)]*mean_std_dict[key][1]+mean_std_dict[key][0], default=1)\n",
    "        data_dobu = data_dobu  /60/weight*1000\n",
    "    except:\n",
    "        data_dobu = 0\n",
    "    key = \"Epinephrine\"\n",
    "    \n",
    "    try:\n",
    "        data_epi = max(matrix[:, var_to_ind[key]][torch.nonzero(matrix[:, var_to_ind[key]], as_tuple=True)]*mean_std_dict[key][1]+mean_std_dict[key][0], default=1)\n",
    "        data_epi = data_epi  /60/weight*1000\n",
    "    except:\n",
    "        data_epi = 0\n",
    "    key = \"Norepinephrine\"\n",
    "    \n",
    "    try:\n",
    "        data_nore = max(matrix[:, var_to_ind[key]][torch.nonzero(matrix[:, var_to_ind[key]], as_tuple=True)]*mean_std_dict[key][1]+mean_std_dict[key][0], default=1)\n",
    "        data_nore = data_nore /60/weight*1000\n",
    "    except:\n",
    "        data_nore = 0\n",
    "        \n",
    "\n",
    "\n",
    "    key = \"SBP\"\n",
    "    SBP = (matrix[:, var_to_ind[key]]*mean_std_dict[key][1]+mean_std_dict[key][0])\n",
    "\n",
    "    key = \"DBP\"\n",
    "    DBP = (matrix[:, var_to_ind[key]]*mean_std_dict[key][1]+mean_std_dict[key][0])\n",
    "\n",
    "    MAP = 2/3 * DBP + 1/3 * SBP\n",
    "    MAP = min(MAP[torch.nonzero(MAP, as_tuple=True)], default=100)\n",
    "                 \n",
    "        \n",
    "    return MAP, data_dop, data_dobu, data_epi, data_nore \n",
    "    \n",
    "def CS_SOFA(data):\n",
    "    map = data[0]\n",
    "    dop, dobu, epi, nore = data[1:5]\n",
    "    # print('CS data: mdden', data)\n",
    "    if (dop > 15) or (epi > 0.1) or (nore > 0.01): CS = 4\n",
    "    elif (dop > 5) or (epi > 0) or (nore > 0): CS = 3\n",
    "    elif (dop > 0) or (dobu > 0): CS = 2\n",
    "    elif map < 70: CS = 1\n",
    "    else: CS = 0\n",
    "    # print('CS Sofa is:', CS)\n",
    "    return CS \n",
    "    \n",
    "factor_keys =[\"GCS_eye\", \"GCS_motor\", \"GCS_verbal\", \"Bilirubin (Total)\", \"Platelet Count\", \"Urine\", \"Creatinine Blood\", \"FiO2\", \"PO2\", \"Dopamine\", \"Dobutamine\",\n",
    "\"Epinephrine\", \"Norepinephrine\", \"SBP\", \"DBP\"]\n",
    "factor_indices = sorted([var_to_ind[x]-1 for x in factor_keys])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd8241f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-23T12:56:12.977077Z",
     "iopub.status.busy": "2023-11-23T12:56:12.976625Z",
     "iopub.status.idle": "2023-11-23T12:56:13.037525Z",
     "shell.execute_reply": "2023-11-23T12:56:13.036229Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#Michi: die ganzen Argparses könnte man noch anders implementieren, wollte es nur schnell zum laufen bringen.\n",
    "import argparse\n",
    "parser = argparse.ArgumentParser(description='Autoformer & Transformer family for Time Series Forecasting')\n",
    "\n",
    "# basic config\n",
    "parser.add_argument('--is_training', type=int, required=False, default=1, help='status')\n",
    "parser.add_argument('--train_only', type=bool, required=False, default=False, help='perform training on full input dataset without validation and testing')\n",
    "parser.add_argument('--model_id', type=str, required=False, default='test', help='model id')\n",
    "parser.add_argument('--model', type=str, required=False, default='Autoformer',\n",
    "                    help='model name, options: [Autoformer, Informer, Transformer]')\n",
    "\n",
    "# data loader\n",
    "parser.add_argument('--data', type=str, required=False, default='ETTm1', help='dataset type')\n",
    "parser.add_argument('--root_path', type=str, default='./data/ETT/', help='root path of the data file')\n",
    "parser.add_argument('--data_path', type=str, default='ETTh1.csv', help='data file')\n",
    "parser.add_argument('--features', type=str, default='M',\n",
    "                    help='forecasting task, options:[M, S, MS]; M:multivariate predict multivariate, S:univariate predict univariate, MS:multivariate predict univariate')\n",
    "parser.add_argument('--target', type=str, default='OT', help='target feature in S or MS task')\n",
    "parser.add_argument('--freq', type=str, default='h',\n",
    "                    help='freq for time features encoding, options:[s:secondly, t:minutely, h:hourly, d:daily, b:business days, w:weekly, m:monthly], you can also use more detailed freq like 15min or 3h')\n",
    "parser.add_argument('--checkpoints', type=str, default='./checkpoints/', help='location of model checkpoints')\n",
    "\n",
    "# forecasting task\n",
    "parser.add_argument('--seq_len', type=int, default=24, help='input sequence length')\n",
    "parser.add_argument('--label_len', type=int, default=0, help='start token length')\n",
    "parser.add_argument('--pred_len', type=int, default=24, help='prediction sequence length')\n",
    "\n",
    "\n",
    "# DLinear\n",
    "parser.add_argument('--individual', action='store_true', default=False, help='DLinear: a linear layer for each variate(channel) individually')\n",
    "# Formers \n",
    "#Michi: bisher nur default=3 zum laufen gebracht\n",
    "parser.add_argument('--embed_type', type=int, default=3, help='0: default 1: value embedding + temporal embedding + positional embedding 2: value embedding + temporal embedding 3: value embedding + positional embedding 4: value embedding')\n",
    "parser.add_argument('--enc_in', type=int, default=262, help='encoder input size') # DLinear with --individual, use this hyperparameter as the number of channels\n",
    "parser.add_argument('--dec_in', type=int, default=131, help='decoder input size')\n",
    "parser.add_argument('--c_out', type=int, default=131, help='output size')\n",
    "parser.add_argument('--d_model', type=int, default=512, help='dimension of model')\n",
    "parser.add_argument('--n_heads', type=int, default=8, help='num of heads')\n",
    "parser.add_argument('--e_layers', type=int, default=2, help='num of encoder layers')\n",
    "parser.add_argument('--d_layers', type=int, default=1, help='num of decoder layers')\n",
    "parser.add_argument('--d_ff', type=int, default=2048, help='dimension of fcn')\n",
    "parser.add_argument('--moving_avg', type=int, default=25, help='window size of moving average')\n",
    "parser.add_argument('--factor', type=int, default=1, help='attn factor')\n",
    "parser.add_argument('--distil', action='store_false',\n",
    "                    help='whether to use distilling in encoder, using this argument means not using distilling',\n",
    "                    default=True)\n",
    "parser.add_argument('--dropout', type=float, default=0.05, help='dropout')\n",
    "parser.add_argument('--embed', type=str, default='timeF',\n",
    "                    help='time features encoding, options:[timeF, fixed, learned]')\n",
    "parser.add_argument('--activation', type=str, default='gelu', help='activation')\n",
    "parser.add_argument('--output_attention', action='store_true', help='whether to output attention in ecoder')\n",
    "parser.add_argument('--do_predict', action='store_true', help='whether to predict unseen future data')\n",
    "\n",
    "# optimization\n",
    "parser.add_argument('--num_workers', type=int, default=10, help='data loader num workers')\n",
    "parser.add_argument('--itr', type=int, default=2, help='experiments times')\n",
    "parser.add_argument('--train_epochs', type=int, default=10, help='train epochs')\n",
    "parser.add_argument('--batch_size', type=int, default=32, help='batch size of train input data')\n",
    "parser.add_argument('--patience', type=int, default=3, help='early stopping patience')\n",
    "parser.add_argument('--learning_rate', type=float, default=0.0001, help='optimizer learning rate')\n",
    "parser.add_argument('--des', type=str, default='test', help='exp description')\n",
    "parser.add_argument('--loss', type=str, default='mse', help='loss function')\n",
    "parser.add_argument('--lradj', type=str, default='type1', help='adjust learning rate')\n",
    "parser.add_argument('--use_amp', action='store_true', help='use automatic mixed precision training', default=False)\n",
    "\n",
    "# GPU\n",
    "parser.add_argument('--use_gpu', type=bool, default=True, help='use gpu')\n",
    "parser.add_argument('--gpu', type=int, default=0, help='gpu')\n",
    "parser.add_argument('--use_multi_gpu', action='store_true', help='use multiple gpus', default=False)\n",
    "parser.add_argument('--devices', type=str, default='0,1,2,3', help='device ids of multile gpus')\n",
    "parser.add_argument('--test_flop', action='store_true', default=False, help='See utils/tools for usage')\n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "import importlib\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b118361e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-23T12:56:13.044668Z",
     "iopub.status.busy": "2023-11-23T12:56:13.044206Z",
     "iopub.status.idle": "2023-11-23T14:39:06.633300Z",
     "shell.execute_reply": "2023-11-23T14:39:06.631896Z"
    }
   },
   "outputs": [],
   "source": [
    "import models.InformerAutoregressiveWithForcedDecoding as autoformer\n",
    "importlib.reload(autoformer)\n",
    "model = autoformer.Model(args).cuda()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875c7053",
   "metadata": {},
   "source": [
    "# Crossinteractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bc3213",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "torch.cuda.empty_cache()\n",
    "import models.InformerAutoregressiveWithForcedDecoding as autoformer\n",
    "importlib.reload(autoformer)\n",
    "batch_size, lr, patience = 32, 0.0005, 6  # batch_size increased, patience 10 --> 6\n",
    "d, N, he, dropout = 50, 2, 4, 0.2\n",
    "model = autoformer.Model(args).cuda()\n",
    "\n",
    "\n",
    "# Load pretrained weights here.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "#fore_savepath = 'mimic_iii_24h_strats_no_interp_with_ss_fore_blubdiblub.pytorch'\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'max', patience=3)\n",
    "model.load_state_dict(torch.load(\"mimic_iii_24h_strats_no_interp_with_ss_fore_informer_ims_fr_detach.pytorch\"))\n",
    "#model.load_state_dict(torch.load(fore_savepath))\n",
    "model.eval()\n",
    "y_true, y_pred = [], []\n",
    "print(test_input)\n",
    "pbar = tqdm(range(0, len(test_input), batch_size))\n",
    "loss_list = []\n",
    "test_loss = 0\n",
    "\n",
    "keys = [\"Dopamine\", \"Dobutamine\", \"Norepinephrine\"]\n",
    "\n",
    "#data_dop = max(matrix[:, var_to_ind[key]][torch.nonzero(matrix[:, var_to_ind[key]], as_tuple=True)]*mean_std_dict[key][1]+mean_std_dict[key][0], default=1)\n",
    "\n",
    "min_dict = dict()\n",
    "max_dict = dict()\n",
    "twentyfive_dict = dict()\n",
    "median_dict = dict()\n",
    "seventyfive_dict = dict()\n",
    "matrix = torch.tensor(test_input)\n",
    "for x in range(0, len(test_input)):\n",
    "\n",
    "    for key in keys:\n",
    "        try:\n",
    "            min_data = min(matrix[x, :, var_to_ind[key]][torch.nonzero(matrix[x, :, var_to_ind[key]], as_tuple=True)])#*mean_std_dict[key][1]+mean_std_dict[key][0], default=1)\n",
    "            if key not in min_dict:\n",
    "                min_dict[key] = min_data\n",
    "            else:\n",
    "                min_dict[key] = min([min_data, min_dict[key]])\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            max_data = max(matrix[x, :, var_to_ind[key]][torch.nonzero(matrix[x, :, var_to_ind[key]], as_tuple=True)])#*mean_std_dict[key][1]+mean_std_dict[key][0], default=1)\n",
    "\n",
    "            if key not in max_dict:\n",
    "                max_dict[key] = max_data\n",
    "            else:\n",
    "                max_dict[key] = max([max_data, max_dict[key]])\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            twentyfive_data = torch.quantile(matrix[x, :, var_to_ind[key]][torch.nonzero(matrix[x, :, var_to_ind[key]], as_tuple=True)], 0.25)#*mean_std_dict[key][1]+mean_std_dict[key][0], default=1)\n",
    "\n",
    "            if key not in twentyfive_dict:\n",
    "                twentyfive_dict[key] = twentyfive_data.item()\n",
    "            else:\n",
    "                twentyfive_dict[key] = max([twentyfive_data, twentyfive_dict[key]])\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            median_data = torch.quantile(matrix[x, :, var_to_ind[key]][torch.nonzero(matrix[x, :, var_to_ind[key]], as_tuple=True)], 0.50)#*mean_std_dict[key][1]+mean_std_dict[key][0], default=1)\n",
    "\n",
    "            if key not in median_dict:\n",
    "                median_dict[key] = median_data.item()\n",
    "            else:\n",
    "                median_dict[key] = max([median_data, median_dict[key]])\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            seventyfive_data = torch.quantile(matrix[x, :, var_to_ind[key]][torch.nonzero(matrix[x, :, var_to_ind[key]], as_tuple=True)], 0.75)#*mean_std_dict[key][1]+mean_std_dict[key][0], default=1)\n",
    "\n",
    "            if key not in seventyfive_dict:\n",
    "                seventyfive_dict[key] = seventyfive_data.item()\n",
    "            else:\n",
    "                seventyfive_dict[key] = max([seventyfive_data, seventyfive_dict[key]])\n",
    "        except:\n",
    "            pass\n",
    "print(min_dict, max_dict)\n",
    "resultdict = dict()\n",
    "for key in keys:\n",
    "    maxes, mins, twentyfives, medians, seventyfives = [], [], [], [], []\n",
    "    for start in pbar:\n",
    "        print(start)\n",
    "        matrix = torch.tensor(test_input[start:start+batch_size], dtype=torch.float32).cuda()\n",
    "        input_matrix = matrix[:, :24]\n",
    "\n",
    "        #torch.Size([32, 24, 129])\n",
    "        input_mark = torch.arange(0, input_matrix.size(1)).unsqueeze(0).repeat(input_matrix.size(0), 1).cuda()\n",
    "        #torch.Size([32, 24])\n",
    "        input_mask = input_matrix[:, :24, 131:]\n",
    "        output_matrix = matrix[:, 24:, :131]\n",
    "        #torch.Size([32, 24, 129])\n",
    "        output_mark = torch.arange(0, output_matrix.size(1)).unsqueeze(0).repeat(input_matrix.size(0), 1).cuda()\n",
    "        #torch.Size([32, 24])\n",
    "        output_mask = matrix[:, 24:, 131:]\n",
    "        #torch.Size([32, 24, 129])\n",
    "        #print(var_to_ind[key])\n",
    "        #print(min_dict[key])\n",
    "        #print(max_dict[key])\n",
    "        with torch.no_grad():\n",
    "            output_min = model(input_matrix, input_mark, None, None, trainn=False, change_dict = {var_to_ind[key]-1: min_dict[key]})\n",
    "            output_max = model(input_matrix, input_mark, None, None, trainn=False, change_dict = {var_to_ind[key]-1: max_dict[key]})\n",
    "            \n",
    "            output_25 = model(input_matrix, input_mark, None, None, trainn=False, change_dict = {var_to_ind[key]-1: twentyfive_dict[key]})\n",
    "            output_50 = model(input_matrix, input_mark, None, None, trainn=False, change_dict = {var_to_ind[key]-1: median_dict[key]})\n",
    "            output_75 = model(input_matrix, input_mark, None, None, trainn=False, change_dict = {var_to_ind[key]-1: seventyfive_dict[key]})\n",
    "        #print(output1, output2, output1==output2)\n",
    "        for x in output_min:\n",
    "            mins.append(x.cpu())\n",
    "        for x in output_max:\n",
    "            maxes.append(x.cpu())\n",
    "        for x in output_25:\n",
    "            twentyfives.append(x.cpu())\n",
    "        for x in output_50:\n",
    "            medians.append(x.cpu())\n",
    "        for x in output_75:\n",
    "            seventyfives.append(x.cpu())\n",
    "    print(len(maxes), maxes[0].size())\n",
    "    resultdict[(key, \"min\")]=mins\n",
    "    resultdict[(key, \"max\")]=maxes\n",
    "    resultdict[(key, \"25\")]=twentyfives\n",
    "    resultdict[(key, \"50\")]=medians\n",
    "    resultdict[(key, \"75\")]=seventyfives\n",
    "import pickle\n",
    "\n",
    "with open('crossinteractions.pickle', 'wb') as handle:\n",
    "    pickle.dump([resultdict, var_to_ind, min_dict, max_dict, mean_std_dict], handle)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178f1b03",
   "metadata": {},
   "source": [
    "# 8 Sofa Analysis IMS Informer TF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3b7295",
   "metadata": {},
   "source": [
    "# 7 Sofa Analysis DMS Informer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a553eb",
   "metadata": {},
   "source": [
    "# 10 Sofa Analysis IMS Informer Backprop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38621f04",
   "metadata": {},
   "source": [
    "# 9 Sofa Analysis IMS Informer Detach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87034366",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import models.InformerAutoregressiveFull as autoformer\n",
    "importlib.reload(autoformer)\n",
    "batch_size, lr, patience = 32, 0.0005, 6  # batch_size increased, patience 10 --> 6\n",
    "d, N, he, dropout = 50, 2, 4, 0.2\n",
    "model = autoformer.Model(args).cuda()\n",
    "# Load pretrained weights here.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "#fore_savepath = 'mimic_iii_24h_strats_no_interp_with_ss_fore_blubdiblub.pytorch'\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'max', patience=3)\n",
    "model.load_state_dict(torch.load(\"mimic_iii_24h_strats_no_interp_with_ss_fore_informer_ims_fr_detach2_full.pytorch\"))\n",
    "#model.load_state_dict(torch.load(fore_savepath))\n",
    "model.eval()\n",
    "y_true, y_pred = [], []\n",
    "print(test_input)\n",
    "pbar = tqdm(range(0, len(test_input), batch_size))\n",
    "loss_list = []\n",
    "test_loss = 0\n",
    "test_loss_part1 = 0\n",
    "test_loss_part2 = 0\n",
    "test_loss_sofa_variables = 0\n",
    "sofa_losses = []\n",
    "accuracy_list = []\n",
    "TP, TN, FP, FN = 0, 0, 0, 0\n",
    "test_loss_sum_square_total = 0\n",
    "test_loss_part1_sum_square_total = 0\n",
    "test_loss_part2_sum_square_total = 0\n",
    "test_loss_sofa_sum_square_total = 0\n",
    "for start in pbar:\n",
    "    print(start)\n",
    "    matrix = torch.tensor(test_input[start:start+batch_size], dtype=torch.float32).cuda()\n",
    "    input_matrix = matrix[:, :24]\n",
    "\n",
    "    #torch.Size([32, 24, 129])\n",
    "    input_mark = torch.arange(0, input_matrix.size(1)).unsqueeze(0).repeat(input_matrix.size(0), 1).cuda()\n",
    "    #torch.Size([32, 24])\n",
    "    input_mask = input_matrix[:, :24, 131:]\n",
    "    output_matrix = matrix[:, 24:, :131]\n",
    "    #torch.Size([32, 24, 129])\n",
    "    output_mark = torch.arange(0, output_matrix.size(1)).unsqueeze(0).repeat(input_matrix.size(0), 1).cuda()\n",
    "    #torch.Size([32, 24])\n",
    "    output_mask = matrix[:, 24:, 131:]\n",
    "    \n",
    "    output_mark = torch.arange(0, output_matrix.size(1)).unsqueeze(0).repeat(input_matrix.size(0), 1).cuda()\n",
    "    dec_inp = torch.zeros_like(output_matrix[:, -args.pred_len:, :]).float()\n",
    "    #torch.Size([32, 24, 129])\n",
    "    with torch.no_grad():\n",
    "        output = model(input_matrix, input_mark, dec_inp, output_mark, trainn=False)\n",
    "        loss = output_mask[:, -args.pred_len:, :]*(\n",
    "        output-output_matrix[:, -args.pred_len:, :])**2\n",
    "    loss_list.extend(loss.sum(axis=-1).mean(axis=-1).detach().cpu().tolist())\n",
    "    for outpu, real, mask, input_mat in zip(output, output_matrix, output_mask, input_matrix):\n",
    "        a = (get_sofa(outpu*mask, var_to_ind))\n",
    "        b = (get_sofa(real, var_to_ind))\n",
    "        c = (get_sofa(input_mat, var_to_ind)) #SOFA Source (First Day)\n",
    "        prediction_sofa_difference = (sum(a)-sum(c)) >= 2\n",
    "        gold_sofa_difference = (sum(b)-sum(c)) >= 2\n",
    "        accuracy_list.append(prediction_sofa_difference == gold_sofa_difference)\n",
    "        if gold_sofa_difference:\n",
    "            if prediction_sofa_difference: TP +=1\n",
    "            else: FP += 1\n",
    "        else:\n",
    "            if prediction_sofa_difference: TN +=1\n",
    "            else: FN += 1\n",
    "        c, d = torch.tensor(a, dtype=torch.float32), torch.tensor(b, dtype=torch.float32)\n",
    "        sofa_losses.append(((c-d)**2).sum(axis=-1).mean().item())\n",
    "    loss_old = loss\n",
    "    loss = loss.sum(axis=-1).mean(axis=-1).sum()\n",
    "    loss_part1 = loss_old[:, :8].sum(axis=-1).mean()\n",
    "    loss_part2 = loss_old[:, 8:].sum(axis=-1).mean()\n",
    "    test_loss+=loss\n",
    "    test_loss_part1+=loss_part1\n",
    "    test_loss_part2+=loss_part2\n",
    "    test_loss_sofa_variables+=loss_old[:, :, factor_indices].sum(axis=-1).mean()\n",
    "    test_loss_sum_square_total += torch.sum(loss_old.sum(axis=-1).mean(axis=-1)**2)\n",
    "    test_loss_part1_sum_square_total += torch.sum(loss_old[:, :8].sum(axis=-1).mean(axis=-1)**2)\n",
    "    test_loss_part2_sum_square_total += torch.sum(loss_old[:, 8:].sum(axis=-1).mean(axis=-1)**2)\n",
    "    test_loss_sofa_sum_square_total += torch.sum(loss_old[:, :, factor_indices].sum(axis=-1).mean(axis=-1)**2)\n",
    "test_loss_p = test_loss/len(test_input)\n",
    "se_loss_p = torch.sqrt(test_loss_sum_square_total / len(test_input) - test_loss_p**2)/len(test_input)\n",
    "test_loss_part1_p = test_loss_part1*batch_size/len(test_input)\n",
    "test_loss_part2_p = test_loss_part2*batch_size/len(test_input)\n",
    "test_loss_sofa_variables_p = test_loss_sofa_variables*batch_size/len(test_input)\n",
    "sofa_loss = sum(sofa_losses)/len(sofa_losses)\n",
    "accuracy_sepsis = sum(accuracy_list)/len(accuracy_list)\n",
    "se_accuracy_sepsis = np.sqrt(accuracy_sepsis * (1-accuracy_sepsis)/len(test_input))\n",
    "se_loss_part1_p = torch.sqrt(test_loss_part1_sum_square_total / len(test_input) - test_loss_part1_p**2)/len(test_input)\n",
    "se_loss_part2_p = torch.sqrt(test_loss_part2_sum_square_total / len(test_input) - test_loss_part2_p**2)/len(test_input)\n",
    "se_loss_sofa_p  = torch.sqrt(test_loss_sofa_sum_square_total / len(test_input)  - test_loss_sofa_variables_p**2)/len(test_input)\n",
    "print(test_loss_p, test_loss_part1_p, test_loss_part2_p, sofa_loss, test_loss_sofa_variables_p, accuracy_sepsis)\n",
    "print('Informer_IMS_SF_D',test_loss_p.item(), test_loss_part1_p.item(), test_loss_part2_p.item(), sofa_loss, test_loss_sofa_variables_p.item(), accuracy_sepsis, sep=',',file=open('results.txt','a'))\n",
    "\n",
    "\n",
    "\n",
    "model_name = '9&Dense&IMS&SF&No&'\n",
    "MSE_string = \"{:.3f}\".format(test_loss_p.item()) + \"$_{[\" + \"{:.3f}\".format(test_loss_p.item()-1.96*se_loss_p.item()) +','+ \"{:.3f}\".format(test_loss_p.item()+1.96*se_loss_p.item()) + ']}$&'\n",
    "SOFA_string = \"{:.3f}\".format(sofa_loss)+ '$_{['+\"{:.3f}\".format(sofa_loss-1.96*se_loss_sofa_p.item())+','+\"{:.3f}\".format(sofa_loss+1.96*se_loss_sofa_p.item())+ ']}$&'\n",
    "MSE8_string = \"{:.3f}\".format(test_loss_part1_p.item()) + \"$_{[\" + \"{:.3f}\".format(test_loss_part1_p.item()-1.96*se_loss_part1_p.item()) +','+ \"{:.3f}\".format(test_loss_part1_p.item()+1.96*se_loss_part1_p.item()) + ']}$&'\n",
    "MSE16_string = \"{:.3f}\".format(test_loss_part2_p.item()) + \"$_{[\" + \"{:.3f}\".format(test_loss_part2_p.item()-1.96*se_loss_part2_p.item()) +','+ \"{:.3f}\".format(test_loss_part2_p.item()+1.96*se_loss_part2_p.item()) + ']}$&'\n",
    "SOFA_acc_string = \"{:.2f}\".format(accuracy_sepsis*100)+ '$_{['+\"{:.2f}\".format(100*(accuracy_sepsis-1.96*se_accuracy_sepsis))+','+\"{:.2f}\".format(100*(accuracy_sepsis+1.96*se_accuracy_sepsis))+ ']}$&'\n",
    "print(accuracy_sepsis)\n",
    "print(MSE_string)\n",
    "print(SOFA_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07239a61",
   "metadata": {},
   "source": [
    "# Autof SOFA Analysis Autoformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cfa316",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import models.InformerAutoregressiveFullRegression as autoformer\n",
    "importlib.reload(autoformer)\n",
    "batch_size, lr, patience = 32, 0.0005, 6  # batch_size increased, patience 10 --> 6\n",
    "d, N, he, dropout = 50, 2, 4, 0.2\n",
    "model = autoformer.Model(args).cuda()\n",
    "# Load pretrained weights here.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "#fore_savepath = 'mimic_iii_24h_strats_no_interp_with_ss_fore_blubdiblub.pytorch'\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'max', patience=3)\n",
    "model.load_state_dict(torch.load(\"informer_regression.pytorch\"))\n",
    "#model.load_state_dict(torch.load(fore_savepath))\n",
    "model.eval()\n",
    "y_true, y_pred = [], []\n",
    "print(test_input)\n",
    "pbar = tqdm(range(0, len(test_input), batch_size))\n",
    "loss_list = []\n",
    "test_loss = 0\n",
    "test_loss_part1 = 0\n",
    "test_loss_part2 = 0\n",
    "test_loss_sofa_variables = 0\n",
    "sofa_losses = []\n",
    "accuracy_list = []\n",
    "TP, TN, FP, FN = 0, 0, 0, 0\n",
    "test_loss_sum_square_total = 0\n",
    "test_loss_part1_sum_square_total = 0\n",
    "test_loss_part2_sum_square_total = 0\n",
    "test_loss_sofa_sum_square_total = 0\n",
    "for start in pbar:\n",
    "    print(start)\n",
    "    matrix = torch.tensor(test_input[start:start+batch_size], dtype=torch.float32).cuda()\n",
    "    input_matrix = matrix[:, :24]\n",
    "\n",
    "    #torch.Size([32, 24, 129])\n",
    "    input_mark = torch.arange(0, input_matrix.size(1)).unsqueeze(0).repeat(input_matrix.size(0), 1).cuda()\n",
    "    #torch.Size([32, 24])\n",
    "    input_mask = input_matrix[:, :24, 131:]\n",
    "    output_matrix = matrix[:, 24:, :131]\n",
    "    #torch.Size([32, 24, 129])\n",
    "    output_mark = torch.arange(0, output_matrix.size(1)).unsqueeze(0).repeat(input_matrix.size(0), 1).cuda()\n",
    "    #torch.Size([32, 24])\n",
    "    output_mask = matrix[:, 24:, 131:]\n",
    "    \n",
    "    output_mark = torch.arange(0, output_matrix.size(1)).unsqueeze(0).repeat(input_matrix.size(0), 1).cuda()\n",
    "    dec_inp = torch.zeros_like(output_matrix[:, -args.pred_len:, :]).float()\n",
    "    #torch.Size([32, 24, 129])\n",
    "    with torch.no_grad():\n",
    "        output = model(input_matrix, input_mark, dec_inp, output_mark, trainn=False)\n",
    "    loss = 0\n",
    "    for outpu, real, mask, input_mat in zip(output, output_matrix, output_mask, input_matrix):\n",
    "        a = outpu.item()\n",
    "        b = (get_sofa(real, var_to_ind))\n",
    "        c = (get_sofa(input_mat, var_to_ind)) #SOFA Source (First Day)\n",
    "        \n",
    "        prediction_sofa_difference = (a-sum(c)) >= 2\n",
    "        gold_sofa_difference = (sum(b)-sum(c)) >= 2\n",
    "        print(a, sum(b))\n",
    "        loss_term = (outpu.squeeze()-sum(b))**2\n",
    "        loss+=loss_term\n",
    "        accuracy_list.append(prediction_sofa_difference == gold_sofa_difference)\n",
    "        if gold_sofa_difference:\n",
    "            if prediction_sofa_difference: TP +=1\n",
    "            else: FN += 1\n",
    "        else:\n",
    "            if prediction_sofa_difference: FP +=1\n",
    "            else: TN += 1\n",
    "    loss_old = loss\n",
    "    loss = loss.sum(axis=-1).mean(axis=-1).sum()\n",
    "\n",
    "    test_loss+=loss\n",
    "\n",
    "print(TP, FP, FN, TN)\n",
    "print()\n",
    "accuracy_sepsis = sum(accuracy_list)/len(accuracy_list)\n",
    "se_accuracy_sepsis = np.sqrt(accuracy_sepsis * (1-accuracy_sepsis)/len(test_input))\n",
    "print(accuracy_sepsis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868a1592",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import models.InformerAutoregressiveFullRegression as autoformer\n",
    "importlib.reload(autoformer)\n",
    "batch_size, lr, patience = 32, 0.0005, 6  # batch_size increased, patience 10 --> 6\n",
    "d, N, he, dropout = 50, 2, 4, 0.2\n",
    "model = autoformer.Model(args).cuda()\n",
    "# Load pretrained weights here.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "#fore_savepath = 'mimic_iii_24h_strats_no_interp_with_ss_fore_blubdiblub.pytorch'\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'max', patience=3)\n",
    "model.load_state_dict(torch.load(\"informer_regression2.pytorch\"))\n",
    "#model.load_state_dict(torch.load(fore_savepath))\n",
    "model.eval()\n",
    "y_true, y_pred = [], []\n",
    "print(test_input)\n",
    "pbar = tqdm(range(0, len(test_input), batch_size))\n",
    "loss_list = []\n",
    "test_loss = 0\n",
    "test_loss_part1 = 0\n",
    "test_loss_part2 = 0\n",
    "test_loss_sofa_variables = 0\n",
    "sofa_losses = []\n",
    "accuracy_list = []\n",
    "TP, TN, FP, FN = 0, 0, 0, 0\n",
    "test_loss_sum_square_total = 0\n",
    "test_loss_part1_sum_square_total = 0\n",
    "test_loss_part2_sum_square_total = 0\n",
    "test_loss_sofa_sum_square_total = 0\n",
    "for start in pbar:\n",
    "    print(start)\n",
    "    matrix = torch.tensor(test_input[start:start+batch_size], dtype=torch.float32).cuda()\n",
    "    input_matrix = matrix[:, :24]\n",
    "\n",
    "    #torch.Size([32, 24, 129])\n",
    "    input_mark = torch.arange(0, input_matrix.size(1)).unsqueeze(0).repeat(input_matrix.size(0), 1).cuda()\n",
    "    #torch.Size([32, 24])\n",
    "    input_mask = input_matrix[:, :24, 131:]\n",
    "    output_matrix = matrix[:, 24:, :131]\n",
    "    #torch.Size([32, 24, 129])\n",
    "    output_mark = torch.arange(0, output_matrix.size(1)).unsqueeze(0).repeat(input_matrix.size(0), 1).cuda()\n",
    "    #torch.Size([32, 24])\n",
    "    output_mask = matrix[:, 24:, 131:]\n",
    "    \n",
    "    output_mark = torch.arange(0, output_matrix.size(1)).unsqueeze(0).repeat(input_matrix.size(0), 1).cuda()\n",
    "    dec_inp = torch.zeros_like(output_matrix[:, -args.pred_len:, :]).float()\n",
    "    #torch.Size([32, 24, 129])\n",
    "    with torch.no_grad():\n",
    "        output = model(input_matrix, input_mark, dec_inp, output_mark, trainn=False)\n",
    "    loss = 0\n",
    "    for outpu, real, mask, input_mat in zip(output, output_matrix, output_mask, input_matrix):\n",
    "        a = outpu.item()\n",
    "        b = (get_sofa(real, var_to_ind))\n",
    "        c = (get_sofa(input_mat, var_to_ind)) #SOFA Source (First Day)\n",
    "        \n",
    "        prediction_sofa_difference = (a-sum(c)) >= 2\n",
    "        gold_sofa_difference = (sum(b)-sum(c)) >= 2\n",
    "        print(a, sum(b))\n",
    "        loss_term = (outpu.squeeze()-sum(b))**2\n",
    "        loss+=loss_term\n",
    "        accuracy_list.append(prediction_sofa_difference == gold_sofa_difference)\n",
    "        if gold_sofa_difference:\n",
    "            if prediction_sofa_difference: TP +=1\n",
    "            else: FN += 1\n",
    "        else:\n",
    "            if prediction_sofa_difference: FP +=1\n",
    "            else: TN += 1\n",
    "    loss_old = loss\n",
    "    loss = loss.sum(axis=-1).mean(axis=-1).sum()\n",
    "\n",
    "    test_loss+=loss\n",
    "\n",
    "print(TP, FP, FN, TN)\n",
    "print()\n",
    "accuracy_sepsis = sum(accuracy_list)/len(accuracy_list)\n",
    "se_accuracy_sepsis = np.sqrt(accuracy_sepsis * (1-accuracy_sepsis)/len(test_input))\n",
    "print(accuracy_sepsis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84105c10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
